{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R8VXvLDVppFK"
      },
      "source": [
        "# Import libraries & data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "52NkuawyIYY_"
      },
      "source": [
        "Article: [Temporal evolution of the relationship between political partisanship, underlying social vulnerabilities, and COVID-19 mortality across U.S. counties](https://deliverypdf.ssrn.com/delivery.php?ID=140021013065022109105064093124000120105018010061023037119085116064086000000026017000035020062104054111107000093000109110026026037007090023044116120031078127095119070054065073120025098112098068013072098094092019029127003096101011071108122077110028014066&EXT=pdf&INDEX=TRUE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3u-RPsrZIScy",
        "outputId": "dec73016-9340-469c-a54e-c7e35c0833df"
      },
      "outputs": [],
      "source": [
        "# upgrade excel package to load specific files\n",
        "%pip install -U xlrd\n",
        "# county choropleth graph\n",
        "%pip install -U geopandas\n",
        "%pip install -U pyshp\n",
        "%pip install -U shapely\n",
        "%pip install -U plotly-geo\n",
        "%pip install -U xgboost\n",
        "%pip install -U lightgbm\n",
        "%pip install -U scikit-learn\n",
        "%pip install -U tslearn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RmU-uqHOhUnO"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import pickle\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If working on Google Colab with Google Drive:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7K0jYI37UyTv"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "if (os.getcwd() != \"/content/drive/MyDrive/MIT Internship/American Communities\") and (os.getcwd() != \"/content/drive/My Drive/MIT Internship/American Communities\"):\n",
        "  os.chdir(\"drive/MyDrive/MIT Internship/American Communities\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CB0P1aMKFhXj"
      },
      "source": [
        "Execute the cells below to directly import the datasets (instead of calculate everything)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-H53mgVeFpPF"
      },
      "outputs": [],
      "source": [
        "type_data = \"COVID-19\"  # default type: All Causes, COVID-19, Excess Mortality\n",
        "\n",
        "county_databases = {}\n",
        "\n",
        "# Load all causes dataset\n",
        "county_database = pd.read_csv(\"county_database_all_causes.csv\")\n",
        "county_database.index = county_database.FIPS\n",
        "county_database2 = pd.read_csv(\"county_database2_all_causes.csv\")\n",
        "county_database2.index = county_database2.FIPS\n",
        "county_database2_imputed = pd.read_csv(\"county_database2_imputed_all_causes.csv\")\n",
        "county_database2_imputed.index = county_database2_imputed.FIPS\n",
        "\n",
        "county_database.drop(columns=[\"FIPS.1\"], inplace=True)\n",
        "county_database2.drop(columns=[\"FIPS.1\"], inplace=True)\n",
        "county_database2_imputed.drop(columns=[\"FIPS.1\"], inplace=True)\n",
        "\n",
        "county_databases[\"All Causes\"] = {}\n",
        "county_databases[\"All Causes\"][\"county_database\"] = county_database\n",
        "county_databases[\"All Causes\"][\"county_database2\"] = county_database2\n",
        "county_databases[\"All Causes\"][\"county_database2_imputed\"] = county_database2_imputed\n",
        "\n",
        "# Load COVID-19 dataset\n",
        "county_database = pd.read_csv(\"county_database_covid19.csv\")\n",
        "county_database.index = county_database.FIPS\n",
        "county_database2 = pd.read_csv(\"county_database2_covid19.csv\")\n",
        "county_database2.index = county_database2.FIPS\n",
        "county_database2_imputed = pd.read_csv(\"county_database2_imputed_covid19.csv\")\n",
        "county_database2_imputed.index = county_database2_imputed.FIPS\n",
        "\n",
        "county_database.drop(columns=[\"FIPS.1\"], inplace=True)\n",
        "county_database2.drop(columns=[\"FIPS.1\"], inplace=True)\n",
        "county_database2_imputed.drop(columns=[\"FIPS.1\"], inplace=True)\n",
        "\n",
        "county_databases[\"COVID-19\"] = {}\n",
        "county_databases[\"COVID-19\"][\"county_database\"] = county_database\n",
        "county_databases[\"COVID-19\"][\"county_database2\"] = county_database2\n",
        "county_databases[\"COVID-19\"][\"county_database2_imputed\"] = county_database2_imputed\n",
        "\n",
        "# Load Excess Mortality dataset\n",
        "county_database = pd.read_csv(\"county_database_excess_mortality.csv\")\n",
        "county_database.index = county_database.FIPS\n",
        "county_database2 = pd.read_csv(\"county_database2_excess_mortality.csv\")\n",
        "county_database2.index = county_database2.FIPS\n",
        "county_database2_imputed = pd.read_csv(\"county_database2_imputed_excess_mortality.csv\")\n",
        "county_database2_imputed.index = county_database2_imputed.FIPS\n",
        "\n",
        "county_database.drop(columns=[\"FIPS.1\"], inplace=True)\n",
        "county_database2.drop(columns=[\"FIPS.1\"], inplace=True)\n",
        "county_database2_imputed.drop(columns=[\"FIPS.1\"], inplace=True)\n",
        "\n",
        "county_databases[\"Excess Mortality\"] = {}\n",
        "county_databases[\"Excess Mortality\"][\"county_database\"] = county_database\n",
        "county_databases[\"Excess Mortality\"][\"county_database2\"] = county_database2\n",
        "county_databases[\"Excess Mortality\"][\"county_database2_imputed\"] = county_database2_imputed\n",
        "\n",
        "with open(\"feature_selection\", \"rb\") as fp:  # load feature selection\n",
        "  features = pickle.load(fp)\n",
        "  selected_columns_list = features[0]\n",
        "  X_selected_columns_list = features[1]\n",
        "\n",
        "# load default dataset\n",
        "county_database = county_databases[type_data][\"county_database\"]\n",
        "county_database2 = county_databases[type_data][\"county_database2\"]\n",
        "county_database2_imputed = county_databases[type_data][\"county_database2_imputed\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xgefA-CzLks6"
      },
      "source": [
        "# Create datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WcIO13HtOBvO"
      },
      "source": [
        "## Load & compile datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VK6rhMOL9sH8"
      },
      "source": [
        "### Response Variable (1 dataset for each response variable)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VovW7uoqnISv"
      },
      "source": [
        "#### John Hopkins University\n",
        "[COVID-19 Data Repository by the Center for Systems Science and Engineering (CSSE) at Johns Hopkins University](https://github.com/CSSEGISandData/COVID-19)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7wyndefpT4_N"
      },
      "outputs": [],
      "source": [
        "if not os.path.exists(\"COVID-19_JHU_data\"):\n",
        "  os.mkdir(\"COVID-19_JHU_data\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "Qij1sjfO8cpc",
        "outputId": "e129072e-6792-494f-d6a1-71f327549806"
      },
      "outputs": [],
      "source": [
        "# COVID-19 Data Repository by the Center for Systems Science and Engineering (CSSE) at Johns Hopkins University\n",
        "\"\"\"\n",
        "!unzip csse_covid_19_daily_reports.zip -d COVID-19_JHU_data\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G0n2UxzrVDyI"
      },
      "outputs": [],
      "source": [
        "import glob\n",
        "import math\n",
        "df_to_concatenate = []\n",
        "for filename in glob.glob(r\"COVID-19_JHU_data/csse_covid_19_daily_reports/*.csv\"):\n",
        "  df_date = pd.read_csv(filename)\n",
        "  if \"FIPS\" in df_date.columns:  # county-level data available\n",
        "    df_date = df_date[df_date[\"Country_Region\"] == \"US\"]\n",
        "    date = filename.split(\"\\\\\")[-1][:-4]\n",
        "    date = date[6:] + \"-\" + date[:2] + \"-\" + date[3:5]\n",
        "    df_date.index = df_date[\"FIPS\"].apply(lambda x: str(int(x)) if not(math.isnan(x)) else np.nan)\n",
        "    df_date = df_date[[\"FIPS\", \"Deaths\"]].dropna()  # remove county with no FIPS/no data\n",
        "    df_date = df_date[[\"Deaths\"]]\n",
        "    df_date.columns = [date]\n",
        "    df_date = df_date.T\n",
        "\n",
        "    # Merge duplicated counties\n",
        "    if df_date.columns.duplicated().any():\n",
        "      df_date = df_date.sum(axis=1, level=0, skipna=True)\n",
        "\n",
        "    df_to_concatenate.append(df_date)\n",
        "  \n",
        "# Merge all the dates\n",
        "df = pd.concat(df_to_concatenate)\n",
        "# Sort the dataframe by chronological order\n",
        "df.sort_index(inplace=True)\n",
        "df = df.fillna(method=\"ffill\")  # fill na forward\n",
        "df = df.cummax()  # cumulative max to deal with sudden drop of deaths\n",
        "df = df.fillna(0)  # then, nan=no data before (so replace with 0)\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m-Rudp5Yt3yT"
      },
      "outputs": [],
      "source": [
        "# applied a 7-day average to smooth the daily death counts to account for noise\n",
        "# in the data at the daily level\n",
        "\n",
        "###########\n",
        "# Floor ??\n",
        "###########\n",
        "\n",
        "df = df.rolling(window=7).mean()\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6WByswMeYj-f"
      },
      "outputs": [],
      "source": [
        "df.to_csv(\"COVID-19_timeseries.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "66wlBZgKZ4k4"
      },
      "source": [
        "[US County Boundaries](https://public.opendatasoft.com/explore/dataset/us-county-boundaries/export/?flg=fr&disjunctive.statefp&disjunctive.countyfp&disjunctive.name&disjunctive.namelsad&disjunctive.stusab&disjunctive.state_name) and the custom python script us_county_boundaries_to_coordinates.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QsyLF62GRAOf"
      },
      "outputs": [],
      "source": [
        "jhu_data = pd.read_csv(\"COVID-19_JHU_data/csse_covid_19_daily_reports/03-06-2022.csv\")\n",
        "jhu_data = jhu_data[jhu_data[\"Country_Region\"] == \"US\"]\n",
        "jhu_data[\"FIPS\"] = jhu_data[\"FIPS\"].apply(lambda x: str(x).replace(\".0\", \"\"))\n",
        "jhu_data = jhu_data.drop_duplicates(subset=\"FIPS\", keep=\"first\")  # drop duplicated counties\n",
        "jhu_data = jhu_data[[\"FIPS\", \"Province_State\"]]\n",
        "\n",
        "us_county_coordinates = pd.read_csv(\"us_county_coordinates.csv\")\n",
        "lat_list = []\n",
        "long_list = []\n",
        "for i in range(len(jhu_data)):\n",
        "  fips = jhu_data[\"FIPS\"].iloc[i]\n",
        "  try:\n",
        "    lat = us_county_coordinates[us_county_coordinates[\"GEOID\"] == int(fips)][\"Lat\"].iloc[0]\n",
        "  except Exception as e:\n",
        "    lat = np.nan\n",
        "  try:\n",
        "    long = us_county_coordinates[us_county_coordinates[\"GEOID\"] == int(fips)][\"Long_\"].iloc[0]\n",
        "  except Exception as e:\n",
        "    long = np.nan\n",
        "  lat_list.append(lat)\n",
        "  long_list.append(long)\n",
        "\n",
        "jhu_data[\"Lat\"] = lat_list\n",
        "jhu_data[\"Long_\"] = long_list\n",
        "jhu_data = jhu_data[~jhu_data[\"FIPS\"].isin([\"nan\"])]\n",
        "jhu_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7IZY6Ig-oRBv"
      },
      "source": [
        "#### Provisional Multiple Cause of Death Data [CDC Wonder](https://wonder.cdc.gov/mcd.html)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P-rq0IcNvqDY"
      },
      "source": [
        "Group by: Residence county, Year, Month"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 658
        },
        "id": "Dal-m303oUTu",
        "outputId": "e5a6ad14-ba99-47e4-d2b6-d701eeb64ffb"
      },
      "outputs": [],
      "source": [
        "cdc2020_1 = pd.read_csv(\"CDC Wonder/CDC_2020_1.csv\")  # Jan-Jun 2020\n",
        "cdc2020_1 = cdc2020_1[~cdc2020_1[\"Month Code\"].isna()]\n",
        "cdc2020_2 = pd.read_csv(\"CDC Wonder/CDC_2020_2.csv\")  # Jul-Dec 2020\n",
        "cdc2020_2 = cdc2020_2[~cdc2020_2[\"Month Code\"].isna()]\n",
        "cdc2021_1 = pd.read_csv(\"CDC Wonder/CDC_2021_1.csv\")  # Jan-Jun 2021\n",
        "cdc2021_1 = cdc2021_1[~cdc2021_1[\"Month Code\"].isna()]\n",
        "cdc2021_2 = pd.read_csv(\"CDC Wonder/CDC_2021_2.csv\")  # Jul-Dec 2021\n",
        "cdc2021_2 = cdc2021_2[~cdc2021_2[\"Month Code\"].isna()]\n",
        "cdc2022_1 = pd.read_csv(\"CDC Wonder/CDC_2022_1.csv\")  # Jan-Feb 2022\n",
        "cdc2022_1 = cdc2022_1[~cdc2022_1[\"Month Code\"].isna()]\n",
        "\n",
        "df1 = pd.concat([cdc2020_1, cdc2020_2, cdc2021_1, cdc2021_2, cdc2022_1])\n",
        "df1[\"Month Code\"] = pd.to_datetime(df1[\"Month Code\"], format=\"%Y/%m\")\n",
        "df1[\"Residence County Code\"] = df1[\"Residence County Code\"].apply(lambda x: str(int(x)))  # remove trailing zeros\n",
        "df1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 969
        },
        "id": "NyP9gsxtD0aN",
        "outputId": "d5082409-ed55-40aa-fd7b-d5375d81f977"
      },
      "outputs": [],
      "source": [
        "fips = np.unique(df1[\"Residence County Code\"])[0]\n",
        "df_fips = df1[df1[\"Residence County Code\"] == fips][[\"Deaths\", \"Month Code\"]]\n",
        "df_fips.index = df_fips[\"Month Code\"]\n",
        "df_fips = df_fips[[\"Deaths\"]]\n",
        "df_fips.columns = [fips]\n",
        "df = df_fips.copy()\n",
        "months = [f\"0{i}\" for i in range(1, 10)] + [\"10\", \"11\", \"12\"]\n",
        "complete_index_list = [pd.to_datetime(f\"{y}-{m}-01\") for y in [2020, 2021] for m in months] + [pd.to_datetime(\"2022-01-01\"), pd.to_datetime(\"2022-02-01\")]\n",
        "df = df.reindex(complete_index_list, fill_value=np.nan)\n",
        "for fips in np.unique(df1[\"Residence County Code\"])[1:]:\n",
        "  df_fips = df1[df1[\"Residence County Code\"] == fips][[\"Deaths\", \"Month Code\"]]\n",
        "  df_fips.index = df_fips[\"Month Code\"]\n",
        "  df_fips = df_fips[[\"Deaths\"]]\n",
        "  df_fips.columns = [fips]\n",
        "  df = df.join(df_fips)\n",
        "######\n",
        "# df = df.rolling(window=7).mean()\n",
        "######\n",
        "df = df.cumsum()  # cumulative sum\n",
        "df = df.fillna(method=\"ffill\")  # fill na forward\n",
        "df = df.fillna(0)  # then, nan=no data before (so replace with 0)\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_3a7MbnIS4eB"
      },
      "source": [
        "[US County Boundaries](https://public.opendatasoft.com/explore/dataset/us-county-boundaries/export/?flg=fr&disjunctive.statefp&disjunctive.countyfp&disjunctive.name&disjunctive.namelsad&disjunctive.stusab&disjunctive.state_name) and the custom python script us_county_boundaries_to_coordinates.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8rmcVkgYi0ry",
        "outputId": "ef3a95ca-18f0-4982-af98-1db01f7f6fd1"
      },
      "outputs": [],
      "source": [
        "state_id = pd.read_csv(\"state_id.csv\", encoding=\"latin\")\n",
        "state_id = {y: x for x, y in state_id.values}\n",
        "state_id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l40GLVPmR_a4",
        "outputId": "febf99e0-57bd-4815-ffd6-10ea1bdbbc4b"
      },
      "outputs": [],
      "source": [
        "jhu_data = df1[[\"Residence County Code\", \"Residence County\"]]\n",
        "jhu_data.columns = [\"FIPS\", \"Province_State\"]\n",
        "jhu_data[\"Province_State\"] = jhu_data[\"Province_State\"].apply(lambda x: x.split(\", \")[1])\n",
        "jhu_data[\"Province_State\"] = jhu_data[\"Province_State\"].apply(lambda x: state_id[x] if x in state_id else x)\n",
        "jhu_data[\"FIPS\"] = jhu_data[\"FIPS\"].apply(lambda x: str(int(x)))\n",
        "jhu_data = jhu_data.drop_duplicates(subset=\"FIPS\", keep=\"first\")  # drop duplicated counties\n",
        "\n",
        "us_county_coordinates = pd.read_csv(\"us_county_coordinates.csv\")\n",
        "lat_list = []\n",
        "long_list = []\n",
        "for i in range(len(jhu_data)):\n",
        "  fips = jhu_data[\"FIPS\"].iloc[i]\n",
        "  try:\n",
        "    lat = us_county_coordinates[us_county_coordinates[\"GEOID\"] == int(fips)][\"Lat\"].iloc[0]\n",
        "  except Exception as e:\n",
        "    lat = np.nan\n",
        "  try:\n",
        "    long = us_county_coordinates[us_county_coordinates[\"GEOID\"] == int(fips)][\"Long_\"].iloc[0]\n",
        "  except Exception as e:\n",
        "    long = np.nan\n",
        "  lat_list.append(lat)\n",
        "  long_list.append(long)\n",
        "\n",
        "jhu_data[\"Lat\"] = lat_list\n",
        "jhu_data[\"Long_\"] = long_list\n",
        "jhu_data = jhu_data[~jhu_data[\"FIPS\"].isin([\"nan\"])]\n",
        "jhu_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rSQSKdResV_U"
      },
      "source": [
        "#### [CDC Wonder](https://wonder.cdc.gov/mcd.html) - COVID-19 (not reliable)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "peFY__x2sZQn"
      },
      "source": [
        "Group by: Residence county, Year, Month\n",
        "\n",
        "Select multiple cause of death: U*07.1 (COVID-19)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 658
        },
        "id": "_ybq0LVpsh0R",
        "outputId": "97978645-4681-4339-84bb-8f3e99900e5e"
      },
      "outputs": [],
      "source": [
        "cdc2020_1 = pd.read_csv(\"CDC Wonder/CDC_covid_2020_1.csv\")  # Jan-Jun 2020\n",
        "cdc2020_1 = cdc2020_1[~cdc2020_1[\"Month Code\"].isnull()]\n",
        "cdc2020_2 = pd.read_csv(\"CDC Wonder/CDC_covid_2020_2.csv\")  # Jul-Dec 2020\n",
        "cdc2020_2 = cdc2020_2[~cdc2020_2[\"Month Code\"].isnull()]\n",
        "cdc2021_1 = pd.read_csv(\"CDC Wonder/CDC_covid_2021_1.csv\")  # Jan-Jun 2021\n",
        "cdc2021_1 = cdc2021_1[~cdc2021_1[\"Month Code\"].isnull()]\n",
        "cdc2021_2 = pd.read_csv(\"CDC Wonder/CDC_covid_2021_2.csv\")  # Jul-Dec 2021\n",
        "cdc2021_2 = cdc2021_2[~cdc2021_2[\"Month Code\"].isnull()]\n",
        "cdc2022_1 = pd.read_csv(\"CDC Wonder/CDC_covid_2022_1.csv\")  # Jan-Feb 2022\n",
        "cdc2022_1 = cdc2022_1[~cdc2022_1[\"Month Code\"].isnull()]\n",
        "\n",
        "df1 = pd.concat([cdc2020_1, cdc2020_2, cdc2021_1, cdc2021_2, cdc2022_1])\n",
        "df1[\"Month Code\"] = pd.to_datetime(df1[\"Month Code\"], format=\"%Y/%m\")\n",
        "df1[\"Residence County Code\"] = df1[\"Residence County Code\"].apply(lambda x: str(int(x)))  # remove trailing zeros\n",
        "df1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "kx5QmsYIs6WN",
        "outputId": "93c31d94-b031-43cf-c44d-9c7cfb4701f1"
      },
      "outputs": [],
      "source": [
        "fips = np.unique(df1[\"Residence County Code\"])[0]\n",
        "df_fips = df1[df1[\"Residence County Code\"] == fips][[\"Deaths\", \"Month Code\"]]\n",
        "df_fips.index = df_fips[\"Month Code\"]\n",
        "df_fips = df_fips[[\"Deaths\"]]\n",
        "df_fips.columns = [fips]\n",
        "df = df_fips.copy()\n",
        "months = [f\"0{i}\" for i in range(1, 10)] + [\"10\", \"11\", \"12\"]\n",
        "complete_index_list = [pd.to_datetime(f\"{y}-{m}-01\") for y in [2020, 2021] for m in months] + [pd.to_datetime(\"2022-01-01\"), pd.to_datetime(\"2022-02-01\")]\n",
        "df = df.reindex(complete_index_list, fill_value=np.nan)\n",
        "for fips in np.unique(df1[\"Residence County Code\"])[1:]:\n",
        "  df_fips = df1[df1[\"Residence County Code\"] == fips][[\"Deaths\", \"Month Code\"]]\n",
        "  df_fips.index = df_fips[\"Month Code\"]\n",
        "  df_fips = df_fips[[\"Deaths\"]]\n",
        "  df_fips.columns = [fips]\n",
        "  df = df.join(df_fips)\n",
        "######\n",
        "# df = df.rolling(window=7).mean()\n",
        "######\n",
        "df = df.cumsum()  # cumulative sum\n",
        "df = df.fillna(method=\"ffill\")  # fill na forward\n",
        "df = df.fillna(0)  # then, nan=no data before (so replace with 0)\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lmt3iioFs-N8"
      },
      "source": [
        "[US County Boundaries](https://public.opendatasoft.com/explore/dataset/us-county-boundaries/export/?flg=fr&disjunctive.statefp&disjunctive.countyfp&disjunctive.name&disjunctive.namelsad&disjunctive.stusab&disjunctive.state_name) and the custom python script us_county_boundaries_to_coordinates.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7sPn0AnDs__l"
      },
      "outputs": [],
      "source": [
        "state_id = pd.read_csv(\"state_id.csv\", encoding=\"latin\")\n",
        "state_id = {y: x for x, y in state_id.values}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 859
        },
        "id": "hvmlhiTptCpO",
        "outputId": "a10c186f-40d4-4d8c-e6ec-8ee755f73d64"
      },
      "outputs": [],
      "source": [
        "jhu_data = df1[[\"Residence County Code\", \"Residence County\"]]\n",
        "jhu_data.columns = [\"FIPS\", \"Province_State\"]\n",
        "jhu_data[\"Province_State\"] = jhu_data[\"Province_State\"].apply(lambda x: x.split(\", \")[1])\n",
        "jhu_data[\"Province_State\"] = jhu_data[\"Province_State\"].apply(lambda x: state_id[x] if x in state_id else x)\n",
        "jhu_data[\"FIPS\"] = jhu_data[\"FIPS\"].apply(lambda x: str(int(x)))\n",
        "jhu_data = jhu_data.drop_duplicates(subset=\"FIPS\", keep=\"first\")  # drop duplicated counties\n",
        "\n",
        "us_county_coordinates = pd.read_csv(\"us_county_coordinates.csv\")\n",
        "lat_list = []\n",
        "long_list = []\n",
        "for i in range(len(jhu_data)):\n",
        "  fips = jhu_data[\"FIPS\"].iloc[i]\n",
        "  try:\n",
        "    lat = us_county_coordinates[us_county_coordinates[\"GEOID\"] == int(fips)][\"Lat\"].iloc[0]\n",
        "  except Exception as e:\n",
        "    lat = np.nan\n",
        "  try:\n",
        "    long = us_county_coordinates[us_county_coordinates[\"GEOID\"] == int(fips)][\"Long_\"].iloc[0]\n",
        "  except Exception as e:\n",
        "    long = np.nan\n",
        "  lat_list.append(lat)\n",
        "  long_list.append(long)\n",
        "\n",
        "jhu_data[\"Lat\"] = lat_list\n",
        "jhu_data[\"Long_\"] = long_list\n",
        "jhu_data = jhu_data[~jhu_data[\"FIPS\"].isin([\"nan\"])]\n",
        "jhu_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MivPzgM-m_r_"
      },
      "source": [
        "#### Excess Mortality - [CDC Wonder](https://wonder.cdc.gov/mcd.html) Monthly"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RiG8Gn6eueez"
      },
      "source": [
        "Three different custom python scripts have benn executed to obtain the excess mortality data:\n",
        "\n",
        "*  process_cdc_wonder_data.py to convert CDC Wonder text files to csv files\n",
        "*  cdc_to_excessmort_format.py to merge CDC death datasets, add population data from [US Census Bureau](https://www.census.gov/data/datasets/time-series/demo/popest/2010s-counties-total.html), more precisely [here](https://www2.census.gov/programs-surveys/popest/datasets/)\n",
        "*  excessmort_cdc.R to apply the excessmort R package to our death data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RCbYPpiRnjhG"
      },
      "source": [
        "Excess mortality data are calculated using the following formula:\n",
        "\n",
        "$\\mu_{t}=N_{t} exp(\\alpha(t)+s(t))$\n",
        "\n",
        "with $Nt$ the population at time $t$, $\\alpha(t)$ a slow trend to account for the increase in life expectancy we have seen in the last few decades, a seasonal trend $s(t)$ to account for more deaths during the winter."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "ARrD1vbLswWa",
        "outputId": "216fd63c-55a2-4108-daa7-48ade42f82bf"
      },
      "outputs": [],
      "source": [
        "excess_mort_data = pd.read_csv(\"CDC Wonder/excess_mort_data.csv\")\n",
        "excess_mort_data[\"date\"] = pd.to_datetime(excess_mort_data[\"date\"])\n",
        "excess_mort_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "k3M3RYIvYo9a",
        "outputId": "8b99070f-5aa9-4f6a-be9c-207413150d42"
      },
      "outputs": [],
      "source": [
        "fips = np.unique(excess_mort_data[\"FIPS\"])[0]\n",
        "df_fips = excess_mort_data[excess_mort_data[\"FIPS\"] == fips][[\"excess_mortality\", \"date\"]]\n",
        "df_fips.index = df_fips[\"date\"]\n",
        "df_fips = df_fips[[\"excess_mortality\"]]\n",
        "df_fips.columns = [fips]\n",
        "df = df_fips.copy()\n",
        "months = [f\"0{i}\" for i in range(1, 10)] + [\"10\", \"11\", \"12\"]\n",
        "complete_index_list = [pd.to_datetime(f\"{y}-{m}-01\") for y in [2020, 2021] for m in months] + [pd.to_datetime(\"2022-01-01\"), pd.to_datetime(\"2022-02-01\")]\n",
        "df = df.reindex(complete_index_list, fill_value=np.nan)\n",
        "for fips in np.unique(excess_mort_data[\"FIPS\"])[1:]:\n",
        "  df_fips = excess_mort_data[excess_mort_data[\"FIPS\"] == fips][[\"excess_mortality\", \"date\"]]\n",
        "  df_fips.index = df_fips[\"date\"]\n",
        "  df_fips = df_fips[[\"excess_mortality\"]]\n",
        "  df_fips.columns = [fips]\n",
        "  df = df.join(df_fips)\n",
        "######\n",
        "# df = df.rolling(window=7).mean()\n",
        "######\n",
        "df = df.cumsum()  # cumulative sum\n",
        "df = df.fillna(method=\"ffill\")  # fill na forward\n",
        "df = df.fillna(0)  # then, nan=no data before (so replace with 0)\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1wginfh1wyDu"
      },
      "source": [
        "Drop outlier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tXAQQ6cIwzYF"
      },
      "outputs": [],
      "source": [
        "df[48153]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KNF8tRi9wlvr"
      },
      "outputs": [],
      "source": [
        "df.drop(columns=[48153], inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4L1FnJkBqJNM"
      },
      "outputs": [],
      "source": [
        "df.to_csv(\"excess_mortality_timeseries.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eVdfrK22osFH"
      },
      "source": [
        "[US County Boundaries](https://public.opendatasoft.com/explore/dataset/us-county-boundaries/export/?flg=fr&disjunctive.statefp&disjunctive.countyfp&disjunctive.name&disjunctive.namelsad&disjunctive.stusab&disjunctive.state_name) and the custom python script us_county_boundaries_to_coordinates.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UHBF061vorTY",
        "outputId": "75b052b6-0665-4d06-9153-6f11d87bc7f3"
      },
      "outputs": [],
      "source": [
        "state_id = pd.read_csv(\"state_id.csv\", encoding=\"latin\")\n",
        "state_id = {y: x for x, y in state_id.values}\n",
        "state_id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 750
        },
        "id": "WJzglPh_ot3p",
        "outputId": "a48dd955-25c8-49e5-d1ed-f386650640ad"
      },
      "outputs": [],
      "source": [
        "jhu_data = excess_mort_data[[\"FIPS\", \"county\"]]\n",
        "jhu_data.columns = [\"FIPS\", \"Province_State\"]\n",
        "jhu_data[\"Province_State\"] = jhu_data[\"Province_State\"].apply(lambda x: x.split(\", \")[1])\n",
        "jhu_data[\"Province_State\"] = jhu_data[\"Province_State\"].apply(lambda x: state_id[x] if x in state_id else x)\n",
        "jhu_data[\"FIPS\"] = jhu_data[\"FIPS\"].apply(lambda x: str(int(x)))\n",
        "jhu_data = jhu_data.drop_duplicates(subset=\"FIPS\", keep=\"first\")  # drop duplicated counties\n",
        "\n",
        "us_county_coordinates = pd.read_csv(\"us_county_coordinates.csv\")\n",
        "lat_list = []\n",
        "long_list = []\n",
        "for i in range(len(jhu_data)):\n",
        "  fips = jhu_data[\"FIPS\"].iloc[i]\n",
        "  try:\n",
        "    lat = us_county_coordinates[us_county_coordinates[\"GEOID\"] == int(fips)][\"Lat\"].iloc[0]\n",
        "  except Exception as e:\n",
        "    lat = np.nan\n",
        "  try:\n",
        "    long = us_county_coordinates[us_county_coordinates[\"GEOID\"] == int(fips)][\"Long_\"].iloc[0]\n",
        "  except Exception as e:\n",
        "    long = np.nan\n",
        "  lat_list.append(lat)\n",
        "  long_list.append(long)\n",
        "\n",
        "jhu_data[\"Lat\"] = lat_list\n",
        "jhu_data[\"Long_\"] = long_list\n",
        "jhu_data = jhu_data[~jhu_data[\"FIPS\"].isin([\"nan\"])]\n",
        "jhu_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NmYjap4GzIlB"
      },
      "source": [
        "### [Airports in the United States of America](https://data.humdata.org/dataset/ourairports-usa)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KlUKg2_mzo-M",
        "outputId": "47af7383-85e2-489e-9703-af06d886228d"
      },
      "outputs": [],
      "source": [
        "airports_df_loc = pd.read_csv(\"us-airports.csv\")\n",
        "airports_df_loc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-G5EOYXfpLMQ"
      },
      "source": [
        "### US Department of Transportation\n",
        "[International_Report_Passengers](https://data.transportation.gov/Aviation/International_Report_Passengers/xgub-n9bw)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rPvFx4HBpMja"
      },
      "outputs": [],
      "source": [
        "IRP = pd.read_csv(\"International_Report_Passengers.csv\")\n",
        "IRP[\"data_dte\"] = pd.to_datetime(IRP[\"data_dte\"], format=\"%m/%d/%Y\")\n",
        "# Filter flights before the pandemic\n",
        "IRP = IRP[IRP[\"data_dte\"] >= pd.to_datetime(\"2020-01-01\")]\n",
        "IRP = IRP[IRP[\"data_dte\"] <= pd.to_datetime(\"2020-03-01\")]\n",
        "airports_foreign_passengers = IRP.groupby(by=\"usg_apt\")[\"Total\"].sum()\n",
        "airports_foreign_passengers.sort_values(ascending=False, inplace=True)\n",
        "airports_foreign_passengers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jHYowPWsyJ2h"
      },
      "outputs": [],
      "source": [
        "plt.hist(airports_foreign_passengers)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j1nhE7yayGta"
      },
      "outputs": [],
      "source": [
        "df_airports_foreign_passengers = airports_foreign_passengers.to_frame()\n",
        "airport_list = list(df_airports_foreign_passengers.index)\n",
        "latitude = []\n",
        "longitude = []\n",
        "for airport in airport_list:\n",
        "  try:\n",
        "    latitude.append(airports_df_loc[airports_df_loc[\"ident\"] == \"K{}\".format(airport)][\"latitude_deg\"].iloc[0])\n",
        "    longitude.append(airports_df_loc[airports_df_loc[\"ident\"] == \"K{}\".format(airport)][\"longitude_deg\"].iloc[0])\n",
        "  except Exception as e:  # small airport an an island\n",
        "    latitude.append(np.nan)\n",
        "    longitude.append(np.nan)\n",
        "df_airports_foreign_passengers[\"latitude\"] = latitude\n",
        "df_airports_foreign_passengers[\"longitude\"] = longitude\n",
        "df_afp = df_airports_foreign_passengers  # shorter name\n",
        "df_afp.reset_index(drop=False, inplace=True)\n",
        "df_afp"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q3GtboKW30uE"
      },
      "source": [
        "Bubble Map Airport Frequentation early 2020 before the pandemic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YdWlllXK357b"
      },
      "outputs": [],
      "source": [
        "import plotly.graph_objects as go\n",
        "\n",
        "text_list = []\n",
        "for i in range(len(df_afp)):\n",
        "  text_list.append(\"{}<br>Passengers {}\".format(df_afp[\"usg_apt\"].iloc[i], \"{:,}\".format(df_afp[\"Total\"].iloc[i])))\n",
        "df_afp[\"text\"] = text_list\n",
        "limits = [(0, 5), (5, 10), (10, 50), (50, 100), (100, len(df_afp))]\n",
        "colors = [\"royalblue\", \"crimson\", \"lightseagreen\", \"orange\", \"lightgrey\"]\n",
        "cities = []\n",
        "scale = 2000\n",
        "\n",
        "fig = go.Figure()\n",
        "for i in range(len(limits)):\n",
        "    lim = limits[i]\n",
        "    df_sub = df_afp[lim[0]:lim[1]]\n",
        "    fig.add_trace(go.Scattergeo(\n",
        "        locationmode = \"USA-states\",\n",
        "        lon = df_sub[\"longitude\"],\n",
        "        lat = df_sub[\"latitude\"],\n",
        "        text = df_sub[\"text\"],\n",
        "        marker = dict(\n",
        "            size = df_sub[\"Total\"] / scale,\n",
        "            color = colors[i],\n",
        "            line_color=\"rgb(40, 40, 40)\",\n",
        "            line_width=0.5,\n",
        "            sizemode = \"area\"\n",
        "        ),\n",
        "        name = \"{0} - {1} top airports (ito passengers)\".format(lim[0]+1, lim[1])))\n",
        "\n",
        "fig.update_layout(\n",
        "        title_text = \"2020 January-March total passengers per airport<br>(Click legend to toggle traces)\",\n",
        "        showlegend = True,\n",
        "        geo = dict(\n",
        "            scope = \"usa\",\n",
        "            landcolor = \"rgb(217, 217, 217)\",\n",
        "        )\n",
        "    )\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m1fxHEE--dm1"
      },
      "source": [
        "Compute distance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XUxn6cIX-vBx"
      },
      "outputs": [],
      "source": [
        "# Lets' keep the top top_airport_number=50 airports (less than 25% of all airports in the US (except islands))\n",
        "top_airport_number = 50\n",
        "top_airport = df_afp[:top_airport_number]\n",
        "\n",
        "# calculate for each county the minimum distance to a top airport\n",
        "# don't put a distance = 0 if the airport is in the county. Compute the distance instead\n",
        "\n",
        "dist_list = []  # store the minimum distance to a top airport for each county\n",
        "closest_airport_list = []  # name of the closest top airport\n",
        "for i in range(len(jhu_data)):\n",
        "  lat_c = float(jhu_data[\"Lat\"].iloc[i])  # latitude county i\n",
        "  long_c = float(jhu_data[\"Long_\"].iloc[i])  # longitude county i\n",
        "  county_coord = np.array([lat_c, long_c])\n",
        "  \n",
        "  # initialize variables\n",
        "  min_dist = np.inf\n",
        "  closest_airport = np.nan\n",
        "  # find min distance\n",
        "  for j in range(top_airport_number):\n",
        "    lat_a = float(df_afp[\"latitude\"].iloc[j])  # latitude airport j\n",
        "    long_a = float(df_afp[\"longitude\"].iloc[j])  # longitude airport j\n",
        "    airport_coord = np.array([lat_a, long_a])\n",
        "    distance = np.linalg.norm(county_coord - airport_coord)  # L2 norm: euclidean distance\n",
        "    if distance < min_dist:\n",
        "      min_dist = distance\n",
        "      closest_airport = df_afp[\"usg_apt\"].iloc[j]\n",
        "  dist_list.append(min_dist)\n",
        "  closest_airport_list.append(closest_airport)\n",
        "\n",
        "# Update John Hopkins dataset\n",
        "jhu_data[\"min_distance_top_airport\"] = dist_list\n",
        "jhu_data[\"closest_airport\"] = closest_airport_list\n",
        "jhu_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YpOl0SaknWz7"
      },
      "source": [
        "### HHS Regions\n",
        "[HHS Region Table](https://hrbrmstr.github.io/cdcfluview/reference/hhs_regions.html) and a custom R script to transform the R dataset into a csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cwvJBNVjna1h"
      },
      "outputs": [],
      "source": [
        "hhs = pd.read_csv(\"hhs_regions.csv\")\n",
        "hhs.drop(columns=[\"Unnamed: 0\"], inplace=True)\n",
        "\n",
        "hhs_regions = []\n",
        "for i in range(len(jhu_data)):\n",
        "  try:\n",
        "    state = jhu_data[\"Province_State\"].iloc[i]\n",
        "    region = hhs[hhs[\"state_or_territory\"] == state][\"region_number\"].iloc[0]\n",
        "  except Exception as e:\n",
        "    region = np.nan\n",
        "  hhs_regions.append(region)\n",
        "jhu_data[\"HHS Region\"] = hhs_regions\n",
        "jhu_data[\"HHS Region\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ppUQjdLHEZLz"
      },
      "outputs": [],
      "source": [
        "hhs_region_dic = {1: \"Region 1 - Boston\",\n",
        "                  2: \"Region 2 - New York\",\n",
        "                  3: \"Region 3 - Philadelphia\",\n",
        "                  4: \"Region 4 - Atlanta\",\n",
        "                  5: \"Region 5 - Chicago\",\n",
        "                  6: \"Region 6 - Dallas\",\n",
        "                  7: \"Region 7 - Kansas City\",\n",
        "                  8: \"Region 8 - Denver\",\n",
        "                  9: \"Region 9 - San Francisco\",\n",
        "                  10: \"Region 10 - Seattle\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JU2_kb-ppMqM"
      },
      "source": [
        "### Oxford COVID-19 Government Response Tracker\n",
        "[COVID-19 GOVERNMENT RESPONSE TRACKER](https://github.com/OxCGRT/covid-policy-tracker/blob/master/data/OxCGRT_latest.csv)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AVk8mbbvpQea"
      },
      "outputs": [],
      "source": [
        "oxcgrt = pd.read_csv(\"OxCGRT_latest.csv\")\n",
        "oxcgrt = oxcgrt[oxcgrt[\"CountryName\"] == \"United States\"]  # keep US data only\n",
        "oxcgrt[\"Date\"] = pd.to_datetime(oxcgrt[\"Date\"], format=\"%Y%m%d\")\n",
        "\n",
        "# Date filter, between 2020/01/01 and max_date\n",
        "\n",
        "max_date_list = [\"2020-05-31\", \"2020-10-31\", \"2021-02-28\", \"2021-07-31\",\n",
        "                 \"2021-10-31\", \"2022-02-28\"]\n",
        "min_date_list = [\"2020-02-01\", \"2020-05-31\", \"2020-10-31\", \"2021-02-28\",\n",
        "                 \"2021-07-31\", \"2021-10-31\"]\n",
        "max_date = pd.to_datetime(\"2021-02-01\")\n",
        "\n",
        "si_lists = []  # for plots after\n",
        "std_lists = []  # for plots after\n",
        "\n",
        "for per in range(6):\n",
        "  min_date = pd.to_datetime(min_date_list[per])\n",
        "  max_date = pd.to_datetime(max_date_list[per])\n",
        "  oxcgrt2 = oxcgrt[oxcgrt[\"Date\"] <= max_date]\n",
        "  oxcgrt2 = oxcgrt2[oxcgrt2[\"Date\"] > min_date]\n",
        "\n",
        "  # group by state\n",
        "  oxcgrt2 = oxcgrt2.groupby(by=\"RegionName\")[\"StringencyIndex\"]\n",
        "  # extract features and back to dataframe format\n",
        "  oxcgrt2_mean = oxcgrt2.mean().to_frame()  # calculate the mean of the Stringency Index over the study period\n",
        "  oxcgrt2_median = oxcgrt2.median().to_frame()\n",
        "  oxcgrt2_min = oxcgrt2.min().to_frame()\n",
        "  oxcgrt2_max = oxcgrt2.max().to_frame()\n",
        "  std_df = oxcgrt2.std().to_frame()\n",
        "\n",
        "  # Update county database\n",
        "  \"\"\"\n",
        "  # Not stringency index for these states. NaN value instead\n",
        "  American Samoa\n",
        "  Diamond Princess\n",
        "  District of Columbia\n",
        "  Grand Princess\n",
        "  Guam\n",
        "  Northern Mariana Islands\n",
        "  Puerto Rico\n",
        "  Virgin Islands\n",
        "  \"\"\"\n",
        "\n",
        "  stringency_index_list = []  # mean\n",
        "  si_median_list = []\n",
        "  si_min_list = []\n",
        "  si_max_list = []\n",
        "  std_list = []\n",
        "  for i in range(len(jhu_data)):\n",
        "    try:\n",
        "      state = jhu_data[\"Province_State\"].iloc[i]\n",
        "      si_mean = oxcgrt2_mean.loc[state].iloc[0]\n",
        "      si_median = oxcgrt2_median.loc[state].iloc[0]\n",
        "      si_min = oxcgrt2_min.loc[state].iloc[0]\n",
        "      si_max = oxcgrt2_max.loc[state].iloc[0]\n",
        "      std = std_df.loc[state].iloc[0]\n",
        "    except Exception as e:\n",
        "      si_mean = np.nan\n",
        "      si_median = np.nan\n",
        "      si_min = np.nan\n",
        "      si_max = np.nan\n",
        "      std = np.nan\n",
        "    stringency_index_list.append(si_mean)\n",
        "    si_median_list.append(si_median)\n",
        "    si_min_list.append(si_min)\n",
        "    si_max_list.append(si_max)\n",
        "    std_list.append(std)\n",
        "  \n",
        "  si_lists.append(stringency_index_list)\n",
        "  std_lists.append(std_list)\n",
        "\n",
        "  jhu_data[f\"StringencyIndex{per+1}_mean\"] = stringency_index_list\n",
        "  jhu_data[f\"StringencyIndex{per+1}_median\"] = si_median_list\n",
        "  jhu_data[f\"StringencyIndex{per+1}_min\"] = si_min_list\n",
        "  jhu_data[f\"StringencyIndex{per+1}_max\"] = si_max_list\n",
        "  jhu_data[f\"StringencyIndex{per+1}_std\"] = std_list\n",
        "jhu_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-u7PrNcphLeE"
      },
      "source": [
        "Check the standard deviations'distributions to check if it's ok to take the mean of the stringency index over a period"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1_sMYRVDhfOc"
      },
      "outputs": [],
      "source": [
        "from plotly.subplots import make_subplots\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "fig = make_subplots(rows=2, cols=3)\n",
        "\n",
        "for per in range(6):\n",
        "  row = (per // 3) + 1\n",
        "  col = (per % 3) + 1\n",
        "  meanSI = np.nanmean(std_lists[per])\n",
        "  fig.add_trace(\n",
        "      go.Histogram(x=std_lists[per],\n",
        "                   name=f\"Period {per+1}: mean={round(meanSI, 2)}\"),\n",
        "      row=row, col=col\n",
        "  )\n",
        "\n",
        "fig.update_layout(height=row*300, width=900,\n",
        "                  title_text=f\"Distribution of Stringency Index's standard deviation for each county for all the periods\",\n",
        "                  xaxis_title=f\"Stringency index standard deviation\",\n",
        "                  yaxis_title=\"Count\",\n",
        "                  legend_title=\"Period\")\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Rnf2BVWkrZs"
      },
      "source": [
        "Check the mean"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HZipg7Ekks4W"
      },
      "outputs": [],
      "source": [
        "from plotly.subplots import make_subplots\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "fig = make_subplots(rows=2, cols=3)\n",
        "\n",
        "for per in range(6):\n",
        "  row = (per // 3) + 1\n",
        "  col = (per % 3) + 1\n",
        "  meanSI = np.nanmean(si_lists[per])\n",
        "  fig.add_trace(\n",
        "      go.Histogram(x=si_lists[per],\n",
        "                   name=f\"Period {per+1}: mean={round(meanSI, 2)}\"),\n",
        "      row=row, col=col\n",
        "  )\n",
        "\n",
        "fig.update_layout(height=row*300, width=900,\n",
        "                  title_text=f\"Distribution of Stringency Index's mean for each county for all the periods\",\n",
        "                  xaxis_title=f\"Average stringency index\",\n",
        "                  yaxis_title=\"Count\",\n",
        "                  legend_title=\"Period\")\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mBM_T60zpQor"
      },
      "source": [
        "### Election Results\n",
        "[Presidential precinct data for the 2020 general election](https://github.com/TheUpshot/presidential-precinct-map-2020)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9DwmCxM9mc6Q"
      },
      "outputs": [],
      "source": [
        "# Load the preprocessed csv (original data from precincts-with-results.geojson.gz)\n",
        "# custom preprocessing script process_election_data.py\n",
        "# total population of each county extracted from ACS\n",
        "political_data = pd.read_csv(\"political_leaning.csv\")\n",
        "political_data.index = political_data[\"Unnamed: 0\"]\n",
        "political_data.drop(columns=[\"Unnamed: 0\"], inplace=True)\n",
        "\n",
        "# change columns name to remove 0s from the beginning of the FIPS\n",
        "political_data.columns = [str(int(x)) for x in list(political_data.columns)]\n",
        "political_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gisACnTNjmQU"
      },
      "source": [
        "Update county information dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MQVmcJaQjqQg"
      },
      "outputs": [],
      "source": [
        "# Update county database\n",
        "political_leaning_list = []\n",
        "for i in range(len(jhu_data)):\n",
        "  fips = jhu_data[\"FIPS\"].iloc[i]\n",
        "  try:\n",
        "    political_leaning = political_data.loc[\"political_leaning\"][fips]\n",
        "  except Exception as e:\n",
        "    political_leaning = np.nan\n",
        "  political_leaning_list.append(political_leaning)\n",
        "jhu_data[\"political_leaning\"] = political_leaning_list\n",
        "jhu_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tvs6yHT2otj8"
      },
      "source": [
        "### American Communities Project\n",
        "[American Communities Project](https://www.americancommunities.org/#map)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "81ME4Fla-7rl"
      },
      "outputs": [],
      "source": [
        "# American Communities Project (ACP) typology by county\n",
        "county_df = pd.read_excel(\"County-Type-Share.xlsx\")\n",
        "\n",
        "county_info = county_df[[\"Key\", \"New Names\"]].dropna()\n",
        "county_info[\"Key\"] = county_info[\"Key\"].astype(int)\n",
        "county_df = county_df[[\"County\", \"Type Number\", \"FIPS\"]]\n",
        "description_county = {\"Exurbs\": \"Wealthy, well-educated, largely white. Low crime rates & longer commutes.\",\n",
        "                      \"Graying America\": \"Large senior population. Middle-income, low diversity, avg. education.\",\n",
        "                      \"African American South\": \"Median roughly 40% African American. Low income, high unemployment.\",\n",
        "                      \"Evangelical Hubs\": \"Many evangelical adherents. Fewer college grads & healthcare providers.\",\n",
        "                      \"Working Class Country\": \"Highly educated, low diversity. Young with high turnover in population.\",\n",
        "                      \"Military Posts\": \"Middle-income, diverse communities around military bases.\",\n",
        "                      \"Urban Suburbs\": \"Educated and densely populated. Racially and economically diverse.\",\n",
        "                      \"Hispanic Centers\": \"Heavily Hispanic & rural. Young, lower incomes, limited healthcare access.\",\n",
        "                      \"Native American Lands\": \"Large Native American population. Young, low income, many uninsured.\",\n",
        "                      \"Rural Middle America\": \"Largely rural and white. Middle-income, avg. college grad numbers.\",\n",
        "                      \"College Towns\": \"Highly educated, low diversity. Young with high turnover in population.\",\n",
        "                      \"LDS Enclaves\": \"Large youth population. Middle-income, low diversity, low crime.\",\n",
        "                      \"Aging Farmlands\": \"Agricultural with many seniors. Little diversity, low unemployment.\",\n",
        "                      \"Big Cities\": \"Dense & diverse. High incomes & high poverty. High crime rates.\",\n",
        "                      \"Middle Suburbs\": \"Middle-income, low diversity, and below average for college education.\"}\n",
        "county_info[\"County Description\"] = county_info[\"New Names\"].apply(lambda x: description_county[x])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RGwOdk5sB8mZ"
      },
      "outputs": [],
      "source": [
        "county_info.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RmazE8OBaQOt"
      },
      "outputs": [],
      "source": [
        "county_info_dic = {}\n",
        "for i in range(len(county_info)):\n",
        "  county_info_dic[county_info[\"Key\"].iloc[i]] = county_info[\"New Names\"].iloc[i]\n",
        "county_info_dic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6QJ6XfvxB-dy"
      },
      "outputs": [],
      "source": [
        "county_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0w2vXbHpszXP"
      },
      "source": [
        "Update county database"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y_3ZbaHOs1XF"
      },
      "outputs": [],
      "source": [
        "acp_list = []\n",
        "for i in range(len(jhu_data)):\n",
        "  fips = jhu_data[\"FIPS\"].iloc[i]\n",
        "  try:\n",
        "    acp = county_df[county_df[\"FIPS\"] == int(fips)][\"Type Number\"].iloc[0]\n",
        "  except Exception as e:\n",
        "    acp = np.nan\n",
        "  acp_list.append(acp)\n",
        "jhu_data[\"acp\"] = acp_list\n",
        "jhu_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2UQ1TAacMJA8"
      },
      "outputs": [],
      "source": [
        "# Features with name\n",
        "import math\n",
        "jhu_data[\"acp_name\"] = jhu_data[\"acp\"].apply(lambda x: county_info_dic[x] if not(math.isnan(x)) else \"Unknown\")\n",
        "jhu_data[\"acp_name_with_number\"] = jhu_data[\"acp\"].apply(lambda x: str(int(x)) + \" \" + str(county_info_dic[x]) if not(math.isnan(x)) else \"Unknown\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1JFwEuUjoyiJ"
      },
      "source": [
        "### American Community Survey\n",
        "[American Community Survey 2014-2018 5-Year Estimates Now Available](https://www.census.gov/newsroom/press-releases/2019/acs-5-year.html), more precisely [here](https://www2.census.gov/programs-surveys/acs/summary_file/2018/data/5_year_comparison_profiles/county/) and for the areas [here](https://www.census.gov/library/publications/2011/compendia/usa-counties-2011.html#LND)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h2d2bUz2RLal"
      },
      "source": [
        "~7min runtime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g6o9cBkuo9YO"
      },
      "outputs": [],
      "source": [
        "# race/ethnic composition\n",
        "pct_white_l = []\n",
        "pct_black_l = []\n",
        "pct_asian_l = []\n",
        "pct_hispanic_l = []\n",
        "pct_other_l = []\n",
        "# education\n",
        "education_l = []\n",
        "# age\n",
        "under_19_l = []\n",
        "over_65_l = []\n",
        "# income: median household income\n",
        "income_l = []\n",
        "# pop densisty\n",
        "total_pop_l = []\n",
        "county_area_l = []\n",
        "# household crowding: more occupants than rooms\n",
        "hc_l = []\n",
        "\n",
        "# load the files\n",
        "cp05 = pd.read_csv(\"American_Community_Survey_-_United_States_Census/cp05.csv\", encoding=\"latin\")\n",
        "cp05[\"county_id\"] = cp05[\"GEOID\"].apply(lambda x: str(int(x.split(\"US\")[1])))\n",
        "cp04 = pd.read_csv(\"American_Community_Survey_-_United_States_Census/cp04.csv\", encoding=\"latin\")\n",
        "cp04[\"county_id\"] = cp04[\"GEOID\"].apply(lambda x: str(int(x.split(\"US\")[1])))\n",
        "cp03 = pd.read_csv(\"American_Community_Survey_-_United_States_Census/cp03.csv\", encoding=\"latin\")\n",
        "cp03[\"county_id\"] = cp03[\"GEOID\"].apply(lambda x: str(int(x.split(\"US\")[1])))\n",
        "cp02 = pd.read_csv(\"American_Community_Survey_-_United_States_Census/cp02.csv\", encoding=\"latin\")\n",
        "cp02[\"county_id\"] = cp02[\"GEOID\"].apply(lambda x: str(int(x.split(\"US\")[1])))\n",
        "# county land area data: process it\n",
        "land_df = pd.read_excel(\"American_Community_Survey_-_United_States_Census/LND01.xls\")\n",
        "column_names = list(land_df.columns)\n",
        "metadata_land = pd.read_excel(\"American_Community_Survey_-_United_States_Census/Mastdata.xls\")\n",
        "for i, v in enumerate(column_names):\n",
        "  sub_df = metadata_land[metadata_land[\"Item_Id\"] == v]\n",
        "  if not(sub_df.empty):  # change the name of the column to a more comprehensible one\n",
        "    column_names[i] = sub_df[\"Item_Description\"].iloc[0]\n",
        "land_df.columns = column_names  # update columns names\n",
        "land_df[\"STCOU\"] = land_df[\"STCOU\"].astype(str)\n",
        "\n",
        "# useful function to fetch data\n",
        "def _fetch_data(df, title):\n",
        "  result = df[df[\"TITLE\"] == title][\"EST_1418\"].iloc[0]\n",
        "  if \"(X)\" in result:\n",
        "    result = np.nan\n",
        "  else:\n",
        "    result = float(result.replace(\",\", \"\"))\n",
        "  return result\n",
        "\n",
        "# retrieve the information\n",
        "for i in range(len(jhu_data)):\n",
        "  fips = jhu_data[\"FIPS\"].iloc[i]\n",
        "\n",
        "  # land area\n",
        "  county_area_df = land_df[land_df[\"STCOU\"] == fips]\n",
        "  if county_area_df.empty:\n",
        "    county_area = np.nan\n",
        "  else:\n",
        "    county_area = float(county_area_df[\"Land area in square miles 2000\"].iloc[0])\n",
        "\n",
        "  # other info\n",
        "  cp05_county = cp05[cp05[\"county_id\"] == fips]\n",
        "  if cp05_county.empty:\n",
        "    pct_white = np.nan\n",
        "    pct_black = np.nan\n",
        "    pct_asian = np.nan\n",
        "    pct_hispanic = np.nan\n",
        "    pct_other = np.nan\n",
        "    total_pop = np.nan\n",
        "    under_19 = np.nan\n",
        "    over_65 = np.nan\n",
        "  else:\n",
        "    pct_white = _fetch_data(cp05_county, \"White\")\n",
        "    pct_black = _fetch_data(cp05_county, \"Black or African American\")\n",
        "    pct_asian = _fetch_data(cp05_county, \"Asian\")\n",
        "    pct_hispanic = _fetch_data(cp05_county, \"Hispanic or Latino (of any race)\")\n",
        "    pct_other = _fetch_data(cp05_county, \"Some other race\")\n",
        "    total_pop = _fetch_data(cp05_county, \"Total population\")\n",
        "    under_19 = _fetch_data(cp05_county, \"Under 18 years\")\n",
        "    over_65 = _fetch_data(cp05_county, \"65 years and over\")\n",
        "\n",
        "  cp04_county = cp04[cp04[\"county_id\"] == fips]\n",
        "  if cp04_county.empty:\n",
        "    hc = np.nan\n",
        "  else:\n",
        "    hc = _fetch_data(cp04_county, \"1.00 or less\")\n",
        "    if not(np.isnan(hc)):\n",
        "      hc = 100 - hc\n",
        "\n",
        "  cp03_county = cp03[cp03[\"county_id\"] == fips]\n",
        "  if cp03_county.empty:\n",
        "    income = np.nan\n",
        "  else:\n",
        "    income = _fetch_data(cp03_county, \"Median household income (dollars)\")\n",
        "  \n",
        "  cp02_county = cp02[cp02[\"county_id\"] == fips]\n",
        "  if cp02_county.empty:\n",
        "    education = np.nan\n",
        "  else:\n",
        "    education = _fetch_data(cp02_county, \"Percent high school graduate or higher\")\n",
        "\n",
        "  pct_white_l.append(pct_white)\n",
        "  pct_black_l.append(pct_black)\n",
        "  pct_asian_l.append(pct_asian)\n",
        "  pct_hispanic_l.append(pct_hispanic)\n",
        "  pct_other_l.append(pct_other)\n",
        "  education_l.append(education)\n",
        "  under_19_l.append(under_19)\n",
        "  over_65_l.append(over_65)\n",
        "  income_l.append(income)\n",
        "  total_pop_l.append(total_pop)\n",
        "  county_area_l.append(county_area)\n",
        "  hc_l.append(hc)\n",
        "\n",
        "jhu_data[\"pch_white\"] = pct_white_l\n",
        "jhu_data[\"pct_black\"] = pct_black_l\n",
        "jhu_data[\"pct_asian\"] = pct_asian_l\n",
        "jhu_data[\"pct_hispanic\"] = pct_hispanic_l\n",
        "jhu_data[\"pct_other\"] = pct_other_l\n",
        "jhu_data[\"education\"] = education_l\n",
        "jhu_data[\"under_19\"] = under_19_l\n",
        "jhu_data[\"over_65\"] = over_65_l\n",
        "jhu_data[\"income\"] = income_l\n",
        "jhu_data[\"total_pop\"] = total_pop_l\n",
        "jhu_data[\"county_area\"] = county_area_l\n",
        "jhu_data[\"hc\"] = hc_l\n",
        "jhu_data[\"pop_density\"] = jhu_data[\"total_pop\"] / jhu_data[\"county_area\"]\n",
        "# jhu_data.drop(columns=[\"total_pop\", \"county_area\"], inplace=True)  # calculus artifacts\n",
        "jhu_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q7v6Ocj3pC4a"
      },
      "source": [
        "### VERA Institute\n",
        "[Incarceration Trends Dataset](https://github.com/vera-institute/incarceration-trends)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RryoEd4oUCOZ"
      },
      "source": [
        "Total jail population is defined as the average daily number of people held in jail through December 31 of a given year."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rbU3CJj_pGuz"
      },
      "outputs": [],
      "source": [
        "vera_data = pd.read_csv(\"incarceration_trends.csv\")\n",
        "vera_data[\"fips\"] = vera_data[\"fips\"].astype(str)\n",
        "vera_data = vera_data[vera_data[\"year\"] == 2018]  # filter only last year of data (2018)\n",
        "vera_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9CvvIGmOUSy9"
      },
      "source": [
        "Add VERA jail data to database"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pBaRe9ZTUXee"
      },
      "outputs": [],
      "source": [
        "tot_jail_pop_l = []\n",
        "for i in range(len(jhu_data)):\n",
        "  fips = jhu_data[\"FIPS\"].iloc[i]\n",
        "  vera_subdf = vera_data[vera_data[\"fips\"] == fips]\n",
        "  if vera_subdf.empty:\n",
        "    tot_jail_pop = np.nan\n",
        "  else:\n",
        "    tot_jail_pop = float(vera_subdf[\"total_jail_pop\"].iloc[0])\n",
        "  tot_jail_pop_l.append(tot_jail_pop)\n",
        "jhu_data[\"total_jail_pop\"] = tot_jail_pop_l\n",
        "jhu_data[\"total_jail_pop\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JMDBcvO6o9ie"
      },
      "source": [
        "### Center for Medicare and Medicaid Services\n",
        "[COVID-19 Nursing Home Data](https://data.cms.gov/covid-19/covid-19-nursing-home-data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IRiaZJ0Jr7T9"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "!unzip \"Center_for_Medicare_and_Medicaid_Services/faclevel_2020.zip\" -d \"Center_for_Medicare_and_Medicaid_Services\"\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "neJxtWRypCsp"
      },
      "outputs": [],
      "source": [
        "nursing_data = pd.read_csv(\"Center_for_Medicare_and_Medicaid_Services/faclevel_2020.csv\")\n",
        "# Calculate average number of occupied bed per county over the year 2020\n",
        "nursing_data = nursing_data.groupby(by=[\"County\", \"Provider Name\"])[\"Total Number of Occupied Beds\"].mean().groupby(by=\"County\").sum()\n",
        "nursing_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NVsnlzEpeja-"
      },
      "outputs": [],
      "source": [
        "pairs_fips_countyName = vera_data[[\"fips\", \"county_name\"]]\n",
        "pairs_fips_countyName[\"county_name\"] = pairs_fips_countyName[\"county_name\"].apply(lambda x: x.replace(\" County\", \"\"))\n",
        "nursing_pop_l = []\n",
        "for i in range(len(jhu_data)):\n",
        "  fips = jhu_data[\"FIPS\"].iloc[i]\n",
        "  try:\n",
        "    cty_name = pairs_fips_countyName[pairs_fips_countyName[\"fips\"] == fips][\"county_name\"].iloc[0]\n",
        "    nursing_pop = nursing_data.loc[cty_name]\n",
        "  except Exception as e:\n",
        "    nursing_pop = np.nan\n",
        "  nursing_pop_l.append(nursing_pop)\n",
        "jhu_data[\"nursing_population\"] = nursing_pop_l\n",
        "jhu_data[\"nursing_population\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UFwR9dSTpGPV"
      },
      "source": [
        "### PLACES dataset\n",
        "[PLACES: Local Data for Better Health, County Data 2021 release500 Cities & Places](https://chronicdata.cdc.gov/500-Cities-Places/PLACES-Local-Data-for-Better-Health-County-Data-20/swc5-untb) & [2020](https://chronicdata.cdc.gov/500-Cities-Places/PLACES-Local-Data-for-Better-Health-County-Data-20/dv4u-3x3q)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ycOAUT1DpHOo"
      },
      "outputs": [],
      "source": [
        "places = pd.read_csv(\"PLACES/PLACES__Local_Data_for_Better_Health__County_Data_2021_release.csv\")  # 2021 release contains year 2019 data\n",
        "places = places[places[\"Measure\"] == \"Obesity among adults aged >=18 years\"]\n",
        "places = places[places[\"Year\"] == 2019]\n",
        "places"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6AT_W4g6esIz"
      },
      "outputs": [],
      "source": [
        "obesity_data_l = []\n",
        "county_name_l = []\n",
        "for i in range(len(jhu_data)):\n",
        "  fips = jhu_data[\"FIPS\"].iloc[i]\n",
        "  try:\n",
        "    cty_name = pairs_fips_countyName[pairs_fips_countyName[\"fips\"] == fips][\"county_name\"].iloc[0]\n",
        "    obesity_data = float(places[places[\"LocationName\"] == cty_name][\"Data_Value\"].iloc[0])\n",
        "  except Exception as e:\n",
        "    cty_name = np.nan\n",
        "    obesity_data = np.nan\n",
        "  obesity_data_l.append(obesity_data)\n",
        "  county_name_l.append(cty_name)\n",
        "jhu_data[\"obesity\"] = obesity_data_l\n",
        "jhu_data[\"county_name\"] = county_name_l\n",
        "jhu_data[\"obesity\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3zThG0lszogP"
      },
      "source": [
        "### Nationwide Wastewater Monitoring Network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YOUDIAA1zxq1"
      },
      "source": [
        "[Data repository for Biobot Analytics Nationwide Wastewater Monitoring Network](https://github.com/biobotanalytics/covid19-wastewater-data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5uVGhcXoz5xO"
      },
      "outputs": [],
      "source": [
        "biobot = pd.read_csv(\"wastewater_by_county.csv\")\n",
        "biobot[\"sampling_week\"] = pd.to_datetime(biobot[\"sampling_week\"])\n",
        "biobot.drop(columns=[\"Unnamed: 0\"], inplace=True)\n",
        "biobot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FjGQOVL12z3e"
      },
      "outputs": [],
      "source": [
        "# Date filter, between 2020/01/01 and max_date\n",
        "\n",
        "max_date_list = [\"2020-05-31\", \"2020-10-31\", \"2021-02-28\", \"2021-07-31\",\n",
        "                 \"2021-10-31\", \"2022-02-28\"]\n",
        "min_date_list = [\"2020-02-01\", \"2020-05-31\", \"2020-10-31\", \"2021-02-28\",\n",
        "                 \"2021-07-31\", \"2021-10-31\"]\n",
        "max_date = pd.to_datetime(\"2021-02-01\")\n",
        "\n",
        "# ecra = effective_concentration_rolling_average\n",
        "ecra_lists = []  # for plots after\n",
        "std_lists = []  # for plots after\n",
        "\n",
        "for per in range(6):\n",
        "  min_date = pd.to_datetime(min_date_list[per])\n",
        "  max_date = pd.to_datetime(max_date_list[per])\n",
        "  biobot2 = biobot[biobot[\"sampling_week\"] <= max_date]\n",
        "  biobot2 = biobot2[biobot2[\"sampling_week\"] > min_date]\n",
        "\n",
        "  # group by county\n",
        "  biobot2 = biobot2.groupby(by=\"fipscode\")[\"effective_concentration_rolling_average\"]\n",
        "  # extract features and back to dataframe format\n",
        "  biobot2_mean = biobot2.mean().to_frame()  # calculate the mean of the effective_concentration_rolling_average over the study period\n",
        "  biobot2_median = biobot2.median().to_frame()\n",
        "  biobot2_min = biobot2.min().to_frame()\n",
        "  biobot2_max = biobot2.max().to_frame()\n",
        "  std_df = biobot2.std().to_frame()\n",
        "\n",
        "  # Update county database\n",
        "\n",
        "  ecra_list = []  # mean\n",
        "  ecra_median_list = []\n",
        "  ecra_min_list = []\n",
        "  ecra_max_list = []\n",
        "  std_list = []\n",
        "  for i in range(len(jhu_data)):\n",
        "    try:\n",
        "      fips = jhu_data[\"FIPS\"].iloc[i]\n",
        "      fips = int(fips)  # convert to int type (type of biobot fips code)\n",
        "      ecra_mean = biobot2_mean.loc[fips].iloc[0]\n",
        "      ecra_median = biobot2_median.loc[fips].iloc[0]\n",
        "      ecra_min = biobot2_min.loc[fips].iloc[0]\n",
        "      ecra_max = biobot2_max.loc[fips].iloc[0]\n",
        "      std = std_df.loc[fips].iloc[0]\n",
        "    except Exception as e:\n",
        "      ecra_mean = np.nan\n",
        "      ecra_median = np.nan\n",
        "      ecra_min = np.nan\n",
        "      ecra_max = np.nan\n",
        "      std = np.nan\n",
        "    ecra_list.append(ecra_mean)\n",
        "    ecra_median_list.append(ecra_median)\n",
        "    ecra_min_list.append(ecra_min)\n",
        "    ecra_max_list.append(ecra_max)\n",
        "    std_list.append(std)\n",
        "  \n",
        "  ecra_lists.append(ecra_list)\n",
        "  std_lists.append(std_list)\n",
        "\n",
        "  jhu_data[f\"ecra{per+1}_mean\"] = ecra_list\n",
        "  jhu_data[f\"ecra{per+1}_median\"] = ecra_median_list\n",
        "  jhu_data[f\"ecra{per+1}_min\"] = ecra_min_list\n",
        "  jhu_data[f\"ecra{per+1}_max\"] = ecra_max_list\n",
        "  jhu_data[f\"ecra{per+1}_std\"] = std_list\n",
        "jhu_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nGPeK73u8fpa"
      },
      "source": [
        "## Optional import (not implemented)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WYzR8jZzpTF_"
      },
      "source": [
        "### Delphi Epidata\n",
        "[a]()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xO87AxOypWDy"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "esrh1fg0pWM6"
      },
      "source": [
        "### Google Mobility\n",
        "[a]()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nF_iYNsopZfe"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IaAPycyEpZn0"
      },
      "source": [
        "### The COVID Tracking Project\n",
        "[a]()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "emyStBL4pa3m"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "snVbKnLXYF6A"
      },
      "source": [
        "## Create county database"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8AeKRh2YYHmI",
        "outputId": "bd5f9881-bc67-484f-ce2d-e104b308bc77"
      },
      "outputs": [],
      "source": [
        "# Rename our county-level database and remove the county with a nan FIPS\n",
        "county_database = jhu_data[jhu_data[\"FIPS\"] != \"nan\"]\n",
        "\n",
        "# Add some log scale columns & percentage columns\n",
        "county_database[\"log_pop_density\"] = np.log(county_database[\"pop_density\"])\n",
        "county_database[\"log_crowding\"] = np.log(county_database[\"hc\"])\n",
        "county_database[\"log_crowding\"] = county_database[\"log_crowding\"].replace(-np.inf, -10)  # -inf crowding replaced by -10 for fitting models\n",
        "county_database[\"pct_nursing\"] = county_database[\"nursing_population\"] / county_database[\"total_pop\"]\n",
        "county_database[\"pct_jail\"] = county_database[\"total_jail_pop\"] / county_database[\"total_pop\"]\n",
        "\n",
        "# Change index\n",
        "county_database.index = county_database.FIPS\n",
        "\n",
        "# Replace inf values by nan\n",
        "county_database = county_database.replace(np.inf, np.nan)\n",
        "county_database = county_database.replace(-np.inf, np.nan)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XsrZU1moKw9u"
      },
      "source": [
        "## Load additional data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wvdGxO9EKzuf"
      },
      "source": [
        "### [Risk factors for increased COVID-19 case-fatality in the United States: A county-level analysis during the first wave](https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0258308)\n",
        "\n",
        "GitHub [link](https://github.com/jmillar201/COVID19_CFR)\n",
        "\n",
        "Features are imported a csv file created with the custom script rds_file_to_csv.R (drop the column geometry that contains lists and convert the dataframe from an rds file to a csv file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "id": "Sl7bT75clBqy",
        "outputId": "b7b00701-d771-4e79-f378-87200771270d"
      },
      "outputs": [],
      "source": [
        "df_alt = pd.read_csv(\"Alt_data/county_indicators_2020-06-14.csv\", sep=\",\", engine=\"python\")\n",
        "df_alt = df_alt[~np.isnan(df_alt[\"FIPS\"])]  # drop counties without fips\n",
        "df_alt[\"FIPS\"] = df_alt[\"FIPS\"].apply(lambda x: str(int(x)))  # remove trailing zeros on fips\n",
        "df_alt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IfvfBubzsJDr"
      },
      "outputs": [],
      "source": [
        "# Add data: SVI (social vulnerability index), healthcare accessibility, comorbities\n",
        "\n",
        "# npi_keystone too many nan values\n",
        "columns_to_keep = [\"FIPS\", \"sv_groupquarterpop\",\n",
        "                   'sv_pdisability', 'sv_singleparent',\n",
        "                   'sv_penglish',\n",
        "                   'sv_pmultiunit', 'sv_pmobilehome', 'ses_ppoverty',\n",
        "                   'ses_punemployed',\n",
        "                   'hc_hospitals_per1000', 'hc_icubeds_per1000',\n",
        "                   'hc_icubeds_per60more1000', 'hc_icubeds_per65more1000',\n",
        "                   'hc_pnotinsured_acs',\n",
        "                   'hc_primarycare_per1000',\n",
        "                   'como_medicareheartdizprev', 'hc_medicaid',\n",
        "                   'como_pdiabetes', 'como_htn_hosp', 'como_htn_mort',\n",
        "                   'como_cvd_hosp',\n",
        "                   'como_cvd_mort', 'como_allheartdis_hosp', 'allheartdis_mort',\n",
        "                   'como_stroke_hosp', 'como_stroke_mort', 'como_smoking',\n",
        "                   'como_COPD', 'como_asthma', 'como_cancer5yr']\n",
        "df_alt = df_alt[columns_to_keep]\n",
        "df_alt.index = df_alt.FIPS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hZM2wf0dDL4V"
      },
      "source": [
        "## Clustering (to determine periods)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "446duRajYPAY"
      },
      "source": [
        "### Load timeseries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hJMCXF78YRCX"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(\"COVID-19_timeseries.csv\")\n",
        "df.index = df[\"Unnamed: 0\"]\n",
        "col = list(df.columns)\n",
        "col[0] = \"FIPS\"\n",
        "df.columns = col\n",
        "df.drop(columns=[\"FIPS\"], inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VsqisC7Q55CM"
      },
      "source": [
        "### February 2020 - February 2021: 3 periods clustering (old)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w_JTDQ6XOciv"
      },
      "outputs": [],
      "source": [
        "df.index = pd.to_datetime(df.index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ISEJ6rniPmd_",
        "outputId": "d8a5f7ef-b52f-4d96-fd57-2fc7da667a02"
      },
      "outputs": [],
      "source": [
        "A = df[df.index <= pd.to_datetime(\"2021-02-28\")]  # not transposed like in the article!\n",
        "A = A.fillna(0)  # let's suppose that no data = no death\n",
        "A_zscore = (A - A.mean(axis=0)) / A.std(axis=0)\n",
        "A_zscore"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Iih8CY1GQU7H",
        "outputId": "404e2494-eb40-4aac-d2ea-899b3c39b02e"
      },
      "outputs": [],
      "source": [
        "# Drop county with no values (only nan and or 0 death)\n",
        "county_to_drop = []\n",
        "for fips in list(A_zscore.columns):\n",
        "  if np.isnan(A_zscore[fips]).sum() == len(A_zscore):\n",
        "    county_to_drop.append(fips)\n",
        "print(\"{} counties dropped\".format(len(county_to_drop)))\n",
        "A_zscore.drop(columns=county_to_drop, inplace=True)\n",
        "A_zscore = A_zscore.T  # transpose the matrix: for clustering\n",
        "A_zscore"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SHJv86RBVYsc"
      },
      "source": [
        "Elbow plot (WCSS: sum of the distances between each county and its assigned cluster)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Y9k3LDwTjPW",
        "outputId": "726a7deb-da6b-459f-f96f-18252385ae0a"
      },
      "outputs": [],
      "source": [
        "from sklearn.cluster import KMeans\n",
        "\n",
        "wcss_list = []\n",
        "for k in range(1, 20):\n",
        "  kmeans = KMeans(n_clusters=k, random_state=42).fit(A_zscore)\n",
        "  wcss_list.append(kmeans.inertia_)\n",
        "plt.plot([i for i in range(1, 20)], wcss_list, \"o-\", color=\"blue\")\n",
        "plt.xlabel(\"Number of clusters\")\n",
        "plt.ylabel(\"Distance\")\n",
        "plt.title(\"Elbow plot for k-means clustering\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U3AEEKxHVxr5"
      },
      "source": [
        "Choose k=3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rrXiCR7JSzes",
        "outputId": "4f2a83d9-a291-483a-fdfa-61305c91b7a4"
      },
      "outputs": [],
      "source": [
        "from sklearn.cluster import KMeans\n",
        "kmeans = KMeans(n_clusters=3, random_state=0).fit(A_zscore)\n",
        "labels = kmeans.labels_\n",
        "labels_dic = {}\n",
        "for i in range(len(labels)):\n",
        "  labels_dic[A_zscore.index[i]] = labels[i]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ReSXK06OXZi6",
        "outputId": "cbebfc04-b5e4-4f65-eec2-2bcda7130295"
      },
      "outputs": [],
      "source": [
        "cluster_list = []\n",
        "for i in range(len(county_database)):\n",
        "  fips = county_database[\"FIPS\"].iloc[i]\n",
        "  try:\n",
        "    cluster = labels_dic[fips]\n",
        "  except Exception as e:\n",
        "    cluster = np.nan\n",
        "  cluster_list.append(cluster)\n",
        "county_database[\"cluster\"] = cluster_list\n",
        "county_database[\"cluster\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rBT6TEuUY6n1"
      },
      "outputs": [],
      "source": [
        "color_continuous_scale = \"agsunset\"\n",
        "label_col = \"cluster\"\n",
        "label_display = \"Counties by Cluster\"\n",
        "create_custom_choropleth(county_database, counties_json, label_col,\n",
        "                         label_display, color_continuous_scale)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w7fcwrojZY38"
      },
      "source": [
        "Representative Cluster COVID-19 Dynamics (deaths)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mNVsf_qOZYX5",
        "outputId": "3c3e1796-0cb8-4306-e1c0-b3289685b732"
      },
      "outputs": [],
      "source": [
        "dates = list(A_zscore.columns)\n",
        "color_list = [(79/255, 41/255, 146/255),\n",
        "              (235/255, 83/255, 134/255),\n",
        "              (237/255, 217/255, 163/255)]\n",
        "for i in range(kmeans.n_clusters):\n",
        "  plt.plot(dates, kmeans.cluster_centers_[i], c=color_list[i])\n",
        "\n",
        "plt.xlabel(\"Date\")\n",
        "plt.ylabel(\"Deaths (z-score)\")\n",
        "plt.title(\"Representative Cluster COVID-19 Dynamics (deaths)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### March 2021 - February 2022"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df.index = pd.to_datetime(df.index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "A = df[(df.index > pd.to_datetime(\"2021-02-28\")) & (df.index <= pd.to_datetime(\"2022-02-28\"))]  # not transposed like in the article!\n",
        "A = A.fillna(0)  # let's suppose that no data = no death\n",
        "A_zscore = (A - A.mean(axis=0)) / A.std(axis=0)\n",
        "A_zscore"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Drop county with no values (only nan and or 0 death)\n",
        "county_to_drop = []\n",
        "for fips in list(A_zscore.columns):\n",
        "  if np.isnan(A_zscore[fips]).sum() == len(A_zscore):\n",
        "    county_to_drop.append(fips)\n",
        "print(\"{} counties dropped\".format(len(county_to_drop)))\n",
        "A_zscore.drop(columns=county_to_drop, inplace=True)\n",
        "A_zscore = A_zscore.T  # transpose the matrix: for clustering\n",
        "A_zscore"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.cluster import KMeans\n",
        "\n",
        "wcss_list = []\n",
        "for k in range(1, 20):\n",
        "  kmeans = KMeans(n_clusters=k, random_state=42).fit(A_zscore)\n",
        "  wcss_list.append(kmeans.inertia_)\n",
        "plt.plot([i for i in range(1, 20)], wcss_list, \"o-\", color=\"blue\")\n",
        "plt.xlabel(\"Number of clusters\")\n",
        "plt.ylabel(\"Distance\")\n",
        "plt.title(\"Elbow plot for k-means clustering\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.cluster import KMeans\n",
        "kmeans = KMeans(n_clusters=3, random_state=0).fit(A_zscore)\n",
        "labels = kmeans.labels_\n",
        "labels_dic = {}\n",
        "for i in range(len(labels)):\n",
        "  labels_dic[A_zscore.index[i]] = labels[i]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dates = list(A_zscore.columns)\n",
        "color_list = [(79/255, 41/255, 146/255),\n",
        "              (235/255, 83/255, 134/255),\n",
        "              (237/255, 217/255, 163/255)]\n",
        "for i in range(kmeans.n_clusters):\n",
        "  plt.plot(dates, kmeans.cluster_centers_[i], c=color_list[i])\n",
        "\n",
        "plt.xlabel(\"Date\")\n",
        "plt.ylabel(\"Deaths (z-score)\")\n",
        "plt.title(\"Representative Cluster COVID-19 Dynamics (deaths)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DgnQP45t6BNf"
      },
      "source": [
        "### February 2020 - February 2022"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 661
        },
        "id": "PE6TXd-S6Dlv",
        "outputId": "7def8eae-799b-4eac-f7aa-a57dadfb0a76"
      },
      "outputs": [],
      "source": [
        "df.index = pd.to_datetime(df.index)\n",
        "A = df[df.index <= pd.to_datetime(\"2022-02-28\")]  # not transposed like in the article!\n",
        "A = A.fillna(0)  # let's suppose that no data = no death\n",
        "A_zscore = (A - A.mean(axis=0)) / A.std(axis=0)\n",
        "A_zscore"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 554
        },
        "id": "S2IxVW7Q6Sba",
        "outputId": "e88b1522-73b9-430b-d974-963018cf66f8"
      },
      "outputs": [],
      "source": [
        "# Drop county with no values (only nan and or 0 death)\n",
        "county_to_drop = []\n",
        "for fips in list(A_zscore.columns):\n",
        "  if np.isnan(A_zscore[fips]).sum() == len(A_zscore):\n",
        "    county_to_drop.append(fips)\n",
        "print(\"{} counties dropped\".format(len(county_to_drop)))\n",
        "A_zscore.drop(columns=county_to_drop, inplace=True)\n",
        "A_zscore = A_zscore.T  # transpose the matrix: for clustering\n",
        "A_zscore"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "DI9bxu9A6Upv",
        "outputId": "12aee782-aa8d-4887-d5c5-409a58e5d59c"
      },
      "outputs": [],
      "source": [
        "from sklearn.cluster import KMeans\n",
        "\n",
        "wcss_list = []\n",
        "for k in range(1, 20):\n",
        "  kmeans = KMeans(n_clusters=k, random_state=42).fit(A_zscore)\n",
        "  wcss_list.append(kmeans.inertia_)\n",
        "plt.plot([i for i in range(1, 20)], wcss_list, \"o-\", color=\"blue\")\n",
        "plt.xlabel(\"Number of clusters\")\n",
        "plt.ylabel(\"Distance\")\n",
        "plt.title(\"Elbow plot for k-means clustering\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1V-3jFAl8Grb"
      },
      "source": [
        "Let's choose k=3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P6YQlJps-JBz",
        "outputId": "3d18be48-45d8-417c-8461-d343fd4b7342"
      },
      "outputs": [],
      "source": [
        "n_clusters = 3\n",
        "\n",
        "from sklearn.cluster import KMeans\n",
        "kmeans = KMeans(n_clusters=n_clusters, random_state=0).fit(A_zscore)\n",
        "labels = kmeans.labels_\n",
        "labels_dic = {}\n",
        "for i in range(len(labels)):\n",
        "  labels_dic[int(A_zscore.index[i])] = labels[i]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "id": "blsIuyz66Xzp",
        "outputId": "76126899-6173-47ef-b74f-24463d7f489c"
      },
      "outputs": [],
      "source": [
        "cluster_list = []\n",
        "for i in range(len(county_database)):\n",
        "  fips = county_database[\"FIPS\"].iloc[i]\n",
        "  try:\n",
        "    cluster = labels_dic[int(fips)]\n",
        "  except Exception as e:\n",
        "    cluster = np.nan\n",
        "  cluster_list.append(cluster)\n",
        "county_database[\"cluster\"] = cluster_list\n",
        "plt.hist(county_database[\"cluster\"])\n",
        "plt.title(\"Histogram of the number of counties per cluster\")\n",
        "plt.xlabel(\"Cluster\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "pQBdbU-x6ZkH",
        "outputId": "59e09542-87ec-4c18-c748-1573c96b3fe0"
      },
      "outputs": [],
      "source": [
        "# create_custom_choropleth is defined in the section Plot, subsection Initialization\n",
        "\n",
        "color_continuous_scale = \"agsunset\"\n",
        "label_col = \"cluster\"\n",
        "label_display = \"Counties by Cluster\"\n",
        "create_custom_choropleth(county_database, counties_json, label_col,\n",
        "                         label_display, color_continuous_scale)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 516
        },
        "id": "7T8AKtas6bMZ",
        "outputId": "7c6847bd-a4a4-4ee7-ba42-ea9b69a7a556"
      },
      "outputs": [],
      "source": [
        "dates = list(A_zscore.columns)\n",
        "nb_clusters = kmeans.n_clusters\n",
        "color_list = [(((nb_clusters - 1 - k) * 79 + k * 237)/((nb_clusters - 1) * 255),\n",
        "               ((nb_clusters - 1 - k) * 41 + k * 217)/((nb_clusters - 1) * 255),\n",
        "               ((nb_clusters - 1 - k) * 146 + k * 163)/((nb_clusters - 1) * 255)) for k in range(nb_clusters)]\n",
        "fig = plt.figure(figsize=(24, 10))\n",
        "for i in range(nb_clusters):\n",
        "  plt.plot(dates, kmeans.cluster_centers_[i], c=color_list[i])\n",
        "\n",
        "plt.xlabel(\"Date\")\n",
        "plt.ylabel(\"Deaths (z-score)\")\n",
        "plt.title(\"Representative Cluster COVID-19 Dynamics (deaths)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_W4zrgeYVlsi"
      },
      "source": [
        "### February 2020 - February 2022 (DBSCAN)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bX6UasAxVpYj"
      },
      "outputs": [],
      "source": [
        "df.index = pd.to_datetime(df.index)\n",
        "A = df[df.index <= pd.to_datetime(\"2022-02-28\")]  # not transposed like in the article!\n",
        "A = A.fillna(0)  # let's suppose that no data = no death\n",
        "A_zscore = (A - A.mean(axis=0)) / A.std(axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2QeChNX9VtVb",
        "outputId": "3d79cdbd-669c-4a7f-ac2c-ed12c28e6d3c"
      },
      "outputs": [],
      "source": [
        "# Drop county with no values (only nan and or 0 death)\n",
        "county_to_drop = []\n",
        "for fips in list(A_zscore.columns):\n",
        "  if np.isnan(A_zscore[fips]).sum() == len(A_zscore):\n",
        "    county_to_drop.append(fips)\n",
        "print(\"{} counties dropped\".format(len(county_to_drop)))\n",
        "A_zscore.drop(columns=county_to_drop, inplace=True)\n",
        "A_zscore = A_zscore.T  # transpose the matrix: for clustering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LG5b_05aVxZ2"
      },
      "outputs": [],
      "source": [
        "from sklearn.cluster import DBSCAN\n",
        "\n",
        "nb_tot_clusters = []\n",
        "not_clustered = []\n",
        "for eps in np.linspace(0.0001, 5, 3):\n",
        "  dbscan = DBSCAN(eps=eps, min_samples=10, n_jobs=-1).fit(A_zscore)\n",
        "  nb_tot_clusters.append(len(np.unique(dbscan.labels_)) - 1)\n",
        "  not_clustered.append(100 * np.count_nonzero(dbscan.labels_==-1) / len(dbscan.labels_))\n",
        "\n",
        "# plot\n",
        "fig, axs = plt.subplots(2)\n",
        "fig.suptitle(\"Hyperparameters tuning plot for DBSCAN clustering\")\n",
        "axs[0].plot(np.linspace(0.0001, 5, 100), nb_tot_clusters, \"o-\", color=\"blue\")\n",
        "axs[0].set(xlabel=\"Epsilon\", ylabel=\"Number of clusters\")\n",
        "axs[1].plot(np.linspace(0.0001, 5, 100), not_clustered, \"o-\", color=\"blue\")\n",
        "axs[1].set(xlabel=\"Epsilon\", ylabel=\"Points not clustered (%)\")\n",
        "\n",
        "# Zoom for eps between 3 and 4\n",
        "nb_tot_clusters = []\n",
        "not_clustered = []\n",
        "for eps in np.linspace(3, 4, 100):\n",
        "  dbscan = DBSCAN(eps=eps, min_samples=10, n_jobs=-1).fit(A_zscore)\n",
        "  nb_tot_clusters.append(len(np.unique(dbscan.labels_)) - 1)\n",
        "  not_clustered.append(100 * np.count_nonzero(dbscan.labels_==-1) / len(dbscan.labels_))\n",
        "\n",
        "# plot\n",
        "fig, axs = plt.subplots(2)\n",
        "fig.suptitle(\"Hyperparameters tuning plot for DBSCAN clustering\")\n",
        "axs[0].plot(np.linspace(3, 4, 100), nb_tot_clusters, \"o-\", color=\"blue\")\n",
        "axs[0].set(xlabel=\"Epsilon\", ylabel=\"Number of clusters\")\n",
        "axs[1].plot(np.linspace(3, 4, 100), not_clustered, \"o-\", color=\"blue\")\n",
        "axs[1].set(xlabel=\"Epsilon\", ylabel=\"Points not clustered (%)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pNGOOPoRMosX"
      },
      "source": [
        "Let's choose $\\epsilon = 3.3$ to obtain 3 clusters with less than 15% of unlabeled counties"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oAZqOXuKV0jF"
      },
      "outputs": [],
      "source": [
        "from sklearn.cluster import DBSCAN\n",
        "\n",
        "eps = 3.3\n",
        "dbscan = DBSCAN(eps=eps, min_samples=10, n_jobs=-1).fit(A_zscore)\n",
        "labels = dbscan.labels_\n",
        "labels_dic = {}\n",
        "for i in range(len(labels)):\n",
        "  labels_dic[int(A_zscore.index[i])] = labels[i]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "50LFga3-V3pk"
      },
      "outputs": [],
      "source": [
        "cluster_list = []\n",
        "for i in range(len(county_database)):\n",
        "  fips = county_database[\"FIPS\"].iloc[i]\n",
        "  try:\n",
        "    cluster = labels_dic[fips]\n",
        "  except Exception as e:\n",
        "    cluster = np.nan\n",
        "  cluster_list.append(cluster)\n",
        "county_database[\"cluster_dbscan\"] = cluster_list\n",
        "plt.hist(county_database[\"cluster_dbscan\"])\n",
        "plt.title(\"Histogram of the number of counties per cluster\")\n",
        "plt.xlabel(\"Cluster\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QEvrCDSIV3xH"
      },
      "outputs": [],
      "source": [
        "color_continuous_scale = \"agsunset\"\n",
        "label_col = \"cluster_dbscan\"\n",
        "label_display = \"Counties by Cluster\"\n",
        "create_custom_choropleth(county_database, counties_json, label_col,\n",
        "                         label_display, color_continuous_scale)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0rb6P0RjV8oI"
      },
      "outputs": [],
      "source": [
        "dates = list(A_zscore.columns)\n",
        "nb_clusters = dbscan.n_clusters\n",
        "color_list = [(((nb_clusters - 1 - k) * 79 + k * 237)/((nb_clusters - 1) * 255),\n",
        "               ((nb_clusters - 1 - k) * 41 + k * 217)/((nb_clusters - 1) * 255),\n",
        "               ((nb_clusters - 1 - k) * 146 + k * 163)/((nb_clusters - 1) * 255)) for k in range(nb_clusters)]\n",
        "fig = plt.figure(figsize=(24, 10))\n",
        "for i in range(nb_clusters):\n",
        "  plt.plot(dates, kmeans.cluster_centers_[i], c=color_list[i])\n",
        "\n",
        "plt.xlabel(\"Date\")\n",
        "plt.ylabel(\"Deaths (z-score)\")\n",
        "plt.title(\"Representative Cluster COVID-19 Dynamics (deaths)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4_HDbp-le_hQ"
      },
      "source": [
        "### February 2020 - February 2022 (DTW)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mr7Jz5Lxfw3X",
        "outputId": "40496cba-da1b-485f-c412-8180d1a2d122"
      },
      "outputs": [],
      "source": [
        "df.index = pd.to_datetime(df.index)\n",
        "A = df[df.index <= pd.to_datetime(\"2022-02-28\")]  # not transposed like in the article!\n",
        "A = A.fillna(0)  # let's suppose that no data = no death\n",
        "A_zscore = (A - A.mean(axis=0)) / A.std(axis=0)\n",
        "\n",
        "# Drop county with no values (only nan and or 0 death)\n",
        "county_to_drop = []\n",
        "for fips in list(A_zscore.columns):\n",
        "  if np.isnan(A_zscore[fips]).sum() == len(A_zscore):\n",
        "    county_to_drop.append(fips)\n",
        "print(\"{} counties dropped\".format(len(county_to_drop)))\n",
        "A_zscore.drop(columns=county_to_drop, inplace=True)\n",
        "A_zscore = A_zscore.T  # transpose the matrix: for clustering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gGyeJqYDN5cV"
      },
      "outputs": [],
      "source": [
        "A_zscore_save = A_zscore.copy()\n",
        "A_zscore = A_zscore[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lOKANTV0fGou"
      },
      "outputs": [],
      "source": [
        "from tslearn.clustering import TimeSeriesKMeans\n",
        "\n",
        "wcss_list = []\n",
        "for k in range(1, 20):\n",
        "  kmeans = TimeSeriesKMeans(n_clusters=3, metric=\"dtw\",\n",
        "                            max_iter=10, random_state=42).fit(A_zscore)\n",
        "  wcss_list.append(kmeans.inertia_)\n",
        "plt.plot([i for i in range(1, 20)], wcss_list, \"o-\", color=\"blue\")\n",
        "plt.xlabel(\"Number of clusters\")\n",
        "plt.ylabel(\"Distance\")\n",
        "plt.title(\"Elbow plot for timeseries k-means clustering with DTW\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8zbXVvMfhCBE"
      },
      "outputs": [],
      "source": [
        "n_clusters = 3\n",
        "\n",
        "from tslearn.clustering import TimeSeriesKMeans\n",
        "kmeans = TimeSeriesKMeans(n_clusters=n_clusters, metric=\"dtw\",\n",
        "                          max_iter=10, random_state=42).fit(A_zscore)\n",
        "labels = kmeans.labels_\n",
        "labels_dic = {}\n",
        "for i in range(len(labels)):\n",
        "  labels_dic[int(A_zscore.index[i])] = labels[i]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "id": "D6HkSgpUhIUr",
        "outputId": "d1051305-8252-46fe-886b-b588d36da93a"
      },
      "outputs": [],
      "source": [
        "cluster_list = []\n",
        "for i in range(len(county_database)):\n",
        "  fips = county_database[\"FIPS\"].iloc[i]\n",
        "  try:\n",
        "    cluster = labels_dic[fips]\n",
        "  except Exception as e:\n",
        "    cluster = np.nan\n",
        "  cluster_list.append(cluster)\n",
        "county_database[\"cluster_dtw\"] = cluster_list\n",
        "plt.hist(county_database[\"cluster_dtw\"])\n",
        "plt.title(\"Histogram of the number of counties per cluster\")\n",
        "plt.xlabel(\"Cluster\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "qnLtZvQChLEj",
        "outputId": "4ffa6ef1-1023-4268-d7dc-ef812947885d"
      },
      "outputs": [],
      "source": [
        "color_continuous_scale = \"agsunset\"\n",
        "label_col = \"cluster_dtw\"\n",
        "label_display = \"Counties by Cluster\"\n",
        "create_custom_choropleth(county_database, counties_json, label_col,\n",
        "                         label_display, color_continuous_scale)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HJ5G5GRxhQXr"
      },
      "outputs": [],
      "source": [
        "dates = list(A_zscore.columns)\n",
        "nb_clusters = kmeans.n_clusters\n",
        "color_list = [(((nb_clusters - 1 - k) * 79 + k * 237)/((nb_clusters - 1) * 255),\n",
        "               ((nb_clusters - 1 - k) * 41 + k * 217)/((nb_clusters - 1) * 255),\n",
        "               ((nb_clusters - 1 - k) * 146 + k * 163)/((nb_clusters - 1) * 255)) for k in range(nb_clusters)]\n",
        "fig = plt.figure(figsize=(24, 10))\n",
        "for i in range(nb_clusters):\n",
        "  plt.plot(dates, kmeans.cluster_centers_[i], c=color_list[i])\n",
        "\n",
        "plt.xlabel(\"Date\")\n",
        "plt.ylabel(\"Deaths (z-score)\")\n",
        "plt.title(\"Representative Cluster COVID-19 Dynamics (deaths)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kFvGixXPeEAc"
      },
      "source": [
        "## Add death counts and crude death rates per period (per 100k residents)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G6htnVroeINI"
      },
      "outputs": [],
      "source": [
        "df.index = pd.to_datetime(df.index)\n",
        "period1 = df[df.index <= pd.to_datetime(\"2020-05-31\")]\n",
        "period2 = df[df.index <= pd.to_datetime(\"2020-10-31\")]\n",
        "period2 = period2[period2.index > pd.to_datetime(\"2020-05-31\")]\n",
        "period3 = df[df.index > pd.to_datetime(\"2020-10-31\")]\n",
        "period3 = period3[period3.index <= pd.to_datetime(\"2021-02-28\")]\n",
        "period4 = df[df.index > pd.to_datetime(\"2021-02-28\")]\n",
        "period4 = period4[period4.index <= pd.to_datetime(\"2021-07-31\")]\n",
        "period5 = df[df.index > pd.to_datetime(\"2021-07-31\")]\n",
        "period5 = period5[period5.index <= pd.to_datetime(\"2021-10-31\")]\n",
        "period6 = df[df.index > pd.to_datetime(\"2021-10-31\")]\n",
        "period6 = period6[period6.index <= pd.to_datetime(\"2022-02-28\")]\n",
        "\n",
        "# death rates (covid-19 related)\n",
        "deathCount1_l = []\n",
        "deathCount2_l = []\n",
        "deathCount3_l = []\n",
        "deathCount4_l = []\n",
        "deathCount5_l = []\n",
        "deathCount6_l = []\n",
        "for i in range(len(county_database)):\n",
        "  fips = county_database[\"FIPS\"].iloc[i]\n",
        "  deathCount1_l.append(period1[fips].iloc[-1])\n",
        "  deathCount2_l.append(period2[fips].iloc[-1])\n",
        "  deathCount3_l.append(period3[fips].iloc[-1])\n",
        "  deathCount4_l.append(period4[fips].iloc[-1])\n",
        "  deathCount5_l.append(period5[fips].iloc[-1])\n",
        "  deathCount6_l.append(period6[fips].iloc[-1])\n",
        "factor_increase = 100_000  # per 100k residents\n",
        "county_database[\"deathCount_period1\"] = deathCount1_l\n",
        "county_database[\"deathRate_period1\"] = county_database[\"deathCount_period1\"] * factor_increase / county_database[\"total_pop\"]\n",
        "county_database[\"deathCount_period2\"] = deathCount2_l\n",
        "county_database[\"deathRate_period2\"] = county_database[\"deathCount_period2\"] * factor_increase / county_database[\"total_pop\"]\n",
        "county_database[\"deathCount_period3\"] = deathCount3_l\n",
        "county_database[\"deathRate_period3\"] = county_database[\"deathCount_period3\"] * factor_increase / county_database[\"total_pop\"]\n",
        "county_database[\"deathCount_period4\"] = deathCount4_l\n",
        "county_database[\"deathRate_period4\"] = county_database[\"deathCount_period4\"] * factor_increase / county_database[\"total_pop\"]\n",
        "county_database[\"deathCount_period5\"] = deathCount5_l\n",
        "county_database[\"deathRate_period5\"] = county_database[\"deathCount_period5\"] * factor_increase / county_database[\"total_pop\"]\n",
        "county_database[\"deathCount_period6\"] = deathCount6_l\n",
        "county_database[\"deathRate_period6\"] = county_database[\"deathCount_period6\"] * factor_increase / county_database[\"total_pop\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UAVt7HQmYWnq"
      },
      "source": [
        "## Create county database 2 (with the additional data) and the imputed version at the state level version of county database 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UcKG9VkF4R6D"
      },
      "source": [
        "File Data-Cleaning_2020-06-14.html useful & website [socialexplorer](https://www.socialexplorer.com/data/ACS2015/metadata/?ds=ACS15&var=B25024010)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X30DT2ZCjbM9",
        "outputId": "ab1717fc-fb10-43b9-fb90-99cd75e892d0"
      },
      "outputs": [],
      "source": [
        "county_database[\"FIPS\"] = county_database[\"FIPS\"].apply(lambda x: int(x))\n",
        "county_database.index = county_database[\"FIPS\"]\n",
        "df_alt[\"FIPS\"] = df_alt[\"FIPS\"].apply(lambda x: int(x))\n",
        "df_alt.index = df_alt[\"FIPS\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XXsikvEmYTu2"
      },
      "outputs": [],
      "source": [
        "county_database.index = county_database.FIPS\n",
        "if \"FIPS.1\" in county_database.columns:\n",
        "  county_database.drop(columns=[\"FIPS.1\"], inplace=True)\n",
        "county_database2 = county_database.join(df_alt.drop(columns=[\"FIPS\"]))\n",
        "# county_database2.describe().loc[\"count\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ukJFsK4XW1KG"
      },
      "outputs": [],
      "source": [
        "for c in ['como_medicareheartdizprev', 'hc_medicaid',\n",
        "          'como_pdiabetes', 'como_htn_hosp', 'como_htn_mort',\n",
        "          'como_cvd_hosp', 'como_cvd_mort', 'como_allheartdis_hosp',\n",
        "          'allheartdis_mort', 'como_stroke_hosp', 'como_stroke_mort',\n",
        "          'como_smoking', 'como_COPD', 'como_asthma', 'como_cancer5yr',\n",
        "          \"sv_groupquarterpop\"]:\n",
        "  county_database2[c] = county_database2[c] / county_database2[\"total_pop\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VGjieN1tmYgF"
      },
      "source": [
        "### Impute data at the state level"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wSeyBeonnUf3"
      },
      "outputs": [],
      "source": [
        "for c in county_database2.columns:\n",
        "  county_database2[c] = county_database2[c].replace(np.inf, np.nan)\n",
        "  county_database2[c] = county_database2[c].replace(-np.inf, np.nan)\n",
        "if \"FIPS.1\" in county_database2.columns:\n",
        "  county_database2.drop(columns=[\"FIPS.1\"], inplace=True)\n",
        "columns_to_fill = list(county_database2.columns)\n",
        "for c in [\"FIPS\", \"Province_State\", \"closest_airport\", \"county_name\",\n",
        "          \"acp_name\", \"acp_name_with_number\"]:\n",
        "  columns_to_fill.remove(c)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "p_ycrtWQoNuX",
        "outputId": "33072d2c-bff0-4109-dbc8-114b5343b8b3"
      },
      "outputs": [],
      "source": [
        "# fit an imputer per state\n",
        "from sklearn.impute import SimpleImputer\n",
        "county_database2_imputed = county_database2.copy()\n",
        "for state in np.unique(county_database2_imputed[\"Province_State\"]):\n",
        "  statedf = county_database2_imputed[county_database2_imputed[\"Province_State\"] == state]  # state sub dataframe\n",
        "  columns_to_fill_state = []\n",
        "  for x in columns_to_fill:\n",
        "    if len(statedf[x].dropna()) != 0:  # at least one value in this column to fit an imputer\n",
        "      columns_to_fill_state.append(x)\n",
        "  statedf_to_fill = statedf[columns_to_fill_state]  # state sub dataframe with the columns to fill\n",
        "  impute = SimpleImputer(strategy='mean')\n",
        "  statedf_to_fill = pd.DataFrame(impute.fit_transform(statedf_to_fill),\n",
        "                                 columns=statedf_to_fill.columns,\n",
        "                                 index=statedf_to_fill.index)\n",
        "  statedf[columns_to_fill_state] = statedf_to_fill  # replace the filled columns\n",
        "  county_database2_imputed[county_database2_imputed[\"Province_State\"] == state] = statedf  # update the database for this state\n",
        "county_database2_imputed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0diFzUACbJuI"
      },
      "source": [
        "## Save the datasets (for this response variable)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9v5TzWsgbb7V"
      },
      "outputs": [],
      "source": [
        "# TO EDIT: in function of the chosen response variable\n",
        "type_data_filename =  # covid19, all_causes, excess_mortality"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p1EFzLnB4uqs"
      },
      "outputs": [],
      "source": [
        "county_database.to_csv(f\"county_database_{type_data_filename}.csv\")\n",
        "county_database2.to_csv(f\"county_database2_{type_data_filename}.csv\")\n",
        "county_database2_imputed.to_csv(f\"county_database2_imputed_{type_data_filename}.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HCT3K136fytF"
      },
      "source": [
        "## Correlations & feature selection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rOVrYxoVirHT"
      },
      "source": [
        "We choose a correlation threshold equal to 0.7 (assumption that a correlation greater than 0.7 is too strong)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ooPzibgagHhF",
        "outputId": "19a492ed-b35d-4769-84da-07cb7d060ba5"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "\n",
        "\n",
        "correlation_threshold = 0.7\n",
        "\n",
        "def correlation_feature_selection(period, county_database2_imputed):\n",
        "\n",
        "  numerics = [\"int16\", \"int32\", \"int64\", \"float16\", \"float32\", \"float64\"]\n",
        "  dfcorr = county_database2_imputed.select_dtypes(include=numerics)\n",
        "  # drop features that will be kept whether are not they are correlated to another one\n",
        "  columns_to_drop = [\"FIPS\", \"Lat\", \"Long_\", \"acp\", \"HHS Region\", \"total_pop\",\n",
        "                     \"county_area\"]\n",
        "  # drop stringency index of period in the future\n",
        "  stringency_columns = [f\"StringencyIndex{i}_{s}\" for i in range(period+1, 7) for s in [\"mean\", \"std\", \"median\", \"min\", \"max\"]]\n",
        "  columns_to_drop = columns_to_drop + stringency_columns\n",
        "\n",
        "  # drop wastewater of period in the future\n",
        "  biobot_columns = [f\"ecra{i}_{s}\" for i in range(period+1, 7) for s in [\"mean\", \"std\", \"median\", \"min\", \"max\"]]\n",
        "  columns_to_drop = columns_to_drop + biobot_columns\n",
        "\n",
        "  if period != 1:  # drop distance to airport\n",
        "    columns_to_drop = columns_to_drop + [\"min_distance_top_airport\"]\n",
        "\n",
        "  for per in range(1, 7):\n",
        "    columns_to_drop.append(f\"deathCount_period{per}\")\n",
        "    columns_to_drop.append(f\"deathRate_period{per}\")\n",
        "  dfcorr.drop(columns=columns_to_drop, inplace=True)\n",
        "  dfcorr = dfcorr.corr()\n",
        "\n",
        "  columns = np.full((dfcorr.shape[0],), True, dtype=bool)\n",
        "  for i in range(dfcorr.shape[0]):\n",
        "    if columns[i]:\n",
        "      correlated_to_i = [i]\n",
        "      for j in range(i+1, dfcorr.shape[0]):\n",
        "          if columns[j] and (abs(dfcorr.iloc[i,j]) >= correlation_threshold):\n",
        "            correlated_to_i.append(j)\n",
        "      if len(correlated_to_i) > 1:  # at least one correlated variable\n",
        "        bloc = []\n",
        "        for k in correlated_to_i:\n",
        "          corr = 0\n",
        "          for l in correlated_to_i:\n",
        "            corr += abs(dfcorr.iloc[k,l])\n",
        "          corr = corr / len(correlated_to_i)\n",
        "          bloc.append([k, corr])\n",
        "        bloc.sort(key=lambda x: x[1])  # we select the feature which is on average, more correlated to the other features in the group\n",
        "        feature_within_bloc_to_keep = bloc.pop()[0]\n",
        "        for k, _ in bloc:\n",
        "          columns[k] = False\n",
        "\n",
        "  X_selected_columns = list(dfcorr.columns[columns])\n",
        "  # Add geographical features\n",
        "  X_selected_columns.append(\"acp\")\n",
        "  X_selected_columns.append(\"Lat\")\n",
        "  X_selected_columns.append(\"Long_\")\n",
        "  X_selected_columns.append(\"HHS Region\")\n",
        "\n",
        "  selected_columns = X_selected_columns[:]\n",
        "  # Add total population (after taking the log it will be the sample weight)\n",
        "  selected_columns.append(\"total_pop\")\n",
        "  # Add outcome variables\n",
        "  for per in range(1, 7):\n",
        "    selected_columns.append(f\"deathCount_period{per}\")\n",
        "    selected_columns.append(f\"deathRate_period{per}\")\n",
        "\n",
        "  return dfcorr, selected_columns, X_selected_columns\n",
        "\n",
        "def correlation_feature_selection_all_periods(county_database2_imputed):\n",
        "\n",
        "  numerics = [\"int16\", \"int32\", \"int64\", \"float16\", \"float32\", \"float64\"]\n",
        "  dfcorr = county_database2_imputed.select_dtypes(include=numerics)\n",
        "  # drop features that will be kept whether are not they are correlated to another one\n",
        "  columns_to_drop = [\"FIPS\", \"Lat\", \"Long_\", \"acp\", \"HHS Region\", \"total_pop\",\n",
        "                     \"county_area\"]\n",
        "  \n",
        "  # don't drop stringency index and min distance airport and biobot\n",
        "\n",
        "  for per in range(1, 7):\n",
        "    columns_to_drop.append(f\"deathCount_period{per}\")\n",
        "    columns_to_drop.append(f\"deathRate_period{per}\")\n",
        "  dfcorr.drop(columns=columns_to_drop, inplace=True)\n",
        "  dfcorr = dfcorr.corr()\n",
        "\n",
        "  columns = np.full((dfcorr.shape[0],), True, dtype=bool)\n",
        "  for i in range(dfcorr.shape[0]):\n",
        "    if columns[i]:\n",
        "      correlated_to_i = [i]\n",
        "      for j in range(i+1, dfcorr.shape[0]):\n",
        "          if columns[j] and (abs(dfcorr.iloc[i,j]) >= correlation_threshold):\n",
        "            correlated_to_i.append(j)\n",
        "      if len(correlated_to_i) > 1:  # at least one correlated variable\n",
        "        bloc = []\n",
        "        for k in correlated_to_i:\n",
        "          corr = 0\n",
        "          for l in correlated_to_i:\n",
        "            corr += abs(dfcorr.iloc[k,l])\n",
        "          corr = corr / len(correlated_to_i)\n",
        "          bloc.append([k, corr])\n",
        "        bloc.sort(key=lambda x: x[1])  # we select the feature which is on average, more correlated to the other features in the group\n",
        "        feature_within_bloc_to_keep = bloc.pop()[0]\n",
        "        for k, _ in bloc:\n",
        "          columns[k] = False\n",
        "\n",
        "  X_selected_columns = list(dfcorr.columns[columns])\n",
        "  # Add geographical features\n",
        "  X_selected_columns.append(\"acp\")\n",
        "  X_selected_columns.append(\"Lat\")\n",
        "  X_selected_columns.append(\"Long_\")\n",
        "  X_selected_columns.append(\"HHS Region\")\n",
        "\n",
        "  selected_columns = X_selected_columns[:]\n",
        "  # Add total population (after taking the log it will be the sample weight)\n",
        "  selected_columns.append(\"total_pop\")\n",
        "  # Add outcome variables\n",
        "  for per in range(1, 7):\n",
        "    selected_columns.append(f\"deathCount_period{per}\")\n",
        "    selected_columns.append(f\"deathRate_period{per}\")\n",
        "\n",
        "  return dfcorr, selected_columns, X_selected_columns\n",
        "\n",
        "dfcorr_list, selected_columns_list, X_selected_columns_list = [], [], []\n",
        "for period in range(1, 7):\n",
        "  dfcorr, selected_columns, X_selected_columns = correlation_feature_selection(period,\n",
        "                                                                               county_database2_imputed)\n",
        "  dfcorr_list.append(dfcorr)\n",
        "  selected_columns_list.append(selected_columns)\n",
        "  X_selected_columns_list.append(X_selected_columns)\n",
        "\n",
        "dfcorr, selected_columns, X_selected_columns = correlation_feature_selection_all_periods(county_database2_imputed)\n",
        "dfcorr_list.append(dfcorr)\n",
        "selected_columns_list.append(selected_columns)\n",
        "X_selected_columns_list.append(X_selected_columns)\n",
        "\n",
        "with open(\"feature_selection\", \"wb\") as fp:  # save feature selection\n",
        "  features = [selected_columns_list, X_selected_columns_list]\n",
        "  pickle.dump(features, fp)\n",
        "\n",
        "# plot a correlation heatmap\n",
        "period = 1\n",
        "fig = plt.figure(figsize=(24, 24))\n",
        "sns.heatmap(dfcorr_list[period-1], cmap=\"RdBu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nx5UDfcaAgwW"
      },
      "source": [
        "# Dataset analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J7OkzwLFAjvg"
      },
      "source": [
        "## Pandas Profiling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7kkSji9gA-n7",
        "outputId": "057a4250-ddf7-47cc-8431-8294b848900c"
      },
      "outputs": [],
      "source": [
        "!pip install -U pandas-profiling\n",
        "!pip install -U pandas-profiling[notebook]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ZeMdDrN4hwo"
      },
      "outputs": [],
      "source": [
        "from pandas_profiling import ProfileReport\n",
        "# minimal=True for large datasets (remove computational intensive calculations such as correlation matrix, check duplicate rows etc)\n",
        "profile = ProfileReport(county_database.reset_index(drop=True, inplace=False),\n",
        "                        minimal=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81,
          "referenced_widgets": [
            "5cfc7a39f4d94040a211bcd815b1034d",
            "dde0ca8614fb4228aeb3c4838ce78fba",
            "4de42db77a83433598e325e29bfe77f0",
            "34500ae12cd049c389956a9a5c16f2d8",
            "3673ab82c5044152aa5dc0f3a0255682",
            "5551e6ab9399444b9caa3639855e712c",
            "09065aca8f7c48a6a801f3b760716e72",
            "4922e8013103495bbb55d4a5602eb56c",
            "d816f05c405f4a0ebc48e12d42454414",
            "59604904ed874df0a3f6885525cbe9b9",
            "6d4d4425bc7348ffa666258102753048",
            "430fc357f18a4903a71bde653692520a",
            "4a253363cbd14ccda67e8b64ebe6d597",
            "8b1d5c7847df4a9cb309a4782b6530c1",
            "e34011f650cb4c03bf8fcba7f5d38162",
            "d0cc4e6615e24326b7d735184873e2d4",
            "543c2ac1c0514dde8e32c0c9206ce789",
            "18724235591b450e9fcc0a436051516d",
            "aa8e805a1084497692b181a4ea2fa32c",
            "3bfd433a96c949dda7adc66394199a84",
            "8387e7edffc7440090e41a40a9783662",
            "61bdeafd837341079c12ff7260ba83e0"
          ]
        },
        "id": "Tg_0XKDE49t3",
        "outputId": "7444ac2c-0221-4f8d-be2f-6458f3b12848"
      },
      "outputs": [],
      "source": [
        "if not(os.path.exists(\"pandas-profiling\")):\n",
        "  os.mkdir(\"pandas-profiling\")\n",
        "profile.to_file(\"pandas-profiling/report.html\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "svYaVTmQKFe-"
      },
      "source": [
        "## Outliers & shapes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "list(county_database2.columns)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "44GtgkJ3MR-z"
      },
      "source": [
        "### Shapes of the different datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "39uQfNvZKKiB",
        "outputId": "9bac3107-38fa-4951-c310-fbb2120ac023"
      },
      "outputs": [],
      "source": [
        "county_database2.index = county_database2.FIPS\n",
        "dt = county_database2.copy(deep=True)\n",
        "# transform\n",
        "dt[\"total_pop\"] = np.log(dt[\"total_pop\"])\n",
        "print(\"Initial dataset shape: {}\".format(dt.shape))\n",
        "\n",
        "# drop nan values (we loose approximately 1/3 of the counties with missing data)\n",
        "dt.dropna(inplace=True)\n",
        "print(\"Dataset shape after nan values removed: {}\".format(dt.shape))\n",
        "\n",
        "county_database2_imputed.index = county_database2_imputed.FIPS\n",
        "dt = county_database2_imputed.copy(deep=True)\n",
        "# transform\n",
        "dt[\"total_pop\"] = np.log(dt[\"total_pop\"])\n",
        "\n",
        "print(\"Dataset shape after imputing data at the state level: {}\".format(dt.shape))\n",
        "dt.dropna(inplace=True)\n",
        "print(\"Same dataset after nan values removed: {}\".format(dt.shape))\n",
        "\n",
        "dt = county_database2_imputed.copy(deep=True)\n",
        "columns_to_drop = []\n",
        "for c in dt.columns:\n",
        "  if (\"ecra\" in c) or (c == \"political_leaning\"):\n",
        "    columns_to_drop.append(c)\n",
        "dt.drop(columns=columns_to_drop, inplace=True)\n",
        "print(\"Dataset shape after imputing data at the state level (without biobot data & political leaning): {}\".format(dt.shape))\n",
        "dt.dropna(inplace=True)\n",
        "print(\"Same dataset after nan values removed: {}\".format(dt.shape))\n",
        "\n",
        "dt = county_database2_imputed.copy(deep=True)\n",
        "columns_to_drop = []\n",
        "for c in dt.columns:\n",
        "  if \"ecra\" in c:\n",
        "    columns_to_drop.append(c)\n",
        "dt.drop(columns=columns_to_drop, inplace=True)\n",
        "print(\"Dataset shape after imputing data at the state level (without biobot data): {}\".format(dt.shape))\n",
        "dt.dropna(inplace=True)\n",
        "print(\"Same dataset after nan values removed: {}\".format(dt.shape))\n",
        "\n",
        "y1 = (dt[\"deathCount_period1\"] >= 5).astype(int)\n",
        "y2 = (dt[\"deathCount_period2\"] >= 5).astype(int)\n",
        "for i, v in enumerate(y1):\n",
        "  if v == 1:  # infected county period 1\n",
        "    y2.iloc[i] = 0\n",
        "y3 = (dt[\"deathCount_period3\"] >= 5).astype(int)\n",
        "for i, v in enumerate(y1):\n",
        "  if v == 1:  # infected county period 1\n",
        "    y3.iloc[i] = 0\n",
        "for i, v in enumerate(y2):\n",
        "  if v == 1:  # infected county period 2\n",
        "    y3.iloc[i] = 0\n",
        "dtNoIntroduced = dt[(dt[\"deathCount_period1\"] < 5) & (dt[\"deathCount_period2\"] < 5) & (dt[\"deathCount_period3\"] < 5)]\n",
        "print(\"\\n#####################\")\n",
        "print(\"Introduction models\")\n",
        "print(\"#####################\\n\")\n",
        "print(\"Dataset shape: {}\".format(dt.shape))\n",
        "print(\"Number of COVID-19 introductions in period 1: {} ({}%)\".format(y1.sum(),\n",
        "                                                                      round(100 * (y1.sum()) / len(dt), 2) ))\n",
        "print(\"Number of COVID-19 introductions in period 2: {} ({}%)\".format(y2.sum(),\n",
        "                                                                      round(100 * (y2.sum()) / len(dt), 2) ))\n",
        "print(\"Number of COVID-19 introductions in period 3: {} ({}%)\".format(y3.sum(),\n",
        "                                                                      round(100 * (y3.sum()) / len(dt), 2) ))\n",
        "print(\"Number of counties with less than 5 COVID-19 related death during period 1,2,3: {} ({}%)\".format(len(dtNoIntroduced),\n",
        "                                                                                                        round(100 * len(dtNoIntroduced) / len(dt), 2)))\n",
        "print(\"\\n#####################\")\n",
        "print(\"Virus spread models\")\n",
        "print(\"#####################\\n\")\n",
        "dt1 = dt[dt[\"deathCount_period1\"] >= 5]\n",
        "dt2 = dt[(dt[\"deathCount_period2\"] >= 5) | (dt[\"deathCount_period1\"] >= 5)]\n",
        "dt3 = dt[((dt[\"deathCount_period2\"] >= 5) | (dt[\"deathCount_period3\"] >= 5)) & (dt[\"deathCount_period1\"] < 5)]\n",
        "nodt = dt[(dt[\"deathCount_period1\"] < 5) & (dt[\"deathCount_period2\"] < 5) & (dt[\"deathCount_period3\"] < 5)]\n",
        "print(\"Number of counties considered for period 1 models: {} ({}%)\".format(len(dt1),\n",
        "                                                                           round(100 * len(dt1) / len(dt), 2) ))\n",
        "print(\"Number of counties considered for period 2 models: {} ({}%)\".format(len(dt2),\n",
        "                                                                           round(100 * len(dt2) / len(dt), 2) ))\n",
        "print(\"Number of counties considered for period 3 models: {} ({}%)\".format(len(dt3),\n",
        "                                                                           round(100 * len(dt3) / len(dt), 2) ))\n",
        "print(\"Number of counties never considered for virus spread models: {} ({}%)\".format(len(nodt),\n",
        "                                                                                     round(100 * len(nodt) / len(dt), 2) ))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X2763S7YPdIY"
      },
      "source": [
        "### Outliers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 394
        },
        "id": "X5EPdNOvUeRW",
        "outputId": "4f414400-088c-442f-dbec-93005a288ad6"
      },
      "outputs": [],
      "source": [
        "columns_to_check = ['min_distance_top_airport',\n",
        "                    'political_leaning', 'pch_white', 'pct_black',\n",
        "                    'pct_asian', 'pct_hispanic', 'pct_other', 'education',\n",
        "                    'under_19', 'over_65', 'income', 'hc',\n",
        "                    'total_jail_pop', 'nursing_population', 'obesity',\n",
        "                    'log_pop_density', 'log_crowding', 'pct_nursing',\n",
        "                    'pct_jail', 'sv_groupquarterpop', 'sv_pdisability',\n",
        "                    'sv_singleparent', 'sv_penglish',\n",
        "                    'sv_pmultiunit', 'sv_pmobilehome', 'ses_ppoverty',\n",
        "                    'ses_punemployed',\n",
        "                    'hc_hospitals_per1000', 'hc_icubeds_per1000',\n",
        "                    'hc_icubeds_per60more1000', 'hc_icubeds_per65more1000',\n",
        "                    'hc_pnotinsured_acs',\n",
        "                    'hc_primarycare_per1000', 'como_medicareheartdizprev',\n",
        "                    'hc_medicaid', 'como_pdiabetes',\n",
        "                    'como_htn_hosp', 'como_htn_mort', 'como_cvd_hosp',\n",
        "                    'como_cvd_mort', 'como_allheartdis_hosp',\n",
        "                    'allheartdis_mort', 'como_stroke_hosp', 'como_stroke_mort',\n",
        "                    'como_smoking', 'como_COPD', 'como_asthma']\n",
        "for per in range(1, 7):\n",
        "    for cat in [\"min\", \"median\", \"max\", \"mean\", \"std\"]:\n",
        "        columns_to_check.append(f\"StringencyIndex{per}_{cat}\")\n",
        "county_database2[columns_to_check].describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6zplFYVtm1c4"
      },
      "source": [
        "Outlier detection with 3 sigma. Doesn't make a lot of sense due to the high number of features being percentages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "02oHb1W6Vy9c",
        "outputId": "9759d065-e8c9-4c15-f91d-879b6cfc19c6"
      },
      "outputs": [],
      "source": [
        "county_database2[\"isOutlier\"] = len(county_database2) * [False]\n",
        "outliers_pairs = []\n",
        "for c in columns_to_check:\n",
        "  upper_boundary = county_database2[c].mean() + 3 * county_database2[c].std()\n",
        "  lower_boundary = county_database2[c].mean() - 3 * county_database2[c].std()\n",
        "  for i, v in enumerate(county_database2[c]):\n",
        "    if (v < lower_boundary) or (v > upper_boundary):\n",
        "      outliers_pairs.append([i, c, v, lower_boundary, upper_boundary])\n",
        "    county_database2[\"isOutlier\"].iloc[i] = county_database2[\"isOutlier\"].iloc[i] or (v < lower_boundary) or (v > upper_boundary)\n",
        "print(\"Number of outliers: {} ({}%)\".format(county_database2[\"isOutlier\"].sum(),\n",
        "                                            round(100 * (county_database2[\"isOutlier\"].sum() / len(county_database2)), 2)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IfmHAo1iYps9"
      },
      "source": [
        "## Fit exponentials first wave"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xkkgnE4GYtSa"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(\"COVID-19_timeseries.csv\")\n",
        "df.index = df[\"Unnamed: 0\"]\n",
        "col = list(df.columns)\n",
        "col[0] = \"FIPS\"\n",
        "df.columns = col\n",
        "df.drop(columns=[\"FIPS\"], inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gXu0KA0JYymJ"
      },
      "outputs": [],
      "source": [
        "df.index = pd.to_datetime(df.index)\n",
        "\n",
        "county_to_drop = []\n",
        "for fips in list(df.columns):\n",
        "  if (df[fips].isnull()).sum() == len(df):  # only nan\n",
        "    county_to_drop.append(fips)\n",
        "  if np.count_nonzero(df[fips] == 0) == len(df):  # only 0\n",
        "    county_to_drop.append(fips)\n",
        "print(\"{} counties dropped\".format(len(county_to_drop)))\n",
        "df.drop(columns=county_to_drop, inplace=True)\n",
        "df = df.fillna(0)  # let's suppose that no data = no death\n",
        "df = df.T\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ul_p_Dr5Y74b"
      },
      "outputs": [],
      "source": [
        "new = (df.T - df.T.shift(1))[1:]  # new death per day (not cumulative data)\n",
        "\n",
        "# drop counties with no death in the first period\n",
        "county_to_drop = []\n",
        "sub_new = new[new.index <= pd.to_datetime(\"2020-06-30\")]\n",
        "for fips in list(sub_new.columns):\n",
        "  if np.count_nonzero(sub_new[fips] == 0) == len(sub_new):  # no death\n",
        "    county_to_drop.append(fips)\n",
        "print(\"{} counties dropped\".format(len(county_to_drop)))\n",
        "new.drop(columns=county_to_drop, inplace=True)\n",
        "new = new.T\n",
        "new"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PVDrVQhyY9yV"
      },
      "source": [
        "Plot first 10 counties (before the end of june)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KJbKVL2aY_EP"
      },
      "outputs": [],
      "source": [
        "new[:10].T[new[:10].T.index <= pd.to_datetime(\"2020-06-30\")].plot()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p07WqDvcZAef"
      },
      "source": [
        "Let's select the maximum number of daily death before the end of June as a threshold to truncate (for each county) and fit an exponential on the cumulative number of deaths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rt3_0pzPZBtp"
      },
      "outputs": [],
      "source": [
        "max_id_list = []\n",
        "for c in list(new.index):\n",
        "  max_id = np.argmax(new.T[new.T.index <= pd.to_datetime(\"2020-06-30\")][c])\n",
        "  if max_id < 5:  # maximum peak of death too early, so let's say the first wave for this county is until the end of June\n",
        "    max_id = len(new)\n",
        "  max_id_list.append(max_id)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aWbHJFfcZDMs"
      },
      "source": [
        "$t_{0}=\\min_{t, x(t)\\neq 0}t$\n",
        "\n",
        "For all $t\\geq t_{0}$,\n",
        "\n",
        "$x(t+1)=x(t)e^{A+B x(t)}$\n",
        "\n",
        "Equivalent to: $y(t) = A+B x(t)$ where $y(t)=ln\\left (\\frac{x(t+1)}{x(t)}\\right )$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3T0Fjm3UZEg_"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "counties_list = list(new.index)\n",
        "fips_used_list = []\n",
        "coefficients = []\n",
        "reg_list = []\n",
        "x_list = []\n",
        "time_list = []\n",
        "for i, fips in enumerate(counties_list):\n",
        "  try:\n",
        "    x = df.loc[fips].to_numpy()\n",
        "    # truncate x\n",
        "    max_id = max_id_list[i]\n",
        "    min_id = np.argmin(x == 0)  # first day with a number of death != 0\n",
        "\n",
        "    if (max_id + 1 - min_id) >= 10:  # at least 10 days with deaths to fit a model\n",
        "\n",
        "      x = x[min_id:max_id+2]  # new is shifted by 1 so max_id too\n",
        "\n",
        "      y = np.log2(x[1:]/x[:-1])\n",
        "      x = x[:-1].reshape(-1, 1)\n",
        "\n",
        "      reg = LinearRegression()\n",
        "      reg.fit(x, y)\n",
        "      x_list.append(x)\n",
        "      time_list.append(list(df.columns)[min_id:max_id+1])\n",
        "      fips_used_list.append(fips)\n",
        "      reg_list.append(reg)\n",
        "      coefficients.append([reg.intercept_, reg.coef_[0]])  # [A, B]\n",
        "  except Exception as e:\n",
        "    pass\n",
        "coefficients = np.array(coefficients)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RBa2KGMbZGsb"
      },
      "source": [
        "Plot some exponential regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ejOZgPn6ZHz9"
      },
      "outputs": [],
      "source": [
        "fig, axs = plt.subplots(2, 3, figsize=(15, 12))\n",
        "for i in range(6):\n",
        "  x = x_list[i]\n",
        "  reg = reg_list[i]\n",
        "  fips = fips_used_list[i]\n",
        "  time_l = time_list[i]\n",
        "  A, B = coefficients[i]\n",
        "  y = reg.predict(x)\n",
        "  x = x.reshape(len(x))\n",
        "  y = x * np.exp(y)  # x(t+1) predictions\n",
        "  axs[i//3, i%3].plot(time_l[1:], x[1:], color=\"red\", label=f\"True {fips}\")\n",
        "  axs[i//3, i%3].plot(time_l[1:], y[:-1], color=\"black\", label=f\"Exp fit {fips}\", linestyle=\"dashed\")\n",
        "  axs[i//3, i%3].tick_params(labelrotation=45)\n",
        "  axs[i//3, i%3].legend(loc=\"best\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JfHqRAFEZJvB"
      },
      "outputs": [],
      "source": [
        "pol_list = []\n",
        "for fips in fips_used_list:\n",
        "  try:\n",
        "    pol = county_database2_imputed[county_database2_imputed[\"FIPS\"] == int(fips)][\"political_leaning\"].iloc[0]\n",
        "  except Exception as e:\n",
        "    pol = np.nan\n",
        "  pol_list.append(pol)\n",
        "plt.scatter(coefficients[:,0], coefficients[:,1], c=pol_list)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NTWI1J258oa1"
      },
      "source": [
        "# Plots"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZOzhJUcuzMHy"
      },
      "source": [
        "### Initialization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3uw4Rpm98sxN",
        "outputId": "08b82c21-7354-42d9-bc50-60b1751d3636"
      },
      "outputs": [],
      "source": [
        "import geopandas\n",
        "import shapely\n",
        "import shapefile\n",
        "import plotly\n",
        "from plotly.figure_factory._county_choropleth import create_choropleth\n",
        "\n",
        "colorscale = [\"#f7fbff\",\"#ebf3fb\",\"#deebf7\",\"#d2e3f3\",\"#c6dbef\",\"#b3d2e9\",\"#9ecae1\",\n",
        "              \"#85bcdb\",\"#6baed6\",\"#57a0ce\",\"#4292c6\",\"#3082be\",\"#2171b5\",\"#1361a9\",\n",
        "              \"#08519c\",\"#0b4083\",\"#08306b\"]\n",
        "fips = county_database[\"FIPS\"].tolist()\n",
        "\n",
        "selected_col_name = \"StringencyIndex1_mean\"\n",
        "title = \"USA by mean Stringency Index during period 1\"\n",
        "legend_title = \"stringency index period 1\"\n",
        "min_value = county_database[selected_col_name].min()\n",
        "max_value = county_database[selected_col_name].max()\n",
        "endpts = list(np.linspace(min_value, max_value, len(colorscale) - 1))\n",
        "\n",
        "values = county_database[selected_col_name].tolist()\n",
        "\n",
        "fig = create_choropleth(\n",
        "    fips=fips, values=values,\n",
        "    binning_endpoints=endpts,\n",
        "    colorscale=colorscale,\n",
        "    show_state_data=False,\n",
        "    show_hover=True, centroid_marker={'opacity': 0},\n",
        "    asp=2.9, title=title,\n",
        "    legend_title=legend_title\n",
        ")\n",
        "\n",
        "fig.layout.template = None\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YK47cke_r0Og"
      },
      "source": [
        "Create a function to easily display a feature on the U.S. map"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g-uQorj_pBNy"
      },
      "outputs": [],
      "source": [
        "import plotly.express as px\n",
        "from urllib.request import urlopen\n",
        "import json\n",
        "with urlopen('https://raw.githubusercontent.com/plotly/datasets/master/geojson-counties-fips.json') as response:\n",
        "    counties_json = json.load(response)\n",
        "# duplicate the county with no 0 at the beginning of the FIPS\n",
        "list_of_geo = counties_json[\"features\"]\n",
        "list_of_id = [v[\"id\"] for v in counties_json[\"features\"]]\n",
        "for cty in counties_json[\"features\"]:\n",
        "  new_id = str(int(cty[\"id\"]))\n",
        "  if not(new_id in list_of_id):\n",
        "    list_of_id.append(new_id)\n",
        "    new_cty = cty.copy()\n",
        "    new_cty[\"id\"] = new_id\n",
        "    list_of_geo.append(new_cty)\n",
        "counties_json[\"features\"] = list_of_geo\n",
        "\n",
        "def create_custom_choropleth(county_database, counties_json, label_col,\n",
        "                             label_display, color_continuous_scale=None,\n",
        "                             range_color=None):\n",
        "  if range_color is None:\n",
        "    range_color_min = county_database[label_col].min()\n",
        "    range_color_max = county_database[label_col].max()\n",
        "  else:\n",
        "    range_color_min = range_color[0]\n",
        "    range_color_max = range_color[1]\n",
        "  fig = px.choropleth(county_database, geojson=counties_json, locations=\"FIPS\",\n",
        "                      color=label_col,\n",
        "                      color_continuous_scale=color_continuous_scale,\n",
        "                      range_color=(range_color_min, range_color_max),\n",
        "                      scope=\"usa\",\n",
        "                      labels={label_col:label_display},\n",
        "                      title=\"USA by {}\".format(label_display)\n",
        "                      )\n",
        "  fig.update_layout(margin={\"r\":0,\"t\":0,\"l\":0,\"b\":0})\n",
        "  fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YFFTXMp1zRSI"
      },
      "source": [
        "### Stringency Index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GHAN1bhQr6EO"
      },
      "outputs": [],
      "source": [
        "color_continuous_scale = \"Viridis\"\n",
        "label_col = \"StringencyIndex1_mean\"\n",
        "label_display = \"Stringency Index (mean) Period 1\"\n",
        "create_custom_choropleth(county_database, counties_json, label_col,\n",
        "                         label_display, color_continuous_scale)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NQLCSsxPzT7X"
      },
      "source": [
        "### Political Leaning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZYXqCunXrB8Z"
      },
      "outputs": [],
      "source": [
        "color_continuous_scale = \"rdbu_r\"  # _r to reverse the color scale\n",
        "label_col = \"political_leaning\"\n",
        "label_display = \"Political Leaning\"\n",
        "create_custom_choropleth(county_database, counties_json, label_col,\n",
        "                         label_display, color_continuous_scale)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A1Opi2J_zaES"
      },
      "source": [
        "### American Communities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B6gXT5WHzEbR"
      },
      "outputs": [],
      "source": [
        "color_continuous_scale = \"rainbow\"\n",
        "label_col = \"acp\"\n",
        "label_display = \"American Communities\"\n",
        "create_custom_choropleth(county_database, counties_json, label_col,\n",
        "                         label_display, color_continuous_scale)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gt1AzsOv09bj"
      },
      "source": [
        "### Percent Hispanic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R_R3hThW0_qW"
      },
      "outputs": [],
      "source": [
        "color_continuous_scale = \"purples\"\n",
        "label_col = \"pct_hispanic\"\n",
        "label_display = \"Percent Hispanic\"\n",
        "range_color = (0, 32)\n",
        "create_custom_choropleth(county_database, counties_json, label_col,\n",
        "                         label_display, color_continuous_scale, range_color=range_color)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uHB9KyDO1BKH"
      },
      "source": [
        "### Percent Black"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iD4na_Og1CbP"
      },
      "outputs": [],
      "source": [
        "color_continuous_scale = \"oranges\"\n",
        "label_col = \"pct_black\"\n",
        "label_display = \"Percent Black\"\n",
        "range_color = (0, 39)\n",
        "create_custom_choropleth(county_database, counties_json, label_col,\n",
        "                         label_display, color_continuous_scale,\n",
        "                         range_color=range_color)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hA9CwK6szdwI"
      },
      "source": [
        "### Percent Obesity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JLDdG4eFzftW"
      },
      "outputs": [],
      "source": [
        "color_continuous_scale = \"teal\"\n",
        "label_col = \"obesity\"\n",
        "label_display = \"Percent Obesity\"\n",
        "create_custom_choropleth(county_database, counties_json, label_col,\n",
        "                         label_display, color_continuous_scale)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3_BClck81t_1"
      },
      "source": [
        "### Median Household Income"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "daF-Ma4I1vwA"
      },
      "outputs": [],
      "source": [
        "color_continuous_scale = \"blues\"\n",
        "label_col = \"income\"\n",
        "label_display = \"Median Household Income\"\n",
        "range_color = (20000, 75000)\n",
        "create_custom_choropleth(county_database, counties_json, label_col,\n",
        "                         label_display, color_continuous_scale,\n",
        "                         range_color=range_color)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2HrKnUeLzf64"
      },
      "source": [
        "### Population Density (log scale)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eWlqNlBdzjW0"
      },
      "outputs": [],
      "source": [
        "color_continuous_scale = \"brwnyl\"\n",
        "label_col = \"log_pop_density\"\n",
        "label_display = \"Population Density (log scale)\"\n",
        "range_color = (-1.338, 2.835)\n",
        "create_custom_choropleth(county_database, counties_json, label_col,\n",
        "                         label_display, color_continuous_scale,\n",
        "                         range_color=range_color)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QDA5qTl8zjiF"
      },
      "source": [
        "### Crowding (log scale)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uS9m8Z7tzpfp"
      },
      "outputs": [],
      "source": [
        "color_continuous_scale = \"greens\"\n",
        "label_col = \"log_crowding\"\n",
        "label_display = \"Crowding (log scale)\"\n",
        "range_color = (0, 3.453)\n",
        "create_custom_choropleth(county_database, counties_json, label_col,\n",
        "                         label_display, color_continuous_scale,\n",
        "                         range_color=range_color)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uGFqaBFp3EnL"
      },
      "source": [
        "### Minimum distance to a major airport"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UzS2lKvM3JZV"
      },
      "outputs": [],
      "source": [
        "color_continuous_scale = \"thermal\"\n",
        "label_col = \"min_distance_top_airport\"\n",
        "label_display = \"Minimum distance to a major airport\"\n",
        "# use quantiles\n",
        "range_color_min = np.quantile(county_database[label_col].dropna(), 0.2)\n",
        "range_color_max = np.quantile(county_database[label_col].dropna(), 0.8)\n",
        "range_color = (range_color_min, range_color_max)\n",
        "create_custom_choropleth(county_database, counties_json, label_col,\n",
        "                         label_display, color_continuous_scale,\n",
        "                         range_color=range_color)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UUQEr4QY3Jlx"
      },
      "source": [
        "### Education (level greater of equal than highschool) %"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LMRxzOzI3M6X"
      },
      "outputs": [],
      "source": [
        "color_continuous_scale = \"spectral\"\n",
        "label_col = \"education\"\n",
        "label_display = \"Education (%)\"\n",
        "create_custom_choropleth(county_database, counties_json, label_col,\n",
        "                         label_display, color_continuous_scale)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JPdZ4ffB3NP0"
      },
      "source": [
        "### Percent Population >= 65"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IGrRoA5h3Rau"
      },
      "outputs": [],
      "source": [
        "color_continuous_scale = \"tropic\"\n",
        "label_col = \"over_65\"\n",
        "label_display = \"Percent over 65y\"\n",
        "range_color_min = np.quantile(county_database[label_col].dropna(), 0.2)\n",
        "range_color_max = np.quantile(county_database[label_col].dropna(), 0.8)\n",
        "range_color = (range_color_min, range_color_max)\n",
        "create_custom_choropleth(county_database, counties_json, label_col,\n",
        "                         label_display, color_continuous_scale,\n",
        "                         range_color=range_color)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dH4DTW5g3SxK"
      },
      "source": [
        "### Percent Nursing Population"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BORmQyeJ3V9k"
      },
      "outputs": [],
      "source": [
        "color_continuous_scale = \"picnic\"\n",
        "label_col = \"pct_nursing\"\n",
        "label_display = \"Percent Nursing Population\"\n",
        "range_color_min = np.quantile(county_database[label_col].dropna(), 0.2)\n",
        "range_color_max = np.quantile(county_database[label_col].dropna(), 0.8)\n",
        "range_color = (range_color_min, range_color_max)\n",
        "create_custom_choropleth(county_database, counties_json, label_col,\n",
        "                         label_display, color_continuous_scale,\n",
        "                         range_color=range_color)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7QH7QOOG3vwC"
      },
      "source": [
        "### Percent Jail Population"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SgAjmolw3z0_"
      },
      "outputs": [],
      "source": [
        "color_continuous_scale = \"reds\"\n",
        "label_col = \"pct_jail\"\n",
        "label_display = \"Percent Jail Population\"\n",
        "range_color_min = np.quantile(county_database[label_col].dropna(), 0.2)\n",
        "range_color_max = np.quantile(county_database[label_col].dropna(), 0.8)\n",
        "range_color = (range_color_min, range_color_max)\n",
        "create_custom_choropleth(county_database, counties_json, label_col,\n",
        "                         label_display, color_continuous_scale,\n",
        "                         range_color=range_color)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qBL_psDjqcm2"
      },
      "source": [
        "### HHS Regions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "laux0AHnqbqd",
        "outputId": "3e9e8449-0d5b-4f18-a8f3-1c963e131c0f"
      },
      "outputs": [],
      "source": [
        "color_continuous_scale = \"rainbow\"\n",
        "label_col = \"HHS Region\"\n",
        "label_display = \"HHS Region\"\n",
        "create_custom_choropleth(county_database, counties_json, label_col,\n",
        "                         label_display, color_continuous_scale)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q8eCIAES34Sq"
      },
      "source": [
        "### Crude death rate scatter plots (with trends)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WsJp8A4_dgqL"
      },
      "source": [
        "#### Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3LaBtv8KdS3_"
      },
      "source": [
        "Custom function to plot the scatter plots with trends"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rmNEMGIO35v-"
      },
      "outputs": [],
      "source": [
        "from plotly.subplots import make_subplots\n",
        "import plotly.graph_objects as go\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "def custom_scatter_plot(feat_col=\"political_leaning\",\n",
        "                        feat_name=\"Political Leaning\", *,\n",
        "                        type_data=\"COVID-19\",\n",
        "                        cmid=None,\n",
        "                        min_val=None, max_val=None,\n",
        "                        filter=None, filter_threshold=None,\n",
        "                        **kwargs):\n",
        "\n",
        "  additional_text = \"\"  # for the title\n",
        "  height = 600  # height of the plot\n",
        "\n",
        "  county_db_to_use = county_databases[type_data][\"county_database2_imputed\"]\n",
        "\n",
        "  if \"filter_equality\" in kwargs:\n",
        "    county_db_to_use = county_db_to_use[county_db_to_use[kwargs.get(\"filter_equality\")] == kwargs.get(\"filter_equality_value\")]\n",
        "    additional_text = additional_text + kwargs.get(\"additional_text\", \"\")\n",
        "\n",
        "  if filter is None:\n",
        "    county_db = county_db_to_use\n",
        "    county_db2 = county_db_to_use  # not used\n",
        "    rows = 2\n",
        "  else:\n",
        "    county_db = county_db_to_use[county_db_to_use[filter] <= filter_threshold]\n",
        "    county_db2 = county_db_to_use[county_db_to_use[filter] > filter_threshold]\n",
        "    rows = 4\n",
        "\n",
        "  if cmid is None:\n",
        "    cmid = 0\n",
        "\n",
        "  # shared_yaxes=\"all\" or missing from kwargs\n",
        "  shared_yaxes = kwargs.get(\"shared_yaxes\", False)\n",
        "  fig = make_subplots(rows=rows, cols=3,\n",
        "                      subplot_titles=tuple([\"Period {i}\".format(i=i) for i in range(1, 7)]),\n",
        "                      shared_yaxes=shared_yaxes)\n",
        "  for per in range(6):\n",
        "    fig.add_trace(\n",
        "        go.Scatter(x=county_db[feat_col],\n",
        "                   y=county_db[f\"deathRate_period{per+1}\"],\n",
        "                   mode=\"markers\",\n",
        "                   marker_color=county_db[feat_col],\n",
        "                   marker_colorscale=\"rdbu_r\",\n",
        "                   marker={\"cmid\": cmid},\n",
        "                   name=f\"Period {per+1}\",\n",
        "                   showlegend=False),\n",
        "        row=(per//3 + 1), col=(per%3 + 1)\n",
        "    )\n",
        "\n",
        "    dt = county_db[[feat_col, f\"deathRate_period{per+1}\", \"total_pop\"]].dropna()\n",
        "    lr = LinearRegression(fit_intercept=True)\n",
        "    lr.fit(dt[feat_col].to_numpy().reshape(-1, 1),\n",
        "          dt[f\"deathRate_period{per+1}\"].to_numpy(),\n",
        "          np.log(dt[\"total_pop\"]).to_numpy())  # log total pop sample weight\n",
        "    \n",
        "    slope = round(lr.coef_[0], 2)\n",
        "\n",
        "    if min_val is None:\n",
        "      min_val2 = county_db[feat_col].min()\n",
        "    else:\n",
        "      min_val2 = min_val\n",
        "    if max_val is None:\n",
        "      max_val2 = county_db[feat_col].max()\n",
        "    else:\n",
        "      max_val2 = max_val\n",
        "    y_pred = lr.predict(np.linspace(min_val2, max_val2, 1000).reshape(-1, 1))\n",
        "    fig.append_trace(go.Scatter(x=np.linspace(min_val2, max_val2, 1000), y=y_pred,\n",
        "                                name=f\"Regression Period {per+1}. Slope={slope}\",\n",
        "                                line=dict(color=\"black\", width=4,\n",
        "                                          dash=\"dash\")\n",
        "                                ), row=(per//3 + 1), col=(per%3 + 1))\n",
        "\n",
        "    if not(filter is None):\n",
        "      fig.add_trace(\n",
        "        go.Scatter(x=county_db2[feat_col],\n",
        "                   y=county_db2[f\"deathRate_period{per+1}\"],\n",
        "                   mode=\"markers\",\n",
        "                   marker_color=county_db2[feat_col],\n",
        "                   marker_colorscale=\"rdbu_r\",\n",
        "                   marker={\"cmid\": cmid},\n",
        "                   name=f\"Period {per+1}, {filter} above\",\n",
        "                   showlegend=False),\n",
        "        row=(per//3 + 3), col=(per%3 + 1)\n",
        "      )\n",
        "      dt = county_db2[[feat_col, f\"deathRate_period{per+1}\", \"total_pop\"]].dropna()\n",
        "      lr = LinearRegression(fit_intercept=True)\n",
        "      lr.fit(dt[feat_col].to_numpy().reshape(-1, 1),\n",
        "            dt[f\"deathRate_period{per+1}\"].to_numpy(),\n",
        "            np.log(dt[\"total_pop\"]).to_numpy())  # log total pop sample weight\n",
        "      \n",
        "      slope = round(lr.coef_[0], 2)\n",
        "\n",
        "      if min_val is None:\n",
        "        min_val2 = county_db2[feat_col].min()\n",
        "      else:\n",
        "        min_val2 = min_val\n",
        "      if max_val is None:\n",
        "        max_val2 = county_db2[feat_col].max()\n",
        "      else:\n",
        "        max_val2 = max_val\n",
        "      y_pred = lr.predict(np.linspace(min_val2, max_val2, 1000).reshape(-1, 1))\n",
        "      fig.append_trace(go.Scatter(x=np.linspace(min_val2, max_val2, 1000), y=y_pred,\n",
        "                                  name=f\"Regression Period {per+1}, {filter} above. Slope={slope}\",\n",
        "                                  line=dict(color=\"black\", width=4,\n",
        "                                            dash=\"dash\")\n",
        "                                  ), row=(per//3 + 3), col=(per%3 + 1))\n",
        "      additional_text = additional_text + f\"<br>{filter} <= {filter_threshold} compared to {filter} above\"\n",
        "      height = 1200  # higher image is mandatory as we add more rows\n",
        "  fig.update_layout(height=height, width=800,\n",
        "                    title_text=f\"{type_data} Crude Death Rate and {feat_name} by period{additional_text}\",\n",
        "                    xaxis_title=f\"{feat_name}\",\n",
        "                    yaxis_title=f\"{type_data} crude death rate\",\n",
        "                    legend_title=\"Slopes\")\n",
        "  fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VEZ8JnpRdKVL"
      },
      "source": [
        "Function for the stringency index (as it is a function of the period)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8HSLFVbYco4O"
      },
      "outputs": [],
      "source": [
        "def custom_scatter_plot_stringency(*,\n",
        "                                   min_val=None, max_val=None,\n",
        "                                   type_data=\"COVID-19\",\n",
        "                                   filter=None, filter_threshold=None):\n",
        "  feat_name = \"Stringency Index\"\n",
        "\n",
        "  county_db_to_use = county_databases[type_data][\"county_database2_imputed\"]\n",
        "\n",
        "  if filter is None:\n",
        "    county_db = county_db_to_use\n",
        "    county_db2 = county_db_to_use  # not used\n",
        "    rows = 2\n",
        "  else:\n",
        "    county_db = county_db_to_use[county_db_to_use[filter] <= filter_threshold]\n",
        "    county_db2 = county_db_to_use[county_db_to_use[filter] > filter_threshold]\n",
        "    rows = 4\n",
        "\n",
        "  fig = make_subplots(rows=rows, cols=3,\n",
        "                      subplot_titles=tuple([\"Period {i}\".format(i=i) for i in range(1, 7)]),\n",
        "                      shared_yaxes=\"all\")\n",
        "  for per in range(6):\n",
        "    fig.add_trace(\n",
        "        go.Scatter(x=county_db[f\"StringencyIndex{per+1}_mean\"],\n",
        "                   y=county_db[f\"deathRate_period{per+1}\"],\n",
        "                   mode=\"markers\",\n",
        "                   marker_color=county_db[f\"StringencyIndex{per+1}_mean\"],\n",
        "                   marker_colorscale=\"rdbu_r\",\n",
        "                   marker={\"cmid\": 37},\n",
        "                   name=f\"Period {per+1}\",\n",
        "                   showlegend=False),\n",
        "        row=(per//3 + 1), col=(per%3 + 1)\n",
        "    )\n",
        "\n",
        "    dt = county_db[[f\"StringencyIndex{per+1}_mean\", f\"deathRate_period{per+1}\", \"total_pop\"]].dropna()\n",
        "    lr = LinearRegression(fit_intercept=True)\n",
        "    lr.fit(dt[f\"StringencyIndex{per+1}_mean\"].to_numpy().reshape(-1, 1),\n",
        "           dt[f\"deathRate_period{per+1}\"].to_numpy(),\n",
        "           np.log(dt[\"total_pop\"]).to_numpy())  # log total pop sample weight\n",
        "    \n",
        "    slope = round(lr.coef_[0], 2)\n",
        "\n",
        "    if min_val is None:\n",
        "      min_val2 = county_db[f\"StringencyIndex{per+1}_mean\"].min()\n",
        "    else:\n",
        "      min_val2 = min_val\n",
        "    if max_val is None:\n",
        "      max_val2 = county_db[f\"StringencyIndex{per+1}_mean\"].max()\n",
        "    else:\n",
        "      max_val2 = max_val\n",
        "\n",
        "    y_pred = lr.predict(np.linspace(min_val2, max_val2, 1000).reshape(-1, 1))\n",
        "    fig.append_trace(go.Scatter(x=np.linspace(min_val2, max_val2, 1000), y=y_pred,\n",
        "                                name=f\"Regression Period {per+1}. Slope={slope}\",\n",
        "                                line=dict(color=\"black\", width=4,\n",
        "                                          dash=\"dash\")\n",
        "                                ), row=(per//3 + 1), col=(per%3 + 1))\n",
        "    additional_text = \"\"  # for the title\n",
        "    height = 600  # height of the plot\n",
        "\n",
        "    if not(filter is None):\n",
        "      fig.add_trace(\n",
        "        go.Scatter(x=county_db2[f\"StringencyIndex{per+1}_mean\"],\n",
        "                   y=county_db2[f\"deathRate_period{per+1}\"],\n",
        "                   mode=\"markers\",\n",
        "                   marker_color=county_db2[f\"StringencyIndex{per+1}_mean\"],\n",
        "                   marker_colorscale=\"rdbu_r\",\n",
        "                   marker={\"cmid\": 37},\n",
        "                   name=f\"Period {per+1}, {filter} above\",\n",
        "                   showlegend=False),\n",
        "        row=(per//3 + 3), col=(per%3 + 1)\n",
        "      )\n",
        "      dt = county_db2[[f\"StringencyIndex{per+1}_mean\", f\"deathRate_period{per+1}\", \"total_pop\"]].dropna()\n",
        "      lr = LinearRegression(fit_intercept=True)\n",
        "      lr.fit(dt[f\"StringencyIndex{per+1}_mean\"].to_numpy().reshape(-1, 1),\n",
        "            dt[f\"deathRate_period{per+1}\"].to_numpy(),\n",
        "            np.log(dt[\"total_pop\"]).to_numpy())  # log total pop sample weight\n",
        "      \n",
        "      slope = round(lr.coef_[0], 2)\n",
        "\n",
        "      if min_val is None:\n",
        "        min_val2 = county_db2[f\"StringencyIndex{per+1}_mean\"].min()\n",
        "      else:\n",
        "        min_val2 = min_val\n",
        "      if max_val is None:\n",
        "        max_val2 = county_db2[f\"StringencyIndex{per+1}_mean\"].max()\n",
        "      else:\n",
        "        max_val2 = max_val\n",
        "      y_pred = lr.predict(np.linspace(min_val2, max_val2, 1000).reshape(-1, 1))\n",
        "      fig.append_trace(go.Scatter(x=np.linspace(min_val2, max_val2, 1000), y=y_pred,\n",
        "                                  name=f\"Regression Period {per+1}, {filter} above. Slope={slope}\",\n",
        "                                  line=dict(color=\"black\", width=4,\n",
        "                                            dash=\"dash\")\n",
        "                                  ), row=(per//3 + 3), col=(per%3 + 1))\n",
        "      additional_text = f\"<br>{filter} <= {filter_threshold} compared to {filter} above\"\n",
        "      height = 1200  # higher image is mandatory as we add more rows\n",
        "  fig.update_layout(height=height, width=800,\n",
        "                    title_text=f\"{type_data} Crude Death Rate and {feat_name} by period{additional_text}\",\n",
        "                    xaxis_title=f\"{feat_name}\",\n",
        "                    yaxis_title=f\"{type_data} crude death rate\",\n",
        "                    legend_title=\"Slopes\")\n",
        "  fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dy1sIuqudkkt"
      },
      "source": [
        "#### Plots"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tZLhPlWPwcBT"
      },
      "outputs": [],
      "source": [
        "custom_scatter_plot(feat_col=\"political_leaning\",\n",
        "                    feat_name=\"Political Leaning\",\n",
        "                    type_data=\"COVID-19\",\n",
        "                    min_val=-0.5, max_val=0.5,\n",
        "                    cmid=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nXnp8VECVetc"
      },
      "outputs": [],
      "source": [
        "custom_scatter_plot(feat_col=\"political_leaning\",\n",
        "                    feat_name=\"Political Leaning\",\n",
        "                    type_data=\"All Causes\",\n",
        "                    min_val=-0.5, max_val=0.5,\n",
        "                    cmid=0,\n",
        "                    filter=\"log_pop_density\",\n",
        "                    filter_threshold=round(np.quantile(county_databases[\"All Causes\"][\"county_database2_imputed\"][\"log_pop_density\"].dropna(), 0.9), 2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EskdS87SiKnu"
      },
      "outputs": [],
      "source": [
        "custom_scatter_plot(feat_col=\"political_leaning\",\n",
        "                    feat_name=\"Political Leaning\",\n",
        "                    type_data=\"All Causes\",\n",
        "                    min_val=-0.5, max_val=0.5,\n",
        "                    cmid=0,\n",
        "                    filter=\"log_pop_density\",\n",
        "                    filter_threshold=round(np.quantile(county_databases[\"All Causes\"][\"county_database2_imputed\"][\"log_pop_density\"].dropna(), 0.9), 2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 617
        },
        "id": "mNYipbQ9vTcL",
        "outputId": "db38f289-9572-4654-fd7c-5b138183537a"
      },
      "outputs": [],
      "source": [
        "custom_scatter_plot(feat_col=\"political_leaning\",\n",
        "                    feat_name=\"Political Leaning\",\n",
        "                    type_data=\"Excess Mortality\",\n",
        "                    min_val=-0.5, max_val=0.5,\n",
        "                    cmid=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a9dwPT5vwWMY"
      },
      "outputs": [],
      "source": [
        "custom_scatter_plot(feat_col=\"StringencyIndex1\",\n",
        "                    feat_name=\"Stringency Index Period 1\",\n",
        "                    type_data=\"COVID-19\",\n",
        "                    cmid=37)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2eSj6PPXdCi7"
      },
      "outputs": [],
      "source": [
        "custom_scatter_plot_stringency(type_data=\"COVID-19\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AmYQMaJErEYj"
      },
      "outputs": [],
      "source": [
        "custom_scatter_plot_stringency(type_data=\"All Causes\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HD7kw1AazL7Z"
      },
      "outputs": [],
      "source": [
        "custom_scatter_plot_stringency(type_data=\"Excess Mortality\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "acp_nb = 5\n",
        "acp_name = county_database[county_database[\"acp\"] == acp_nb][\"acp_name\"].iloc[0]\n",
        "kwargs = {\"filter_equality\": \"acp\",\n",
        "          \"filter_equality_value\": acp_nb,\n",
        "          \"additional_text\": f\"<br>American Communities: {acp_name}\"}\n",
        "custom_scatter_plot(feat_col=\"political_leaning\",\n",
        "                    feat_name=\"Political Leaning\",\n",
        "                    type_data=\"Excess Mortality\",\n",
        "                    min_val=-0.5, max_val=0.5,\n",
        "                    cmid=0, **kwargs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 617
        },
        "id": "cEHNNN2Ownnx",
        "outputId": "5fdb7353-c381-4226-a031-2f2a94e11e16"
      },
      "outputs": [],
      "source": [
        "custom_scatter_plot(feat_col=\"log_pop_density\",\n",
        "                    feat_name=\"Log Pop density\",\n",
        "                    cmid=4.5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 617
        },
        "id": "o6DnIBfUyfZB",
        "outputId": "2f72617f-547d-4049-8274-e319505aec6b"
      },
      "outputs": [],
      "source": [
        "custom_scatter_plot(feat_col=\"over_65\",\n",
        "                    feat_name=\"Over 65 %\",\n",
        "                    cmid=30)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 617
        },
        "id": "xfUW1MT6yuoY",
        "outputId": "11437763-6869-43f0-9462-481bc5579490"
      },
      "outputs": [],
      "source": [
        "custom_scatter_plot(feat_col=\"obesity\",\n",
        "                    feat_name=\"Obesity %\",\n",
        "                    type_data=\"COVID-19\",\n",
        "                    cmid=35)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 617
        },
        "id": "xqXXpos_rOwI",
        "outputId": "98edf2a7-409c-42d0-d45c-d56f878630d4"
      },
      "outputs": [],
      "source": [
        "custom_scatter_plot(feat_col=\"obesity\",\n",
        "                    feat_name=\"Obesity %\",\n",
        "                    type_data=\"All Causes\",\n",
        "                    cmid=35)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4moP1OjVzVme"
      },
      "outputs": [],
      "source": [
        "custom_scatter_plot(feat_col=\"obesity\",\n",
        "                    feat_name=\"Obesity %\",\n",
        "                    type_data=\"Excess Mortality\",\n",
        "                    cmid=35)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 617
        },
        "id": "yGrmVVTak6tk",
        "outputId": "2a3b0a1b-b589-4d7b-9757-00e5bd5a4b65"
      },
      "outputs": [],
      "source": [
        "custom_scatter_plot(feat_col=\"pct_hispanic\",\n",
        "                    feat_name=\"Hispanic population %\",\n",
        "                    type_data=\"COVID-19\",\n",
        "                    cmid=county_databases[\"COVID-19\"][\"county_database2_imputed\"][\"pct_hispanic\"].median())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 617
        },
        "id": "eEJFim9akPnJ",
        "outputId": "b341c7a3-5fb9-4a11-a04b-49dd96d34fe3"
      },
      "outputs": [],
      "source": [
        "custom_scatter_plot(feat_col=\"pct_jail\",\n",
        "                    feat_name=\"Jail population %\",\n",
        "                    type_data=\"COVID-19\",\n",
        "                    cmid=county_databases[\"COVID-19\"][\"county_database2_imputed\"][\"pct_jail\"].median())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 617
        },
        "id": "BVKv7QICkWqV",
        "outputId": "29de0147-8535-40b9-d2fd-9f034e902071"
      },
      "outputs": [],
      "source": [
        "custom_scatter_plot(feat_col=\"pct_nursing\",\n",
        "                    feat_name=\"Nursing population %\",\n",
        "                    type_data=\"COVID-19\",\n",
        "                    cmid=county_databases[\"COVID-19\"][\"county_database2_imputed\"][\"pct_nursing\"].median())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 617
        },
        "id": "L6U-UoMDn6cc",
        "outputId": "b6bbd702-c0ef-4510-e8ec-d41363e49e76"
      },
      "outputs": [],
      "source": [
        "custom_scatter_plot(feat_col=\"income\",\n",
        "                    feat_name=\"Median Household Income\",\n",
        "                    type_data=\"COVID-19\",\n",
        "                    cmid=county_databases[\"COVID-19\"][\"county_database2_imputed\"][\"income\"].median())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 617
        },
        "id": "xqMoBR7woavV",
        "outputId": "57c15c89-b1a5-42bb-eb83-150dc0ec7ff2"
      },
      "outputs": [],
      "source": [
        "custom_scatter_plot(feat_col=\"como_asthma\",\n",
        "                    feat_name=\"Comorbidities Asthma %\",\n",
        "                    type_data=\"COVID-19\",\n",
        "                    cmid=county_databases[\"COVID-19\"][\"county_database2_imputed\"][\"como_asthma\"].median())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rucSV1mypEDr"
      },
      "outputs": [],
      "source": [
        "custom_scatter_plot(feat_col=\"income\",\n",
        "                    feat_name=\"Median Household Income\",\n",
        "                    type_data=\"COVID-19\",\n",
        "                    cmid=county_databases[\"COVID-19\"][\"county_database2_imputed\"][\"income\"].median())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PwqGk-A_kHhJ",
        "outputId": "916dd878-c9d2-4644-d116-db580cbee188"
      },
      "outputs": [],
      "source": [
        "list(county_database2_imputed.columns)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bGDWLtNeF3eb"
      },
      "source": [
        "### Crude Death Rate map plots"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vaQ3gyWDh92q"
      },
      "source": [
        "Plot the death count & death rate of each county per period"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ITVXu4XZiCXT"
      },
      "outputs": [],
      "source": [
        "color_continuous_scale = \"matter\"\n",
        "label_col = \"deathCount_period1\"\n",
        "label_display = \"Period 1 Death Count\"\n",
        "range_color = (0, 82.38)\n",
        "create_custom_choropleth(county_database, counties_json, label_col,\n",
        "                         label_display, color_continuous_scale,\n",
        "                         range_color=range_color)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "VOR3ERqaihdf",
        "outputId": "0e13a740-1bce-438f-af91-b231f519168f"
      },
      "outputs": [],
      "source": [
        "color_continuous_scale = \"matter\"\n",
        "label_col = \"deathCount_period2\"\n",
        "label_display = \"Period 2 Death Count\"\n",
        "range_color = (0, 82.38)\n",
        "create_custom_choropleth(county_database, counties_json, label_col,\n",
        "                         label_display, color_continuous_scale,\n",
        "                         range_color=range_color)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_fcK1DCrilUB"
      },
      "outputs": [],
      "source": [
        "color_continuous_scale = \"matter\"\n",
        "label_col = \"deathCount_period3\"\n",
        "label_display = \"Period 3 Death Count\"\n",
        "range_color = (0, 82.38)\n",
        "create_custom_choropleth(county_database, counties_json, label_col,\n",
        "                         label_display, color_continuous_scale,\n",
        "                         range_color=range_color)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1GZcRColNPqr"
      },
      "outputs": [],
      "source": [
        "color_continuous_scale = \"matter\"\n",
        "label_col = \"deathRate_period1\"\n",
        "label_display = \"Period 1 Death Rates\"\n",
        "range_color = (0, 82.38)\n",
        "create_custom_choropleth(county_database, counties_json, label_col,\n",
        "                         label_display, color_continuous_scale,\n",
        "                         range_color=range_color)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "GnnB4hReNQ9W",
        "outputId": "650ca41e-27c2-4464-891d-1261a063bb02"
      },
      "outputs": [],
      "source": [
        "color_continuous_scale = \"matter\"\n",
        "label_col = \"deathRate_period2\"\n",
        "label_display = \"Period 2 Death Rates\"\n",
        "range_color = (0, 82.38)\n",
        "create_custom_choropleth(county_database, counties_json, label_col,\n",
        "                         label_display, color_continuous_scale,\n",
        "                         range_color=range_color)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N-V1yoaPNSrv"
      },
      "outputs": [],
      "source": [
        "color_continuous_scale = \"matter\"\n",
        "label_col = \"deathRate_period3\"\n",
        "label_display = \"Period 3 Death Rates\"\n",
        "range_color = (0, 82.38)\n",
        "create_custom_choropleth(county_database, counties_json, label_col,\n",
        "                         label_display, color_continuous_scale,\n",
        "                         range_color=range_color)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "Ri5kTIpNC9B3",
        "outputId": "70084339-1c36-4f62-97c1-ec3140e4db04"
      },
      "outputs": [],
      "source": [
        "color_continuous_scale = \"matter\"\n",
        "label_col = \"deathRate_period4\"\n",
        "label_display = \"Period 4 Death Rates\"\n",
        "range_color = (0, 500)\n",
        "create_custom_choropleth(county_database, counties_json, label_col,\n",
        "                         label_display, color_continuous_scale,\n",
        "                         range_color=range_color)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "XaDYh-TVDIR6",
        "outputId": "2a052771-b8d6-4789-d527-b9cb5225ce19"
      },
      "outputs": [],
      "source": [
        "color_continuous_scale = \"matter\"\n",
        "label_col = \"deathRate_period5\"\n",
        "label_display = \"Period 5 Death Rates\"\n",
        "range_color = (0, 500)\n",
        "create_custom_choropleth(county_database, counties_json, label_col,\n",
        "                         label_display, color_continuous_scale,\n",
        "                         range_color=range_color)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "Q8Y7TkMgDLOI",
        "outputId": "05937d61-7a10-480f-a852-41e30dd35c47"
      },
      "outputs": [],
      "source": [
        "color_continuous_scale = \"matter\"\n",
        "label_col = \"deathRate_period6\"\n",
        "label_display = \"Period 6 Death Rates\"\n",
        "range_color = (0, 500)\n",
        "create_custom_choropleth(county_database, counties_json, label_col,\n",
        "                         label_display, color_continuous_scale,\n",
        "                         range_color=range_color)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d9xKdGRiPBlp"
      },
      "source": [
        "# Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2aOjO3qEasdh"
      },
      "source": [
        "## Core functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nOX5zge9ipMf"
      },
      "source": [
        "Import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NBDacg-viqDk"
      },
      "outputs": [],
      "source": [
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.linear_model import LogisticRegression, ElasticNet\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import cross_validate, GridSearchCV, RepeatedKFold, train_test_split, StratifiedShuffleSplit\n",
        "from sklearn.metrics import mean_absolute_percentage_error\n",
        "import seaborn as sns\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QoLdLu9Yaw4z"
      },
      "source": [
        "Virus introduction models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1CVSPj_qaySj"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import make_scorer, accuracy_score, \\\n",
        "  balanced_accuracy_score, precision_score, recall_score, f1_score, \\\n",
        "  mean_absolute_error, r2_score, mean_squared_error, roc_auc_score, \\\n",
        "  precision_recall_curve, auc, mean_absolute_percentage_error, \\\n",
        "  average_precision_score, pairwise_distances\n",
        "from sklearn.preprocessing import label_binarize\n",
        "\n",
        "\n",
        "def pr_auc_score(y, y_pred, **kwargs):\n",
        "  if len(y_pred.shape) == 1:  # binary classification\n",
        "    precision, recall, _ = precision_recall_curve(y, y_pred, **kwargs)\n",
        "  else:\n",
        "    classes = list(range(y_pred.shape[1]))\n",
        "    if len(classes) == 2:  # binary classification too\n",
        "        precision, recall, _ = precision_recall_curve(y, y_pred[:,1],\n",
        "                                                      **kwargs)\n",
        "    else:  # multiclass\n",
        "      Y = label_binarize(y, classes=classes)\n",
        "      precision, recall, _ = precision_recall_curve(Y.ravel(), y_pred.ravel(),\n",
        "                                                    **kwargs)\n",
        "  return auc(recall, precision)\n",
        "\n",
        "def avg_precision(y, y_pred, **kwargs):\n",
        "  if len(y_pred.shape) == 1:  # binary classification\n",
        "    return average_precision_score(y, y_pred, average=\"micro\", **kwargs)\n",
        "  classes = list(range(y_pred.shape[1]))\n",
        "  if len(classes) == 2:  # binary classification too\n",
        "    return average_precision_score(y, y_pred[:,1], average=\"micro\", **kwargs)\n",
        "  # multiclass\n",
        "  Y = label_binarize(y, classes=classes)\n",
        "  return average_precision_score(Y, y_pred, average=\"micro\", **kwargs)\n",
        "\n",
        "custom_scorer_clf = {'accuracy': make_scorer(accuracy_score,\n",
        "                                             greater_is_better=True),\n",
        "                     'balanced_accuracy': make_scorer(balanced_accuracy_score,\n",
        "                                                      greater_is_better=True),\n",
        "                     'precision': make_scorer(precision_score, average='macro'),\n",
        "                     'recall': make_scorer(recall_score, average='macro'),\n",
        "                     'f1': make_scorer(f1_score, average='macro',\n",
        "                                       greater_is_better=True),\n",
        "                     'auroc': make_scorer(roc_auc_score, multi_class=\"ovo\",\n",
        "                                          average=\"macro\",\n",
        "                                          needs_proba=True,\n",
        "                                          greater_is_better=True),\n",
        "                     'auprc': make_scorer(pr_auc_score, needs_proba=True,\n",
        "                                          greater_is_better=True),\n",
        "                     'avg_precision': make_scorer(avg_precision,\n",
        "                                                  needs_proba=True,\n",
        "                                                  greater_is_better=True)}\n",
        "\n",
        "def virus_introduction_all(county_db, selected_columns_list,\n",
        "                           X_selected_columns_list, *,\n",
        "                           plot=False):\n",
        "  \n",
        "  def print2(x):\n",
        "    if plot:\n",
        "      print(x)\n",
        "\n",
        "  county_db.index = county_db.FIPS\n",
        "  dt = county_db[selected_columns_list[-1] + [\"Province_State\"]]  # total_pop, Province_State and deathcounts will be removed later\n",
        "\n",
        "  dt[\"total_pop\"] = np.log(dt[\"total_pop\"])  # sample weights\n",
        "  print2(dt.shape)\n",
        "\n",
        "  # drop nan values\n",
        "  dt.dropna(inplace=True)\n",
        "  print2(\"Dataset shape after nan values removed: {}\".format(dt.shape))\n",
        "\n",
        "  # extract groups (province states)\n",
        "  groups = dt[\"Province_State\"].to_numpy()\n",
        "\n",
        "  # Dataset: Period 1, 2, 3 or after\n",
        "  X = dt[X_selected_columns_list[-1]]\n",
        "  y = np.array([0 for _ in range(len(dt))])\n",
        "  for i in range(len(dt)):\n",
        "    p1_count = (dt[\"deathCount_period1\"].iloc[i] >= 5)\n",
        "    p2_count = (dt[\"deathCount_period2\"].iloc[i] >= 5)\n",
        "    p3_count = (dt[\"deathCount_period3\"].iloc[i] >= 5)\n",
        "    if p1_count:\n",
        "      y[i] = 0\n",
        "    elif p2_count and not(p1_count):\n",
        "      y[i] = 1\n",
        "    elif p3_count and not(p1_count or p2_count):\n",
        "      y[i] = 2\n",
        "    else:\n",
        "      y[i] = 3\n",
        "  # weight the regression by the log of a county’s population\n",
        "  sample_weight = dt[\"total_pop\"].to_numpy()\n",
        "\n",
        "  return dt, X, y, sample_weight, groups\n",
        "\n",
        "\n",
        "def virus_introduction_periods(county_db, selected_columns_list,\n",
        "                               X_selected_columns_list,\n",
        "                               *, plot=False):\n",
        "  def print2(x):\n",
        "    if plot:\n",
        "      print(x)\n",
        "  \n",
        "  county_db.index = county_db.FIPS\n",
        "  dt = county_db[selected_columns_list[-1] + [\"Province_State\"]]  # total_pop and deathcounts will be removed later\n",
        "\n",
        "  dt[\"total_pop\"] = np.log(dt[\"total_pop\"])  # sample weights\n",
        "  print2(dt.shape)\n",
        "\n",
        "  # drop nan values\n",
        "  dt.dropna(inplace=True)\n",
        "  print2(\"Dataset shape after nan values removed: {}\".format(dt.shape))\n",
        "\n",
        "  # extract groups (province states)\n",
        "  groups = dt[\"Province_State\"].to_numpy()\n",
        "\n",
        "  # Dataset: Period 1, 2, 3 or after\n",
        "  X = dt[X_selected_columns_list[-1]]\n",
        "  y1 = (dt[\"deathCount_period1\"] >= 5).astype(int)\n",
        "\n",
        "  # Second dataset: remove counties already infected by COVID-19\n",
        "  y2 = (dt[\"deathCount_period2\"] >= 5).astype(int)\n",
        "  for i, v in enumerate(y1):\n",
        "    if v == 1:  # infected county\n",
        "      y2.iloc[i] = 0\n",
        "\n",
        "  # Third dataset: remove counties already infected by COVID-19 in period 1 and 2\n",
        "  y3 = (dt[\"deathCount_period3\"] >= 5).astype(int)\n",
        "  for i, v in enumerate(y1):\n",
        "    if v == 1:  # infected county period 1\n",
        "      y3.iloc[i] = 0\n",
        "  for i, v in enumerate(y2):\n",
        "    if v == 1:  # infected county period 2\n",
        "      y3.iloc[i] = 0\n",
        "  \n",
        "  yother = (((dt[\"deathCount_period4\"] >= 5) | (dt[\"deathCount_period5\"] >= 5) | (dt[\"deathCount_period6\"] >= 5)) & ((dt[\"deathCount_period1\"] < 5) & (dt[\"deathCount_period2\"] < 5) & (dt[\"deathCount_period3\"] < 5))).astype(int)\n",
        "\n",
        "  y_list = [y1, y2, y3, yother]\n",
        "\n",
        "  # weight the regression by the log of a county’s population\n",
        "  sample_weight = dt[\"total_pop\"].to_numpy()\n",
        "\n",
        "  return dt, X, y_list, sample_weight, groups\n",
        "\n",
        "\n",
        "def lr_pipeline_period(X, y, sample_weight, period_number, *, groups=None, plot=True):\n",
        "\n",
        "  model = make_pipeline(MinMaxScaler(), LogisticRegression(random_state=42, n_jobs=-1))\n",
        "  fit_params = {\"logisticregression__sample_weight\": sample_weight}\n",
        "  model.fit(X, y, **fit_params)\n",
        "\n",
        "  coefs_p = pd.DataFrame(\n",
        "   model[1].coef_[0],\n",
        "   columns=[\"Coefficients\"], index=X.columns\n",
        "  )\n",
        "\n",
        "  if plot:\n",
        "    coefs_p.plot(kind=\"barh\", figsize=(9, 7))\n",
        "    plt.title(f\"Coefficient: Virus Introduction Period {period_number} (Logistic Regression)\")\n",
        "    plt.axvline(x=0, color=\".5\")\n",
        "    plt.subplots_adjust(left=.3)\n",
        "\n",
        "  if groups is None:\n",
        "    cv = RepeatedKFold(n_splits=5, n_repeats=5, random_state=42)\n",
        "  else:\n",
        "    sss = StratifiedShuffleSplit(n_splits=5, test_size=0.33, random_state=42)\n",
        "    cv = sss.split(X, groups)\n",
        "  cv_model = cross_validate(\n",
        "  model, X, y, cv=cv,\n",
        "  return_estimator=True, n_jobs=-1, fit_params=fit_params,\n",
        "  scoring=custom_scorer_clf\n",
        "  )\n",
        "  coefs_all_p = pd.DataFrame(\n",
        "    [model[1].coef_[0]\n",
        "      for model in cv_model[\"estimator\"]],\n",
        "    columns=X.columns\n",
        "  )\n",
        "\n",
        "  if plot:\n",
        "    plt.figure(figsize=(9, 7))\n",
        "    sns.boxplot(data=coefs_all_p, orient=\"h\", color=\"blue\", saturation=0.5)\n",
        "    plt.axvline(x=0, color='.5')\n",
        "    plt.xlabel(\"Coefficient importance\")\n",
        "    plt.title(f\"Coefficient importance and its variability - Virus Introduction Period {period_number} (Logistic Regression)\")\n",
        "    plt.subplots_adjust(left=.3)\n",
        "\n",
        "  return coefs_p, coefs_all_p, model, cv_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XRA2v1ZravHa"
      },
      "source": [
        "Virus spread models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jah4CA8Hatu5"
      },
      "outputs": [],
      "source": [
        "def virus_spread_dt(county_db, selected_columns_list, X_selected_columns_list,\n",
        "                    type_data,\n",
        "                    *, plot=False):\n",
        "  def print2(x):\n",
        "    if plot:\n",
        "      print(x)\n",
        "  \n",
        "  county_db.index = county_db.FIPS\n",
        "\n",
        "  # Datasets\n",
        "\n",
        "  # Period 1\n",
        "\n",
        "  dt1 = county_db[selected_columns_list[0] + [\"Province_State\"]]  # total_pop and deathcounts will be removed later\n",
        "  dt1[\"total_pop\"] = np.log(dt1[\"total_pop\"])  # sample weights\n",
        "  print2(dt1.shape)\n",
        "  # drop nan values\n",
        "  dt1.dropna(inplace=True)\n",
        "  print2(\"Dataset shape after nan values removed: {}\".format(dt1.shape))\n",
        "  if type_data != \"All Causes\":\n",
        "    dt1 = dt1[dt1[\"deathCount_period1\"] >= 5]\n",
        "  X1 = dt1[X_selected_columns_list[0]]\n",
        "  y1 = dt1[\"deathRate_period1\"]\n",
        "  # weight the regression by the log of a county’s population\n",
        "  sample_weight1 = dt1[\"total_pop\"].to_numpy()\n",
        "  # extract groups (province states)\n",
        "  groups1 = dt1[\"Province_State\"].to_numpy()\n",
        "  print2(\"Dataset shape period 1: {}\".format(dt1.shape))\n",
        "\n",
        "  # Period 2\n",
        "\n",
        "  dt2 = county_db[selected_columns_list[1] + [\"Province_State\"]]  # total_pop and deathcounts will be removed later\n",
        "  dt2[\"total_pop\"] = np.log(dt2[\"total_pop\"])  # sample weights\n",
        "  print2(dt2.shape)\n",
        "  # drop nan values\n",
        "  dt2.dropna(inplace=True)\n",
        "  print2(\"Dataset shape after nan values removed: {}\".format(dt2.shape))\n",
        "  if type_data != \"All Causes\":\n",
        "    dt2 = dt2[(dt2[\"deathCount_period2\"] >= 5) | (dt2[\"deathCount_period1\"] >= 5)]\n",
        "  X2 = dt2[X_selected_columns_list[1]]\n",
        "  y2 = dt2[\"deathRate_period2\"]\n",
        "  # weight the regression by the log of a county’s population\n",
        "  sample_weight2 = dt2[\"total_pop\"].to_numpy()\n",
        "  # extract groups (province states)\n",
        "  groups2 = dt2[\"Province_State\"].to_numpy()\n",
        "  print2(\"Dataset shape period 2: {}\".format(dt2.shape))\n",
        "\n",
        "  # Dataset 3\n",
        "\n",
        "  dt3 = county_db[selected_columns_list[2] + [\"Province_State\"]]  # total_pop and deathcounts will be removed later\n",
        "  dt3[\"total_pop\"] = np.log(dt3[\"total_pop\"])  # sample weights\n",
        "  print2(dt3.shape)\n",
        "  # drop nan values\n",
        "  dt3.dropna(inplace=True)\n",
        "  print2(\"Dataset shape after nan values removed: {}\".format(dt3.shape))\n",
        "  if type_data != \"All Causes\":\n",
        "    dt3 = dt3[((dt3[\"deathCount_period2\"] >= 5) | (dt3[\"deathCount_period3\"] >= 5)) & (dt3[\"deathCount_period1\"] < 5)]\n",
        "  X3 = dt3[X_selected_columns_list[2]]\n",
        "  y3 = dt3[\"deathRate_period3\"]\n",
        "  # weight the regression by the log of a county’s population\n",
        "  sample_weight3 = dt3[\"total_pop\"].to_numpy()\n",
        "  # extract groups (province states)\n",
        "  groups3 = dt3[\"Province_State\"].to_numpy()\n",
        "  print2(\"Dataset shape period 3: {}\".format(dt3.shape))\n",
        "\n",
        "  # Period 4\n",
        "\n",
        "  dt4 = county_db[selected_columns_list[3] + [\"Province_State\"]]  # total_pop and deathcounts will be removed later\n",
        "  dt4[\"total_pop\"] = np.log(dt4[\"total_pop\"])  # sample weights\n",
        "  print2(dt4.shape)\n",
        "  # drop nan values\n",
        "  dt4.dropna(inplace=True)\n",
        "  print2(\"Dataset shape after nan values removed: {}\".format(dt4.shape))\n",
        "  if type_data != \"All Causes\":\n",
        "    dt4 = dt4[dt4[\"deathCount_period4\"] >= 5]\n",
        "  X4 = dt4[X_selected_columns_list[3]]\n",
        "  y4 = dt4[\"deathRate_period4\"]\n",
        "  # weight the regression by the log of a county’s population\n",
        "  sample_weight4 = dt4[\"total_pop\"].to_numpy()\n",
        "  # extract groups (province states)\n",
        "  groups4 = dt4[\"Province_State\"].to_numpy()\n",
        "  print2(\"Dataset shape period 4: {}\".format(dt4.shape))\n",
        "\n",
        "  # Period 5\n",
        "\n",
        "  dt5 = county_db[selected_columns_list[4] + [\"Province_State\"]]  # total_pop and deathcounts will be removed later\n",
        "  dt5[\"total_pop\"] = np.log(dt5[\"total_pop\"])  # sample weights\n",
        "  print2(dt5.shape)\n",
        "  # drop nan values\n",
        "  dt5.dropna(inplace=True)\n",
        "  print2(\"Dataset shape after nan values removed: {}\".format(dt5.shape))\n",
        "  if type_data != \"All Causes\":\n",
        "    dt5 = dt5[dt5[\"deathCount_period5\"] >= 5]\n",
        "  X5 = dt5[X_selected_columns_list[4]]\n",
        "  y5 = dt5[\"deathRate_period5\"]\n",
        "  # weight the regression by the log of a county’s population\n",
        "  sample_weight5 = dt5[\"total_pop\"].to_numpy()\n",
        "  # extract groups (province states)\n",
        "  groups5 = dt5[\"Province_State\"].to_numpy()\n",
        "  print2(\"Dataset shape period 5: {}\".format(dt5.shape))\n",
        "\n",
        "  # Period 6\n",
        "\n",
        "  dt6 = county_db[selected_columns_list[5] + [\"Province_State\"]]  # total_pop and deathcounts will be removed later\n",
        "  dt6[\"total_pop\"] = np.log(dt6[\"total_pop\"])  # sample weights\n",
        "  print2(dt6.shape)\n",
        "  # drop nan values\n",
        "  dt6.dropna(inplace=True)\n",
        "  print2(\"Dataset shape after nan values removed: {}\".format(dt6.shape))\n",
        "  if type_data != \"All Causes\":\n",
        "    dt6 = dt6[dt6[\"deathCount_period6\"] >= 5]\n",
        "  X6 = dt6[X_selected_columns_list[5]]\n",
        "  y6 = dt6[\"deathRate_period6\"]\n",
        "  # weight the regression by the log of a county’s population\n",
        "  sample_weight6 = dt6[\"total_pop\"].to_numpy()\n",
        "  # extract groups (province states)\n",
        "  groups6 = dt6[\"Province_State\"].to_numpy()\n",
        "  print2(\"Dataset shape period 6: {}\".format(dt6.shape))\n",
        "\n",
        "  dt_list = [dt1, dt2, dt3, dt4, dt5, dt6]\n",
        "  X_list = [X1, X2, X3, X4, X5, X6]\n",
        "  y_list = [y1, y2, y3, y4, y5, y6]\n",
        "  sample_weight_list = [sample_weight1, sample_weight2, sample_weight3,\n",
        "                        sample_weight4, sample_weight5, sample_weight6]\n",
        "  groups_list = [groups1, groups2, groups3, groups4, groups5,\n",
        "                 groups6]\n",
        "\n",
        "  return dt_list, X_list, y_list, sample_weight_list, groups_list\n",
        "\n",
        "\n",
        "def elasticNet_pipeline_period(X_init, y, sample_weight, period_number, *,\n",
        "                               groups=None, plot=True,\n",
        "                               model_hyperparameters=None):\n",
        "  # Split dataset: Train set contains 70% of total counties\n",
        "  X = X_init.copy()  # COPY THE DATASET (because we modify it)\n",
        "  \n",
        "  if model_hyperparameters is None:  # grid search on train set to find the best ones\n",
        "    X[\"sample_weight\"] = sample_weight\n",
        "    if groups is None:\n",
        "      groups_train, groups_test = None, None\n",
        "    else:\n",
        "      X[\"groups\"] = groups\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
        "                                                        stratify=groups,\n",
        "                                                        test_size=0.33,\n",
        "                                                        random_state=42)\n",
        "    sample_weight_train, sample_weight_test = X_train[\"sample_weight\"], X_test[\"sample_weight\"]\n",
        "    X_train.drop(columns=[\"sample_weight\"], inplace=True)\n",
        "    X_test.drop(columns=[\"sample_weight\"], inplace=True)\n",
        "    X.drop(columns=[\"sample_weight\"], inplace=True)\n",
        "\n",
        "    if not(groups is None):\n",
        "      groups_train, groups_test = X_train[\"groups\"], X_test[\"groups\"]\n",
        "      X_train.drop(columns=[\"groups\"], inplace=True)\n",
        "      X_test.drop(columns=[\"groups\"], inplace=True)\n",
        "      X.drop(columns=[\"groups\"], inplace=True)\n",
        "\n",
        "  if groups is None:\n",
        "    cv = RepeatedKFold(n_splits=5, n_repeats=5, random_state=42)\n",
        "    cv_grid_search = RepeatedKFold(n_splits=5, n_repeats=1, random_state=42)\n",
        "  else:\n",
        "    sss = StratifiedShuffleSplit(n_splits=5, test_size=0.33, random_state=42)\n",
        "    cv = sss.split(X, groups)\n",
        "    if model_hyperparameters is None:  # grid search cross val parameters\n",
        "      sss2 = StratifiedShuffleSplit(n_splits=5, test_size=0.33, random_state=42)\n",
        "      cv_grid_search = sss2.split(X_train, groups_train)\n",
        "\n",
        "  # Define the model\n",
        "  model = make_pipeline(MinMaxScaler(), ElasticNet(random_state=42, max_iter=20000))\n",
        "  \n",
        "  scoring = \"neg_mean_absolute_percentage_error\"\n",
        "\n",
        "  # Define & fit the grid search if no hyperparameters provided\n",
        "  if model_hyperparameters is None:\n",
        "    parameters = {\"elasticnet__alpha\": np.arange(0.05, 1+0.05, 0.05),\n",
        "                  \"elasticnet__l1_ratio\": np.arange(0.05, 1+0.05, 0.05)}\n",
        "    # We perform a grid search using 5-fold cross validation on our training data\n",
        "    # and pick the hyper-parameters that yield the lowest Mean Absolute Percentage Error (MAPE)\n",
        "    clf = GridSearchCV(model, parameters,\n",
        "                      cv=cv_grid_search,\n",
        "                      n_jobs=-1, scoring=scoring)\n",
        "    fit_params = {\"elasticnet__sample_weight\": sample_weight_train}\n",
        "    clf.fit(X_train, y_train, **fit_params)\n",
        "    if plot:\n",
        "      print(\"Best parameters: {}\".format(clf.best_params_))\n",
        "    best_params = clf.best_params_\n",
        "\n",
        "  else:\n",
        "    if plot:\n",
        "      print(\"Hyperparameters provided: {}\".format(model_hyperparameters))\n",
        "    best_params = model_hyperparameters\n",
        "\n",
        "  # Train an elastic net on the entire dataset with the optimal\n",
        "  # parameters for the given period\n",
        "  best_params[\"elasticnet__random_state\"] = 42\n",
        "  best_params[\"elasticnet__max_iter\"] = 20000\n",
        "  model = make_pipeline(MinMaxScaler(), ElasticNet(random_state=42, max_iter=20000))\n",
        "  model.set_params(**best_params)\n",
        "  fit_params = {\"elasticnet__sample_weight\": sample_weight}\n",
        "  model.fit(X, y, **fit_params)\n",
        "\n",
        "  # Plot coefficients\n",
        "  coefs_p = pd.DataFrame(\n",
        "   model[1].coef_,\n",
        "   columns=[\"Coefficients\"], index=X.columns\n",
        "  )\n",
        "  \n",
        "  if plot:\n",
        "    print(coefs_p)\n",
        "    coefs_p.plot(kind=\"barh\", figsize=(9, 7))\n",
        "    plt.title(f\"Coefficient: Virus Spread Period {period_number} (Elastic Net)\")\n",
        "    plt.axvline(x=0, color=\".5\")\n",
        "    plt.subplots_adjust(left=.3)\n",
        "\n",
        "  # Confidence intervals (multiple fit after repeated K fold or StratifiedShuffleSplit)\n",
        "  cv_model = cross_validate(\n",
        "   model, X, y, cv=cv,\n",
        "   return_estimator=True, n_jobs=-1, fit_params=fit_params,\n",
        "   scoring=scoring\n",
        "  )\n",
        "  coefs_all_p = pd.DataFrame(\n",
        "    [model[1].coef_\n",
        "      for model in cv_model[\"estimator\"]],\n",
        "      columns=X.columns\n",
        "  )\n",
        "\n",
        "  if plot:\n",
        "    plt.figure(figsize=(9, 7))\n",
        "    sns.boxplot(data=coefs_all_p, orient=\"h\", color=\"blue\", saturation=0.5)\n",
        "    plt.axvline(x=0, color='.5')\n",
        "    plt.xlabel(\"Coefficient importance\")\n",
        "    plt.title(f\"Coefficient importance and its variability - Virus Spread Period {period_number} (Elastic Net)\")\n",
        "    plt.subplots_adjust(left=.3)\n",
        "\n",
        "  return coefs_p, coefs_all_p, model, cv_model\n",
        "\n",
        "\n",
        "def randomForest_pipeline_period(X_init, y, sample_weight, period_number, *,\n",
        "                                 groups=None, plot=True,\n",
        "                                 model_hyperparameters=None):\n",
        "  # Split dataset: Train set contains 70% of total counties\n",
        "  X = X_init.copy()  # COPY THE DATASET (because we modify it)\n",
        "  \n",
        "  if model_hyperparameters is None:\n",
        "    X[\"sample_weight\"] = sample_weight\n",
        "    if groups is None:\n",
        "      groups_train, groups_test = None, None\n",
        "    else:\n",
        "      X[\"groups\"] = groups\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
        "                                                        stratify=groups,\n",
        "                                                        test_size=0.33,\n",
        "                                                        random_state=42)\n",
        "    sample_weight_train, sample_weight_test = X_train[\"sample_weight\"], X_test[\"sample_weight\"]\n",
        "    X_train.drop(columns=[\"sample_weight\"], inplace=True)\n",
        "    X_test.drop(columns=[\"sample_weight\"], inplace=True)\n",
        "    X.drop(columns=[\"sample_weight\"], inplace=True)\n",
        "    \n",
        "    if not(groups is None):\n",
        "      groups_train, groups_test = X_train[\"groups\"], X_test[\"groups\"]\n",
        "      X_train.drop(columns=[\"groups\"], inplace=True)\n",
        "      X_test.drop(columns=[\"groups\"], inplace=True)\n",
        "      X.drop(columns=[\"groups\"], inplace=True)\n",
        "\n",
        "    if groups is None:\n",
        "      cv_grid_search = RepeatedKFold(n_splits=5, n_repeats=1, random_state=42)\n",
        "    else:\n",
        "      sss2 = StratifiedShuffleSplit(n_splits=5, test_size=0.33, random_state=42)\n",
        "      cv_grid_search = sss2.split(X_train, groups_train)\n",
        "\n",
        "  # Define the model\n",
        "  model = make_pipeline(MinMaxScaler(), RandomForestRegressor(random_state=42, n_jobs=-1))\n",
        "\n",
        "  scoring = \"neg_mean_absolute_percentage_error\"\n",
        "\n",
        "  # Define & fit the grid search if no hyperparameters provided\n",
        "  if model_hyperparameters is None:\n",
        "    parameters = {\"randomforestregressor__n_estimators\": [100, 200, 500],\n",
        "                  \"randomforestregressor__max_features\": [len(X.columns),\n",
        "                                  int(np.sqrt(len(X.columns)))],\n",
        "                  \"randomforestregressor__max_depth\": [20, 50],\n",
        "                  \"randomforestregressor__min_samples_split\": [2, 5, 10],\n",
        "                  \"randomforestregressor__min_samples_leaf\": [10, 20, 50],\n",
        "                  \"randomforestregressor__bootstrap\": [True, False]}\n",
        "    # We perform a grid search using 5-fold cross validation on our training data\n",
        "    # and pick the hyper-parameters that yield the lowest Mean Absolute Percentage Error (MAPE)\n",
        "\n",
        "    clf = GridSearchCV(model, parameters,\n",
        "                       cv=cv_grid_search,\n",
        "                       n_jobs=-1, scoring=scoring)\n",
        "    fit_params = {\"randomforestregressor__sample_weight\": sample_weight_train}\n",
        "    clf.fit(X_train, y_train, **fit_params)\n",
        "    if plot:\n",
        "      print(\"Best parameters: {}\".format(clf.best_params_))\n",
        "    best_params = clf.best_params_\n",
        "  else:\n",
        "    if plot:\n",
        "      print(\"Hyperparameters provided: {}\".format(model_hyperparameters))\n",
        "    best_params = model_hyperparameters\n",
        "\n",
        "  # Train a randomforestregressor on the entire dataset with the optimal\n",
        "  # parameters for the given period\n",
        "  best_params[\"randomforestregressor__random_state\"] = 42\n",
        "  best_params[\"randomforestregressor__n_jobs\"] = -1\n",
        "  model = make_pipeline(MinMaxScaler(), RandomForestRegressor(random_state=42, n_jobs=-1))\n",
        "  model.set_params(**best_params)\n",
        "  fit_params = {\"randomforestregressor__sample_weight\": sample_weight}\n",
        "  model.fit(X, y, **fit_params)\n",
        "\n",
        "  # coefficient\n",
        "  feature_names = list(X.columns)\n",
        "  importances = model[1].feature_importances_\n",
        "  std = np.std([tree.feature_importances_ for tree in model[1].estimators_], axis=0)\n",
        "  zipped = zip(importances, std, feature_names)\n",
        "  importancesi, stdi, feature_namesi = [], [], []\n",
        "  for x, y, z in sorted(zipped, key=lambda x: x[0]):\n",
        "    importancesi.append(x)\n",
        "    stdi.append(y)\n",
        "    feature_namesi.append(z)\n",
        "  errorsi = [1.96 * s / np.sqrt(len(stdi)) for s in stdi]\n",
        "  importancesi = pd.DataFrame(importancesi,\n",
        "                              columns=[f\"Feature importances using MDI - Virus Spread Period {period_number} (RandomForestRegressor)\\n95% confidence interval\"],\n",
        "                              index=feature_namesi)\n",
        "  if plot:\n",
        "    print(importancesi)\n",
        "    # plot\n",
        "    importancesi.plot(kind=\"barh\", figsize=(9, 7), xerr=errorsi)  # xerr for horizontal plots\n",
        "    plt.title(f\"Feature importances using MDI - Virus Spread Period {period_number} (RandomForestRegressor)\\n95% confidence interval\")\n",
        "    plt.legend(loc=\"best\")\n",
        "    plt.tight_layout()\n",
        "\n",
        "  importancesi[\"errors\"] = errorsi\n",
        "\n",
        "  return importancesi, model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KftvP7W7pjJr"
      },
      "source": [
        "## Introduction Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HTY0P-ntiL1o"
      },
      "source": [
        "For the introduction models (logistic regressions), we take all the counties in our dataset and we try to predict **only the new infected counties** in a given period."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TU0iMp1kwZs9"
      },
      "source": [
        "### MinMaxScaler & confidence intervals (Dataset 2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "62w89C7lbGCq"
      },
      "source": [
        "#### Define datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VBei6kfQbHwO",
        "outputId": "4c7f2970-0fea-49eb-ad89-a7feb0dcfb7c"
      },
      "outputs": [],
      "source": [
        "dt, X, y_list, \\\n",
        "  sample_weight, groups = virus_introduction_periods(county_database2_imputed,\n",
        "                                                     selected_columns_list,\n",
        "                                                     X_selected_columns_list,\n",
        "                                                     plot=False)\n",
        "y1, y2, y3, yother = y_list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BkO1ZJP8whpy"
      },
      "source": [
        "#### Logistic Regression - Period 1 (February 2020 - May 2020)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JuELlIxrfYle",
        "outputId": "42748cfc-493f-4aa9-85e0-0d3562b6da0c"
      },
      "outputs": [],
      "source": [
        "coefs_p1, coefs_all_p1, _, _ = lr_pipeline_period(X, y1, sample_weight, \"1\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AiRA4bQGxQY_"
      },
      "source": [
        "#### Logistic Regression - Period 2 (June 2020 - October 2020)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9zhS126sxUg1",
        "outputId": "090058c2-2545-4b9d-beb9-49c914640001"
      },
      "outputs": [],
      "source": [
        "coefs_p2, coefs_all_p2, _, _ = lr_pipeline_period(X, y2, sample_weight, \"2\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t1W-EuH8xZyf"
      },
      "source": [
        "#### Logistic Regression - Period 3 (November 2020 - February 2021)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c9j9lMeixbAr",
        "outputId": "6fc6d8ab-cd54-4b74-88c0-6cda1976a5b8"
      },
      "outputs": [],
      "source": [
        "coefs_p3, coefs_all_p3, _, _ = lr_pipeline_period(X, y3, sample_weight, \"3\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0dKQCcsjgve4"
      },
      "source": [
        "#### Logistic Regression - Period greater than 3 (March 2021 - February 2022)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qgIgrw_Ig3SS",
        "outputId": "10eb4057-4289-4d4b-9497-5bd58aa9d693"
      },
      "outputs": [],
      "source": [
        "coefs_pother, coefs_all_pother, _, _ = lr_pipeline_period(X, yother, sample_weight, \">=4\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VbpEyIHAAmml"
      },
      "source": [
        "#### Plot COVID-19 introduction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "oFTpz5K2EoId",
        "outputId": "35ce8933-9ea2-47f0-f126-8c4147dc583d"
      },
      "outputs": [],
      "source": [
        "county_database_plot_intro = county_database2_imputed.copy()\n",
        "covid_intro_l = [\"Unknown\" for _ in range(len(county_database_plot_intro))]\n",
        "for i in range(len(county_database_plot_intro)):\n",
        "  if county_database_plot_intro[\"deathCount_period1\"].iloc[i] >= 5:\n",
        "    covid_intro_l[i] = \"1\"\n",
        "for i in range(len(county_database_plot_intro)):\n",
        "  if (covid_intro_l[i] == \"Unknown\") and (county_database_plot_intro[\"deathCount_period2\"].iloc[i] >= 5):\n",
        "    covid_intro_l[i] = \"2\"\n",
        "for i in range(len(county_database_plot_intro)):\n",
        "  if (covid_intro_l[i] == \"Unknown\") and (county_database_plot_intro[\"deathCount_period3\"].iloc[i] >= 5):\n",
        "    covid_intro_l[i] = \"3\"\n",
        "for i in range(len(county_database_plot_intro)):\n",
        "  if (covid_intro_l[i] == \"Unknown\") and ((county_database_plot_intro[\"deathCount_period4\"].iloc[i] >= 5) or (county_database_plot_intro[\"deathCount_period5\"].iloc[i] >= 5) or (county_database_plot_intro[\"deathCount_period6\"].iloc[i] >= 5)):\n",
        "    covid_intro_l[i] = \">=4\"\n",
        "\n",
        "county_database_plot_intro[\"covid_intro_period\"] = covid_intro_l\n",
        "\n",
        "# plot\n",
        "color_continuous_scale = \"rainbown\"\n",
        "label_col = \"covid_intro_period\"\n",
        "label_display = \"COVID-19 Introduction Period\"\n",
        "create_custom_choropleth(county_database_plot_intro, counties_json, label_col,\n",
        "                         label_display, color_continuous_scale)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y9UtCFBqxiiW"
      },
      "source": [
        "#### Logistic Regression - All periods contributions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "HqwxKfkqxjvF",
        "outputId": "0ecf5436-e035-4d14-f75d-ba5e99eef6df"
      },
      "outputs": [],
      "source": [
        "import plotly.graph_objects as go\n",
        "feature_names = list(X.columns)\n",
        "\n",
        "intervals1 = (1.96 * coefs_all_p1.std() / np.sqrt(len(coefs_all_p1))).to_frame()\n",
        "intervals2 = (1.96 * coefs_all_p2.std() / np.sqrt(len(coefs_all_p2))).to_frame()\n",
        "intervals3 = (1.96 * coefs_all_p3.std() / np.sqrt(len(coefs_all_p3))).to_frame()\n",
        "intervalsother = (1.96 * coefs_all_pother.std() / np.sqrt(len(coefs_all_pother))).to_frame()\n",
        "period1_data = []\n",
        "period2_data = []\n",
        "period3_data = []\n",
        "periodother_data = []\n",
        "error_p1 = []\n",
        "error_p2 = []\n",
        "error_p3 = []\n",
        "error_pother = []\n",
        "for i, feat in enumerate(feature_names):\n",
        "  p1 = coefs_p1.loc[feat].iloc[0]\n",
        "  p2 = coefs_p2.loc[feat].iloc[0]\n",
        "  p3 = coefs_p3.loc[feat].iloc[0]\n",
        "  pother = coefs_pother.loc[feat].iloc[0]\n",
        "  e1 = intervals1.loc[feat].iloc[0]\n",
        "  e2 = intervals2.loc[feat].iloc[0]\n",
        "  e3 = intervals3.loc[feat].iloc[0]\n",
        "  eother = intervalsother.loc[feat].iloc[0]\n",
        "  period1_data.append(p1)\n",
        "  period2_data.append(p2)\n",
        "  period3_data.append(p3)\n",
        "  periodother_data.append(pother)\n",
        "  error_p1.append(e1)\n",
        "  error_p2.append(e2)\n",
        "  error_p3.append(e3)\n",
        "  error_pother.append(eother)\n",
        "fig = go.Figure(data=[\n",
        "    go.Bar(name=\"Period 1 (February 2020 - May 2020)\", x=feature_names, y=period1_data,\n",
        "           error_y=dict(type=\"data\", array=error_p1)),\n",
        "    go.Bar(name=\"Period 2 (June 2020 - October 2020)\", x=feature_names, y=period2_data,\n",
        "           error_y=dict(type=\"data\", array=error_p2)),\n",
        "    go.Bar(name=\"Period 3 (November 2020 - February 2021)\", x=feature_names, y=period3_data,\n",
        "           error_y=dict(type=\"data\", array=error_p3)),\n",
        "    go.Bar(name=\"Period >= 4 (March 2021 - February 2022)\", x=feature_names, y=periodother_data,\n",
        "           error_y=dict(type=\"data\", array=error_pother))\n",
        "])\n",
        "# Change the bar mode\n",
        "fig.update_layout(barmode=\"group\",\n",
        "                  xaxis_title=\"Features\",\n",
        "                  yaxis_title=\"Coefficients importance and its variability\",\n",
        "                  legend_title=\"Periods\",\n",
        "                  title=\"Coefficients importance and its variability (95% confidence interval)<br> for each period (MinMaxScaler & LogisticRegression)\")\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "yenfHzkm7JYi",
        "outputId": "75226864-38d2-499b-84e1-9be0820856f9"
      },
      "outputs": [],
      "source": [
        "directionality = pd.DataFrame()\n",
        "directionality[\"Variable\"] = feature_names\n",
        "directionality[\"Period 1\"] = period1_data\n",
        "directionality[\"Period 2\"] = period2_data\n",
        "directionality[\"Period 3\"] = period3_data\n",
        "directionality[\"Period >= 4\"] = periodother_data\n",
        "for i in range(1, 4):\n",
        "  directionality[f\"Period {i}\"] = directionality[f\"Period {i}\"].apply(lambda x: \"+\" if x >= 0 else \"-\")\n",
        "directionality[\"Period >= 4\"] = directionality[\"Period >= 4\"].apply(lambda x: \"+\" if x >= 0 else \"-\")\n",
        "directionality"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dwBfqXF0d1sH"
      },
      "source": [
        "#### Define dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 334
        },
        "id": "udUtFOqcd3-M",
        "outputId": "32adcb97-9bd9-4701-a182-438e0293b208"
      },
      "outputs": [],
      "source": [
        "dt, X, y, sample_weight, groups = virus_introduction_all(county_database2_imputed,\n",
        "                                                         selected_columns_list,\n",
        "                                                         X_selected_columns_list,\n",
        "                                                         plot=False)\n",
        "plt.hist(y)\n",
        "plt.xlabel(\"Period\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.title(\"Distribution of COVID-19 introductions for each periods (-1) (3=no introduction)\")\n",
        "plt.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x9INom4zbSyF"
      },
      "source": [
        "#### Logistic Regression - Multiple output (taking into account all periods)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 885
        },
        "id": "bmT-F9t8cxXk",
        "outputId": "820f87cb-8423-47c4-89ac-c57b4513658b"
      },
      "outputs": [],
      "source": [
        "coefs_p, coefs_all_p, _, _ = lr_pipeline_period(X, y, sample_weight, \"All\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ANRyqmA1FxkP"
      },
      "source": [
        "## Virus Spread Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mDKMfLml8Vyx"
      },
      "source": [
        "### MinMaxScaler & confidence intervals (ElasticNet) (Dataset 2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0LfIyYFYELXN"
      },
      "source": [
        "Minimise MAPE to select best hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lhCNVkkTEczQ"
      },
      "outputs": [],
      "source": [
        "# old feature transformations\n",
        "\"\"\"\n",
        "dt[\"education\"] = dt[\"education\"]**2\n",
        "dt[\"income\"] = np.sqrt(dt[\"income\"])\n",
        "dt[\"StringencyIndex\"] = dt[\"StringencyIndex\"]**2\n",
        "# dt[\"obesity\"] = np.exp(dt[\"obesity\"])  # exp obesity gives too high values!\n",
        "dt[\"pct_nursing\"] = dt[\"pct_nursing\"]**2\n",
        "dt[\"political_leaning\"] = dt[\"political_leaning\"]**2\n",
        "dt[\"pct_jail\"] = np.sqrt(dt[\"pct_jail\"])\n",
        "dt[\"pct_hispanic\"] = dt[\"pct_hispanic\"]**2\n",
        "dt[\"min_distance_top_airport\"] = np.sqrt(dt[\"min_distance_top_airport\"])\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jUyNcxmpAT8o"
      },
      "source": [
        "#### Define datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dj2pJs0NAMw7"
      },
      "outputs": [],
      "source": [
        "dt_list, X_list, y_list, \\\n",
        "sample_weight_list, groups_list = virus_spread_dt(county_database2_imputed,\n",
        "                                                  selected_columns_list,\n",
        "                                                  X_selected_columns_list,\n",
        "                                                  type_data=\"COVID-19\",\n",
        "                                                  plot=False)\n",
        "dt1, dt2, dt3, dt4, dt5, dt6 = dt_list\n",
        "X1, X2, X3, X4, X5, X6 = X_list\n",
        "y1, y2, y3, y4, y5, y6 = y_list\n",
        "sample_weight1, sample_weight2, sample_weight3, \\\n",
        "  sample_weight4, sample_weight5, sample_weight6 = sample_weight_list\n",
        "groups1, groups2, groups3, groups4, groups5, \\\n",
        "  groups6 = groups_list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4eKfDDGq8-Ky"
      },
      "source": [
        "#### ElasticNet - Period 1 (February 2020 - May 2020)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "8-5o4Ux_9Acq",
        "outputId": "dd8a6328-59be-4c9e-9502-b61186097f01"
      },
      "outputs": [],
      "source": [
        "coefs_p1_enet, coefs_all_p1_enet, _, _ = elasticNet_pipeline_period(X1, y1,\n",
        "                                                                    sample_weight1,\n",
        "                                                                    \"1\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sI3TBDHl9IoG"
      },
      "source": [
        "#### ElasticNet - Period 2 (June 2020 - October 2020)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s0jRVmSD9LKS",
        "outputId": "adf01678-e840-4613-b243-70a74a4c0703"
      },
      "outputs": [],
      "source": [
        "coefs_p2_enet, coefs_all_p2_enet, _, _ = elasticNet_pipeline_period(X2, y2,\n",
        "                                                                    sample_weight2,\n",
        "                                                                    \"2\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2S_FptB_9SX9"
      },
      "source": [
        "#### ElasticNet - Period 3 (November 2020 - February 2021)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "doXwtKd_9WQF",
        "outputId": "20794847-07ca-4598-a9ed-162fc5f828d4"
      },
      "outputs": [],
      "source": [
        "coefs_p3_enet, coefs_all_p3_enet, _, _ = elasticNet_pipeline_period(X3, y3,\n",
        "                                                                    sample_weight3,\n",
        "                                                                    \"3\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EhYA_TI2Aw-3"
      },
      "source": [
        "#### ElasticNet - Period 4 (March 2021 - July 2021)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9G4N2ch4BGsP",
        "outputId": "b22bfd30-1fde-466a-ee41-cccaa2926551"
      },
      "outputs": [],
      "source": [
        "coefs_p4_enet, coefs_all_p4_enet, _, _ = elasticNet_pipeline_period(X4, y4,\n",
        "                                                                    sample_weight4,\n",
        "                                                                    \"4\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f3SEJw5oBG60"
      },
      "source": [
        "#### ElasticNet - Period 5 (August 2021 - October 2021)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Irc3oFXTBJ-1",
        "outputId": "e7574bb5-314c-47ec-d82b-77b56773ba64"
      },
      "outputs": [],
      "source": [
        "coefs_p5_enet, coefs_all_p5_enet, _, _ = elasticNet_pipeline_period(X5, y5,\n",
        "                                                                    sample_weight5,\n",
        "                                                                    \"5\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hx2POqrlBKHt"
      },
      "source": [
        "#### ElasticNet - Period 6 (November 2021 - February 2022)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "KLdIIEDKBKyk",
        "outputId": "31814740-50b7-4f59-d69d-6c8c8e2704c5"
      },
      "outputs": [],
      "source": [
        "coefs_p6_enet, coefs_all_p6_enet, _, _ = elasticNet_pipeline_period(X6, y6,\n",
        "                                                                    sample_weight6,\n",
        "                                                                    \"6\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tJ-Yhm7J9cS7"
      },
      "source": [
        "#### ElasticNet - All periods contributions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "Ee2NZOEU9c9z",
        "outputId": "57b5ca37-89d2-4bf4-d71f-67c204801731"
      },
      "outputs": [],
      "source": [
        "import plotly.graph_objects as go\n",
        "feature_names = list(X1.columns)\n",
        "\n",
        "intervals1 = (1.96 * coefs_all_p1_enet.std() / np.sqrt(len(coefs_all_p1_enet))).to_frame()\n",
        "intervals2 = (1.96 * coefs_all_p2_enet.std() / np.sqrt(len(coefs_all_p2_enet))).to_frame()\n",
        "intervals3 = (1.96 * coefs_all_p3_enet.std() / np.sqrt(len(coefs_all_p3_enet))).to_frame()\n",
        "intervals4 = (1.96 * coefs_all_p4_enet.std() / np.sqrt(len(coefs_all_p4_enet))).to_frame()\n",
        "intervals5 = (1.96 * coefs_all_p5_enet.std() / np.sqrt(len(coefs_all_p5_enet))).to_frame()\n",
        "intervals6 = (1.96 * coefs_all_p6_enet.std() / np.sqrt(len(coefs_all_p6_enet))).to_frame()\n",
        "period1_data = []\n",
        "period2_data = []\n",
        "period3_data = []\n",
        "period4_data = []\n",
        "period5_data = []\n",
        "period6_data = []\n",
        "error_p1 = []\n",
        "error_p2 = []\n",
        "error_p3 = []\n",
        "error_p4 = []\n",
        "error_p5 = []\n",
        "error_p6 = []\n",
        "for i, feat in enumerate(feature_names):\n",
        "  p1 = coefs_p1_enet.loc[feat].iloc[0]\n",
        "  p2 = coefs_p2_enet.loc[feat].iloc[0]\n",
        "  p3 = coefs_p3_enet.loc[feat].iloc[0]\n",
        "  p4 = coefs_p4_enet.loc[feat].iloc[0]\n",
        "  p5 = coefs_p5_enet.loc[feat].iloc[0]\n",
        "  p6 = coefs_p6_enet.loc[feat].iloc[0]\n",
        "  e1 = intervals1.loc[feat].iloc[0]\n",
        "  e2 = intervals2.loc[feat].iloc[0]\n",
        "  e3 = intervals3.loc[feat].iloc[0]\n",
        "  e4 = intervals4.loc[feat].iloc[0]\n",
        "  e5 = intervals5.loc[feat].iloc[0]\n",
        "  e6 = intervals6.loc[feat].iloc[0]\n",
        "  period1_data.append(p1)\n",
        "  period2_data.append(p2)\n",
        "  period3_data.append(p3)\n",
        "  period4_data.append(p4)\n",
        "  period5_data.append(p5)\n",
        "  period6_data.append(p6)\n",
        "  error_p1.append(e1)\n",
        "  error_p2.append(e2)\n",
        "  error_p3.append(e3)\n",
        "  error_p4.append(e4)\n",
        "  error_p5.append(e5)\n",
        "  error_p6.append(e6)\n",
        "fig = go.Figure(data=[\n",
        "    go.Bar(name=\"Period 1 (February 2020 - May 2020)\", x=feature_names, y=period1_data,\n",
        "           error_y=dict(type=\"data\", array=error_p1)),\n",
        "    go.Bar(name=\"Period 2 (June 2020 - October 2020)\", x=feature_names, y=period2_data,\n",
        "           error_y=dict(type=\"data\", array=error_p2)),\n",
        "    go.Bar(name=\"Period 3 (November 2020 - February 2021)\", x=feature_names, y=period3_data,\n",
        "           error_y=dict(type=\"data\", array=error_p3)),\n",
        "    go.Bar(name=\"Period 4 (March 2021 - July 2021)\", x=feature_names, y=period4_data,\n",
        "           error_y=dict(type=\"data\", array=error_p4)),\n",
        "    go.Bar(name=\"Period 5 (August 2021 - October 2021)\", x=feature_names, y=period5_data,\n",
        "           error_y=dict(type=\"data\", array=error_p5)),\n",
        "    go.Bar(name=\"Period 6 (November 2021 - February 2022)\", x=feature_names, y=period6_data,\n",
        "           error_y=dict(type=\"data\", array=error_p6))\n",
        "])\n",
        "# Change the bar mode\n",
        "fig.update_layout(barmode=\"group\",\n",
        "                  xaxis_title=\"Features\",\n",
        "                  yaxis_title=\"Coefficients importance and its variability\",\n",
        "                  legend_title=\"Periods\",\n",
        "                  title=\"Coefficients importance and its variability (95% confidence interval)<br> for each period (MinMaxScaler & ElasticNet)\")\n",
        "fig.update_xaxes(tickangle=-45)\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "W00dIiP69QUR",
        "outputId": "3f1d4e4e-c5e9-41b2-d8d2-c70ea9952cf4"
      },
      "outputs": [],
      "source": [
        "directionality = pd.DataFrame()\n",
        "directionality[\"Variable\"] = feature_names\n",
        "directionality[\"Period 1\"] = period1_data\n",
        "directionality[\"Period 2\"] = period2_data\n",
        "directionality[\"Period 3\"] = period3_data\n",
        "directionality[\"Period 4\"] = period4_data\n",
        "directionality[\"Period 5\"] = period5_data\n",
        "directionality[\"Period 6\"] = period6_data\n",
        "for i in range(1, 7):\n",
        "  directionality[f\"Period {i}\"] = directionality[f\"Period {i}\"].apply(lambda x: \"+\" if x >= 0 else \"-\")\n",
        "directionality"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KONciFtDu6GF"
      },
      "source": [
        "#### Heatmap"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mbSGAt3wyO_m"
      },
      "source": [
        "##### Heatmap coefficients"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "MMABO2lBvUUb",
        "outputId": "eaece711-391c-43d3-b589-d2cff1f8ec5a"
      },
      "outputs": [],
      "source": [
        "import plotly.graph_objects as go\n",
        "import plotly.subplots as sp\n",
        "\n",
        "allCoefficients = pd.DataFrame()\n",
        "allCoefficients[\"Period 1\"] = period1_data\n",
        "allCoefficients[\"Period 2\"] = period2_data\n",
        "allCoefficients[\"Period 3\"] = period3_data\n",
        "allCoefficients[\"Period 4\"] = period4_data\n",
        "allCoefficients[\"Period 5\"] = period5_data\n",
        "allCoefficients[\"Period 6\"] = period6_data\n",
        "allCoefficients.index = feature_names\n",
        "\n",
        "rows = (len(list(allCoefficients.index)) // 10) + 1\n",
        "fig = sp.make_subplots(rows=rows, cols=1)\n",
        "\n",
        "for row in range(rows):\n",
        "  fig.add_trace(go.Heatmap(z=allCoefficients[10*row:10*(row+1)].T.to_numpy(),\n",
        "                           x=list(allCoefficients.index)[10*row:10*(row+1)],\n",
        "                           y=[\"Period 1\", \"Period 2\", \"Period 3\", \"Period 4\",\n",
        "                              \"Period 5\", \"Period 6\"],\n",
        "                           colorscale=\"RdBu_r\",\n",
        "                           zmid=0),\n",
        "                row=(row+1), col=1)\n",
        "\n",
        "fig.update_xaxes(tickangle=-45)\n",
        "fig.update_layout(height=rows*600, width=900,\n",
        "                  title_text=f\"Coefficients for each features for each periods\",\n",
        "                  xaxis_title=\"Features\",\n",
        "                  yaxis_title=\"Periods\")\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aS1h2le8yQbg"
      },
      "source": [
        "##### Heatmap rank"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "C2Z7QOG-ySkx",
        "outputId": "ec0e5fa3-c841-4c1b-83e0-9f357df8c378"
      },
      "outputs": [],
      "source": [
        "import plotly.graph_objects as go\n",
        "import plotly.subplots as sp\n",
        "\n",
        "allRanks = pd.DataFrame()\n",
        "allRanks[\"Period 1\"] = 1 + (np.array(period1_data).argsort().argsort())\n",
        "allRanks[\"Period 2\"] = 1 + (np.array(period2_data).argsort().argsort())\n",
        "allRanks[\"Period 3\"] = 1 + (np.array(period3_data).argsort().argsort())\n",
        "allRanks[\"Period 4\"] = 1 + (np.array(period4_data).argsort().argsort())\n",
        "allRanks[\"Period 5\"] = 1 + (np.array(period5_data).argsort().argsort())\n",
        "allRanks[\"Period 6\"] = 1 + (np.array(period6_data).argsort().argsort())\n",
        "allRanks.index = feature_names\n",
        "\n",
        "rows = (len(list(allRanks.index)) // 10) + 1\n",
        "fig = sp.make_subplots(rows=rows, cols=1)\n",
        "\n",
        "for row in range(rows):\n",
        "  fig.add_trace(go.Heatmap(z=allRanks[10*row:10*(row+1)].T.to_numpy(),\n",
        "                           x=list(allRanks.index)[10*row:10*(row+1)],\n",
        "                           y=[\"Period 1\", \"Period 2\", \"Period 3\", \"Period 4\",\n",
        "                              \"Period 5\", \"Period 6\"],\n",
        "                           colorscale=\"RdBu\",\n",
        "                           zmid=len(allRanks)//2),\n",
        "                row=(row+1), col=1)\n",
        "\n",
        "fig.update_xaxes(tickangle=-45)\n",
        "fig.update_layout(height=rows*600, width=900,\n",
        "                  title_text=f\"Rank for each features for each periods (high rank=lower value)\",\n",
        "                  xaxis_title=\"Features\",\n",
        "                  yaxis_title=\"Periods\")\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TwC4bWRJCnTR"
      },
      "source": [
        "### MinMaxScaler & confidence intervals (RandomForest) (Dataset 2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EdT7EKIzFNgC"
      },
      "source": [
        "#### Define Datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f2onhHtADmH6",
        "outputId": "a077b75f-eaec-4725-f998-7792c7cfcd43"
      },
      "outputs": [],
      "source": [
        "dt_list, X_list, y_list, \\\n",
        "sample_weight_list, groups_list = virus_spread_dt(county_database2_imputed,\n",
        "                                                  selected_columns_list,\n",
        "                                                  X_selected_columns_list,\n",
        "                                                  type_data=\"COVID-19\",\n",
        "                                                  plot=False)\n",
        "dt1, dt2, dt3, dt4, dt5, dt6 = dt_list\n",
        "X1, X2, X3, X4, X5, X6 = X_list\n",
        "y1, y2, y3, y4, y5, y6 = y_list\n",
        "sample_weight1, sample_weight2, sample_weight3, \\\n",
        "  sample_weight4, sample_weight5, sample_weight6 = sample_weight_list\n",
        "groups1, groups2, groups3, groups4, groups5, \\\n",
        "  groups6 = groups_list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "spap_k2oCxs1"
      },
      "source": [
        "#### RandomForest - Period 1 (February 2020 - May 2020)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "tnjslLyQDFKR",
        "outputId": "10e1ec7b-44f8-4135-adb3-214d3620b841"
      },
      "outputs": [],
      "source": [
        "importances1, _ = randomForest_pipeline_period(X1, y1, sample_weight1, \"1\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8yCFRugJC11O"
      },
      "source": [
        "#### RandomForest - Period 2 (June 2020 - October 2020)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rMDz7itODTFI",
        "outputId": "cc56a77c-16df-46a9-893b-83602e0642ce"
      },
      "outputs": [],
      "source": [
        "importances2, _ = randomForest_pipeline_period(X2, y2, sample_weight2, \"2\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DcS4oD7BC56t"
      },
      "source": [
        "#### RandomForest - Period 3 (November 2020 - February 2021)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GYYLTgHnDZP6"
      },
      "outputs": [],
      "source": [
        "importances3, _ = randomForest_pipeline_period(X3, y3, sample_weight3, \"3\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iuSGXoseFuaf"
      },
      "source": [
        "#### RandomForest - Period 4 (March 2021 - July 2021)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ZNCgTclFyVm"
      },
      "outputs": [],
      "source": [
        "importances4, _ = randomForest_pipeline_period(X4, y4, sample_weight4, \"4\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Qq3i74xFylR"
      },
      "source": [
        "#### RandomForest - Period 5 (August 2021 - October 2021)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QQNMUAL-F3FL"
      },
      "outputs": [],
      "source": [
        "importances5, _ = randomForest_pipeline_period(X5, y5, sample_weight5, \"5\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y4fFVj1yF3US"
      },
      "source": [
        "#### RandomForest - Period 6 (November 2021 - February 2022)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bPmW7kpCloV_"
      },
      "outputs": [],
      "source": [
        "importances6, _ = randomForest_pipeline_period(X6, y6, sample_weight6, \"6\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SWjv6lvsC_l8"
      },
      "source": [
        "#### RandomForest - All periods contributions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qN5b9f6qDCbX"
      },
      "outputs": [],
      "source": [
        "import plotly.graph_objects as go\n",
        "feature_names = list(X1.columns)\n",
        "\n",
        "period1_data = []\n",
        "period2_data = []\n",
        "period3_data = []\n",
        "period4_data = []\n",
        "period5_data = []\n",
        "period6_data = []\n",
        "error_p1 = []\n",
        "error_p2 = []\n",
        "error_p3 = []\n",
        "error_p4 = []\n",
        "error_p5 = []\n",
        "error_p6 = []\n",
        "for i, feat in enumerate(feature_names):\n",
        "  p1 = importances1.loc[feat][\"Feature importances using MDI - Virus Spread Period 1 (RandomForestRegressor)\\n95% confidence interval\"]\n",
        "  p2 = importances2.loc[feat][\"Feature importances using MDI - Virus Spread Period 2 (RandomForestRegressor)\\n95% confidence interval\"]\n",
        "  p3 = importances3.loc[feat][\"Feature importances using MDI - Virus Spread Period 3 (RandomForestRegressor)\\n95% confidence interval\"]\n",
        "  p4 = importances4.loc[feat][\"Feature importances using MDI - Virus Spread Period 4 (RandomForestRegressor)\\n95% confidence interval\"]\n",
        "  p5 = importances5.loc[feat][\"Feature importances using MDI - Virus Spread Period 5 (RandomForestRegressor)\\n95% confidence interval\"]\n",
        "  p6 = importances6.loc[feat][\"Feature importances using MDI - Virus Spread Period 6 (RandomForestRegressor)\\n95% confidence interval\"]\n",
        "  e1 = importances1.loc[feat][\"errors\"]\n",
        "  e2 = importances2.loc[feat][\"errors\"]\n",
        "  e3 = importances3.loc[feat][\"errors\"]\n",
        "  e4 = importances4.loc[feat][\"errors\"]\n",
        "  e5 = importances5.loc[feat][\"errors\"]\n",
        "  e6 = importances6.loc[feat][\"errors\"]\n",
        "  period1_data.append(p1)\n",
        "  period2_data.append(p2)\n",
        "  period3_data.append(p3)\n",
        "  period4_data.append(p4)\n",
        "  period5_data.append(p5)\n",
        "  period6_data.append(p6)\n",
        "  error_p1.append(e1)\n",
        "  error_p2.append(e2)\n",
        "  error_p3.append(e3)\n",
        "  error_p4.append(e4)\n",
        "  error_p5.append(e5)\n",
        "  error_p6.append(e6)\n",
        "fig = go.Figure(data=[\n",
        "    go.Bar(name=\"Period 1 (February 2020 - May 2020)\", x=feature_names, y=period1_data,\n",
        "           error_y=dict(type=\"data\", array=error_p1)),\n",
        "    go.Bar(name=\"Period 2 (June 2020 - October 2020)\", x=feature_names, y=period2_data,\n",
        "           error_y=dict(type=\"data\", array=error_p2)),\n",
        "    go.Bar(name=\"Period 3 (November 2020 - February 2021)\", x=feature_names, y=period3_data,\n",
        "           error_y=dict(type=\"data\", array=error_p3)),\n",
        "    go.Bar(name=\"Period 4 (March 2021 - July 2021)\", x=feature_names, y=period4_data,\n",
        "           error_y=dict(type=\"data\", array=error_p4)),\n",
        "    go.Bar(name=\"Period 5 (August 2021 - October 2021)\", x=feature_names, y=period5_data,\n",
        "           error_y=dict(type=\"data\", array=error_p5)),\n",
        "    go.Bar(name=\"Period 6 (November 2021 - February 2022)\", x=feature_names, y=period6_data,\n",
        "           error_y=dict(type=\"data\", array=error_p6))\n",
        "])\n",
        "# Change the bar mode\n",
        "fig.update_layout(barmode=\"group\",\n",
        "                  xaxis_title=\"Features\",\n",
        "                  yaxis_title=\"Feature importance and its variability (across all trees)\",\n",
        "                  legend_title=\"Periods\",\n",
        "                  title=\"Coefficients importance and its variability (across all trees) (95% confidence interval)<br> for each period (MinMaxScaler & RandomForestRegressor)\")\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uHPZkOq9l1hv"
      },
      "source": [
        "## Effect of imputing data, sample weights ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qg6W41Y9oVRQ"
      },
      "source": [
        "#### Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SQ59URhWthpa"
      },
      "source": [
        "##### Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lQqmL3utl3KK"
      },
      "outputs": [],
      "source": [
        "import plotly.graph_objects as go\n",
        "\n",
        "\n",
        "def effect_state_stratify(period=1, type_data=\"COVID-19\", impute=True,\n",
        "                          use_sample_weight=True,\n",
        "                          model_hyperparameters=None):\n",
        "\n",
        "  if impute:\n",
        "    county_db = county_databases[type_data][\"county_database2_imputed\"]\n",
        "  else:\n",
        "    county_db = county_databases[type_data][\"county_database2\"]\n",
        "\n",
        "  # Stratify shuffle split\n",
        "  dt_list, X_list, y_list, \\\n",
        "  sample_weight_list, groups_list = virus_spread_dt(county_db,\n",
        "                                                    selected_columns_list,\n",
        "                                                    X_selected_columns_list,\n",
        "                                                    type_data,\n",
        "                                                    plot=False)\n",
        "\n",
        "  all_equal_weights = np.array([1 for _ in range(len(sample_weight_list[period-1]))])\n",
        "\n",
        "  if use_sample_weight:\n",
        "    weights = sample_weight_list[period-1]\n",
        "  else:\n",
        "    weights = all_equal_weights\n",
        "\n",
        "  try:\n",
        "    _, _, _, cv_model_stratify = elasticNet_pipeline_period(X_list[period-1],\n",
        "                                                            y_list[period-1],\n",
        "                                                            weights,\n",
        "                                                            f\"{period}\",\n",
        "                                                            groups=groups_list[period-1],\n",
        "                                                            plot=False,\n",
        "                                                            model_hyperparameters=model_hyperparameters)\n",
        "  except ValueError as e:\n",
        "    print(e)\n",
        "    print(f\"Can't stratify because some states represented in the dataset don't contains enough counties (period={period})\")\n",
        "    _, _, _, cv_model_stratify = elasticNet_pipeline_period(X_list[period-1],\n",
        "                                                            y_list[period-1],\n",
        "                                                            weights,\n",
        "                                                            f\"{period}\",\n",
        "                                                            groups=None,\n",
        "                                                            plot=False,\n",
        "                                                            model_hyperparameters=model_hyperparameters)\n",
        "  # No stratification\n",
        "  _, _, _, cv_model = elasticNet_pipeline_period(X_list[period-1],\n",
        "                                                 y_list[period-1],\n",
        "                                                 weights,\n",
        "                                                 f\"{period}\",\n",
        "                                                 groups=None,\n",
        "                                                 plot=False,\n",
        "                                                 model_hyperparameters=model_hyperparameters)\n",
        "  # Evaluate and compare the performances (MAPE) of the two models\n",
        "\n",
        "  mape = -np.nanmean(cv_model[\"test_score\"])\n",
        "  mape_stratify = -np.nanmean(cv_model_stratify[\"test_score\"])\n",
        "  mape_err = 1.96 * np.nanstd(cv_model[\"test_score\"]) / np.sqrt(np.sum(~np.isnan(cv_model[\"test_score\"])))\n",
        "  mape_stratify_err = 1.96 * np.nanstd(cv_model_stratify[\"test_score\"]) / np.sqrt(np.sum(~np.isnan(cv_model_stratify[\"test_score\"])))\n",
        "  return mape, mape_stratify, mape_err, mape_stratify_err\n",
        "\n",
        "def effect_imputing_data(period=1, type_data=\"COVID-19\", stratify=True,\n",
        "                         use_sample_weight=True,\n",
        "                         model_hyperparameters=None):\n",
        "\n",
        "  county_db = county_databases[type_data][\"county_database2_imputed\"]\n",
        "\n",
        "  # Impute data\n",
        "  dt_list_imp, X_list_imp, y_list_imp, \\\n",
        "  sample_weight_list_imp, groups_list_imp = virus_spread_dt(county_db,\n",
        "                                                            selected_columns_list,\n",
        "                                                            X_selected_columns_list,\n",
        "                                                            type_data,\n",
        "                                                            plot=False)\n",
        "\n",
        "  all_equal_weights = np.array([1 for _ in range(len(sample_weight_list_imp[period-1]))])\n",
        "\n",
        "  if use_sample_weight:\n",
        "    weights_imputed = sample_weight_list_imp[period-1]\n",
        "  else:\n",
        "    weights_imputed = all_equal_weights\n",
        "  \n",
        "  if stratify:\n",
        "    groups = groups_list_imp[period-1]\n",
        "  else:\n",
        "    groups = None\n",
        "\n",
        "  try:\n",
        "    _, _, _, cv_model_imputed = elasticNet_pipeline_period(X_list_imp[period-1],\n",
        "                                                           y_list_imp[period-1],\n",
        "                                                           weights_imputed,\n",
        "                                                           f\"{period}\",\n",
        "                                                           groups=groups,\n",
        "                                                           plot=False,\n",
        "                                                           model_hyperparameters=model_hyperparameters)\n",
        "  except ValueError as e:\n",
        "    print(e)\n",
        "    print(f\"Can't stratify because some states represented in the dataset don't contains enough counties (period={period})\")\n",
        "    _, _, _, cv_model_imputed = elasticNet_pipeline_period(X_list_imp[period-1],\n",
        "                                                           y_list_imp[period-1],\n",
        "                                                           weights_imputed,\n",
        "                                                           f\"{period}\",\n",
        "                                                           groups=None,\n",
        "                                                           plot=False,\n",
        "                                                           model_hyperparameters=model_hyperparameters)\n",
        "  # Regular data\n",
        "\n",
        "  county_db = county_databases[type_data][\"county_database2\"]\n",
        "\n",
        "  dt_list, X_list, y_list, \\\n",
        "  sample_weight_list, groups_list = virus_spread_dt(county_db,\n",
        "                                                    selected_columns_list,\n",
        "                                                    X_selected_columns_list,\n",
        "                                                    type_data,\n",
        "                                                    plot=False)\n",
        "\n",
        "  all_equal_weights = np.array([1 for _ in range(len(sample_weight_list[period-1]))])\n",
        "\n",
        "  if use_sample_weight:\n",
        "    weights = sample_weight_list[period-1]\n",
        "  else:\n",
        "    weights = all_equal_weights\n",
        "  \n",
        "  if stratify:\n",
        "    groups = groups_list[period-1]\n",
        "  else:\n",
        "    groups = None\n",
        "\n",
        "  try:\n",
        "    _, _, _, cv_model = elasticNet_pipeline_period(X_list[period-1],\n",
        "                                                   y_list[period-1],\n",
        "                                                   weights,\n",
        "                                                   f\"{period}\",\n",
        "                                                   groups=groups,\n",
        "                                                   plot=False,\n",
        "                                                   model_hyperparameters=model_hyperparameters)\n",
        "  except ValueError as e:\n",
        "    print(e)\n",
        "    print(f\"Can't stratify because some states represented in the dataset don't contains enough counties (period={period})\")\n",
        "    _, _, _, cv_model = elasticNet_pipeline_period(X_list[period-1],\n",
        "                                                   y_list[period-1],\n",
        "                                                   weights,\n",
        "                                                   f\"{period}\",\n",
        "                                                   groups=None,\n",
        "                                                   plot=False,\n",
        "                                                   model_hyperparameters=model_hyperparameters)\n",
        "\n",
        "  # Evaluate and compare the performances (MAPE) of the two models\n",
        "  mape = -np.nanmean(cv_model[\"test_score\"])\n",
        "  mape_imputed = -np.nanmean(cv_model_imputed[\"test_score\"])\n",
        "  mape_err = 1.96 * np.nanstd(cv_model[\"test_score\"]) / np.sqrt(np.sum(~np.isnan(cv_model[\"test_score\"])))\n",
        "  mape_imputed_err = 1.96 * np.nanstd(cv_model_imputed[\"test_score\"]) / np.sqrt(np.sum(~np.isnan(cv_model_imputed[\"test_score\"])))\n",
        "  return mape, mape_imputed, mape_err, mape_imputed_err\n",
        "\n",
        "\n",
        "def effect_sample_weight(period=1, type_data=\"COVID-19\", impute=True,\n",
        "                         stratify=True,\n",
        "                         model_hyperparameters=None):\n",
        "\n",
        "  if impute:\n",
        "    county_db = county_databases[type_data][\"county_database2_imputed\"]\n",
        "  else:\n",
        "    county_db = county_databases[type_data][\"county_database2\"]\n",
        "\n",
        "  # Sample weight\n",
        "  dt_list, X_list, y_list, \\\n",
        "  sample_weight_list, groups_list = virus_spread_dt(county_db,\n",
        "                                                    selected_columns_list,\n",
        "                                                    X_selected_columns_list,\n",
        "                                                    type_data,\n",
        "                                                    plot=False)\n",
        "  \n",
        "  if stratify:\n",
        "    groups = groups_list[period-1]\n",
        "  else:\n",
        "    groups = None\n",
        "\n",
        "  try:\n",
        "    _, _, _, cv_model_sample_weight = elasticNet_pipeline_period(X_list[period-1],\n",
        "                                                                 y_list[period-1],\n",
        "                                                                 sample_weight_list[period-1],\n",
        "                                                                 f\"{period}\",\n",
        "                                                                 groups=groups,\n",
        "                                                                 plot=False,\n",
        "                                                                 model_hyperparameters=model_hyperparameters)\n",
        "  except ValueError as e:\n",
        "    print(e)\n",
        "    print(f\"Can't stratify because some states represented in the dataset don't contains enough counties (period={period})\")\n",
        "    _, _, _, cv_model_sample_weight = elasticNet_pipeline_period(X_list[period-1],\n",
        "                                                                 y_list[period-1],\n",
        "                                                                 sample_weight_list[period-1],\n",
        "                                                                 f\"{period}\",\n",
        "                                                                 groups=None,\n",
        "                                                                 plot=False,\n",
        "                                                                 model_hyperparameters=model_hyperparameters)\n",
        "\n",
        "  # No sample weight (only 1)\n",
        "  dt_list, X_list, y_list, \\\n",
        "  sample_weight_list, groups_list = virus_spread_dt(county_db,\n",
        "                                                    selected_columns_list,\n",
        "                                                    X_selected_columns_list,\n",
        "                                                    type_data,\n",
        "                                                    plot=False)\n",
        "  if stratify:\n",
        "    groups = groups_list[period-1]\n",
        "  else:\n",
        "    groups = None\n",
        "\n",
        "  all_equal_weights = np.array([1 for _ in range(len(sample_weight_list[period-1]))])\n",
        "  \n",
        "  try:\n",
        "    _, _, _, cv_model = elasticNet_pipeline_period(X_list[period-1],\n",
        "                                                   y_list[period-1],\n",
        "                                                   all_equal_weights,\n",
        "                                                   f\"{period}\",\n",
        "                                                   groups=groups,\n",
        "                                                   plot=False,\n",
        "                                                   model_hyperparameters=model_hyperparameters)\n",
        "  except ValueError as e:\n",
        "    print(e)\n",
        "    print(f\"Can't stratify because some states represented in the dataset don't contains enough counties (period={period})\")\n",
        "    _, _, _, cv_model = elasticNet_pipeline_period(X_list[period-1],\n",
        "                                                   y_list[period-1],\n",
        "                                                   all_equal_weights,\n",
        "                                                   f\"{period}\",\n",
        "                                                   groups=None,\n",
        "                                                   plot=False,\n",
        "                                                   model_hyperparameters=model_hyperparameters)\n",
        "\n",
        "  # Evaluate and compare the performances (MAPE) of the two models\n",
        "  mape = -np.nanmean(cv_model[\"test_score\"])\n",
        "  mape_sample_weight = -np.nanmean(cv_model_sample_weight[\"test_score\"])\n",
        "  mape_err = 1.96 * np.nanstd(cv_model[\"test_score\"]) / np.sqrt(np.sum(~np.isnan(cv_model[\"test_score\"])))\n",
        "  mape_sample_weight_err = 1.96 * np.nanstd(cv_model_sample_weight[\"test_score\"]) / np.sqrt(np.sum(~np.isnan(cv_model_sample_weight[\"test_score\"])))\n",
        "  return mape, mape_sample_weight, mape_err, mape_sample_weight_err\n",
        "\n",
        "def effect_correlated_features(period=1, type_data=\"COVID-19\", impute=True,\n",
        "                               stratify=True, use_sample_weight=True,\n",
        "                               model_hyperparameters=None):\n",
        "\n",
        "  if impute:\n",
        "    county_db = county_databases[type_data][\"county_database2_imputed\"]\n",
        "  else:\n",
        "    county_db = county_databases[type_data][\"county_database2\"]\n",
        "  \n",
        "  # Remove correlated features\n",
        "  nocorr_dt_list, nocorr_X_list, nocorr_y_list, \\\n",
        "  nocorr_sample_weight_list, nocorr_groups_list = virus_spread_dt(county_db,\n",
        "                                                                  selected_columns_list,\n",
        "                                                                  X_selected_columns_list,\n",
        "                                                                  type_data,\n",
        "                                                                  plot=False)\n",
        "\n",
        "  if stratify:\n",
        "    nocorr_groups = nocorr_groups_list[period-1]\n",
        "  else:\n",
        "    nocorr_groups = None\n",
        "\n",
        "  all_equal_weights = np.array([1 for _ in range(len(nocorr_sample_weight_list[period-1]))])\n",
        "\n",
        "  if use_sample_weight:\n",
        "    nocorr_weights = nocorr_sample_weight_list[period-1]\n",
        "  else:\n",
        "    nocorr_weights = all_equal_weights\n",
        "\n",
        "  try:\n",
        "    _, _, _, cv_model_nocorr = elasticNet_pipeline_period(nocorr_X_list[period-1],\n",
        "                                                          nocorr_y_list[period-1],\n",
        "                                                          nocorr_weights,\n",
        "                                                          f\"{period}\",\n",
        "                                                          groups=nocorr_groups,\n",
        "                                                          plot=False,\n",
        "                                                          model_hyperparameters=model_hyperparameters)\n",
        "  except ValueError as e:\n",
        "    print(e)\n",
        "    print(f\"Can't stratify because some states represented in the dataset don't contains enough counties (period={period})\")\n",
        "    _, _, _, cv_model_nocorr = elasticNet_pipeline_period(nocorr_X_list[period-1],\n",
        "                                                          nocorr_y_list[period-1],\n",
        "                                                          nocorr_weights,\n",
        "                                                          f\"{period}\",\n",
        "                                                          groups=None,\n",
        "                                                          plot=False,\n",
        "                                                          model_hyperparameters=model_hyperparameters)\n",
        "  \n",
        "  # Keep all numerical features (without response variable etc)\n",
        "  all_columns = list(county_database2_imputed.select_dtypes([\"number\"]).columns)\n",
        "  all_columns.remove(\"FIPS\")\n",
        "  X_all_columns = all_columns.copy()\n",
        "  for c in [\"total_pop\", \"county_area\"]:\n",
        "    X_all_columns.remove(c)\n",
        "  for per in range(1, 7):\n",
        "    X_all_columns.remove(f\"deathCount_period{per}\")\n",
        "    X_all_columns.remove(f\"deathRate_period{per}\")\n",
        "  \n",
        "  stringency_columns = [f\"StringencyIndex{i}_{s}\" for i in range(period+1, 7) for s in [\"mean\", \"std\", \"median\", \"min\", \"max\"]]\n",
        "  for c in stringency_columns:\n",
        "    X_all_columns.remove(c)\n",
        "  if period != 1:  # drop distance to airport\n",
        "    X_all_columns.remove(\"min_distance_top_airport\")\n",
        "\n",
        "  # convert to a list of all_columns\n",
        "  all_columns_list = [all_columns for _ in range(7)]\n",
        "  X_all_columns_list = [X_all_columns for _ in range(7)]\n",
        "\n",
        "  dt_list, X_list, y_list, \\\n",
        "  sample_weight_list, groups_list = virus_spread_dt(county_db,\n",
        "                                                    all_columns_list,\n",
        "                                                    X_all_columns_list,\n",
        "                                                    type_data,\n",
        "                                                    plot=False)\n",
        "  if stratify:\n",
        "    groups = groups_list[period-1]\n",
        "  else:\n",
        "    groups = None\n",
        "\n",
        "  all_equal_weights = np.array([1 for _ in range(len(sample_weight_list[period-1]))])\n",
        "\n",
        "  if use_sample_weight:\n",
        "    weights = sample_weight_list[period-1]\n",
        "  else:\n",
        "    weights = all_equal_weights\n",
        "\n",
        "  try:\n",
        "    _, _, _, cv_model = elasticNet_pipeline_period(X_list[period-1],\n",
        "                                                   y_list[period-1],\n",
        "                                                   weights,\n",
        "                                                   f\"{period}\",\n",
        "                                                   groups=groups,\n",
        "                                                   plot=False,\n",
        "                                                   model_hyperparameters=model_hyperparameters)\n",
        "  except ValueError as e:\n",
        "    print(e)\n",
        "    print(f\"Can't stratify because some states represented in the dataset don't contains enough counties (period={period})\")\n",
        "    _, _, _, cv_model = elasticNet_pipeline_period(X_list[period-1],\n",
        "                                                   y_list[period-1],\n",
        "                                                   weights,\n",
        "                                                   f\"{period}\",\n",
        "                                                   groups=None,\n",
        "                                                   plot=False,\n",
        "                                                   model_hyperparameters=model_hyperparameters)\n",
        "\n",
        "  # Evaluate and compare the performances (MAPE) of the two models\n",
        "  mape = -np.nanmean(cv_model[\"test_score\"])\n",
        "  mape_nocorr = -np.nanmean(cv_model_nocorr[\"test_score\"])\n",
        "  mape_err = 1.96 * np.nanstd(cv_model[\"test_score\"]) / np.sqrt(np.sum(~np.isnan(cv_model[\"test_score\"])))\n",
        "  mape_nocorr_err = 1.96 * np.nanstd(cv_model_nocorr[\"test_score\"]) / np.sqrt(np.sum(~np.isnan(cv_model_nocorr[\"test_score\"])))\n",
        "  return mape, mape_nocorr, mape_err, mape_nocorr_err"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-2kL1iqC7S8l"
      },
      "source": [
        "##### Select one set of hyperparameters (to compare only one effect across all periods)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4c0qmOxE7RmM"
      },
      "outputs": [],
      "source": [
        "model_hyperparameters = {\"elasticnet__alpha\": 0.05,\n",
        "                         \"elasticnet__l1_ratio\": 1}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LuVLLH_8Fc6c"
      },
      "source": [
        "##### Effect of stratify (represent all states in each fold) during cross validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 617
        },
        "id": "NG-RS18mFkaP",
        "outputId": "4ce92cbb-0fdc-4b49-e935-6e0a371a16c1"
      },
      "outputs": [],
      "source": [
        "# Parameters\n",
        "type_data = \"COVID-19\"\n",
        "use_sample_weight = True\n",
        "impute = True\n",
        "\n",
        "# Compute the metrics for all the periods\n",
        "mape_list = []\n",
        "mape_err_list = []\n",
        "mape_stratify_list = []\n",
        "mape_stratify_err_list = []\n",
        "for period in tqdm(range(1, 7)):\n",
        "  mape, mape_stratify, \\\n",
        "    mape_err, mape_stratify_err = effect_state_stratify(period=period,\n",
        "                                                        type_data=type_data,\n",
        "                                                        use_sample_weight=use_sample_weight,\n",
        "                                                        impute=impute,\n",
        "                                                        model_hyperparameters=model_hyperparameters)\n",
        "  mape_list.append(mape)\n",
        "  mape_stratify_list.append(mape_stratify)\n",
        "  mape_err_list.append(mape_err)\n",
        "  mape_stratify_err_list.append(mape_stratify_err)\n",
        "\n",
        "period_list = [f\"Period {i}\" for i in range(1, 7)]\n",
        "fig = go.Figure(data=[\n",
        "    go.Bar(name=\"Normal Shuffle Split\", x=period_list, y=mape_list,\n",
        "           error_y=dict(type=\"data\", array=mape_err_list)),\n",
        "    go.Bar(name=\"Stratified Shuffle Split on the state variable\", x=period_list,\n",
        "           y=mape_stratify_list,\n",
        "           error_y=dict(type=\"data\", array=mape_stratify_err_list))\n",
        "])\n",
        "# Change the bar mode\n",
        "use_sample_weight_text = \"Use sample weights\" if use_sample_weight else \"Equal weights\"\n",
        "impute_text = \"Impute at state level\" if impute else \"No imputation\"\n",
        "fig.update_layout(barmode=\"group\",\n",
        "                  xaxis_title=\"Periods\",\n",
        "                  yaxis_title=\"Mean Average Percentage Error (MAPE)\",\n",
        "                  legend_title=\"No stratification/State stratification\",\n",
        "                  title=f\"MAPE difference for each period: dataset with and without stratified shuffle split on the state<br>Results of the cross validation after a hyperparameter tuning (MinMaxScaler & ElasticNet) (95% confidence interval)<br>{use_sample_weight_text}, {impute_text}, {type_data} crude death rate per 100k\")\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RCmBgjIZtlkn"
      },
      "source": [
        "##### Effect of imputing data at the state level"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 726
        },
        "id": "UnLI4vPRdMuL",
        "outputId": "b030f9f3-b2a6-4be8-e243-015b4dc4cd4b"
      },
      "outputs": [],
      "source": [
        "# Parameters\n",
        "type_data = \"COVID-19\"\n",
        "use_sample_weight = True\n",
        "stratify = True\n",
        "\n",
        "# Compute the metrics for all the periods\n",
        "mape_list = []\n",
        "mape_imputed_list = []\n",
        "mape_err_list = []\n",
        "mape_imputed_err_list = []\n",
        "for period in tqdm(range(1, 7)):\n",
        "  mape, mape_imputed, \\\n",
        "    mape_err, mape_imputed_err = effect_imputing_data(period=period,\n",
        "                                                      type_data=type_data,\n",
        "                                                      stratify=stratify,\n",
        "                                                      use_sample_weight=use_sample_weight,\n",
        "                                                      model_hyperparameters=model_hyperparameters)\n",
        "  mape_list.append(mape)\n",
        "  mape_imputed_list.append(mape_imputed)\n",
        "  mape_err_list.append(mape_err)\n",
        "  mape_imputed_err_list.append(mape_imputed_err)\n",
        "\n",
        "period_list = [f\"Period {i}\" for i in range(1, 7)]\n",
        "fig = go.Figure(data=[\n",
        "    go.Bar(name=\"Normal data\", x=period_list, y=mape_list,\n",
        "           error_y=dict(type=\"data\", array=mape_err_list)),\n",
        "    go.Bar(name=\"Imputed data\", x=period_list, y=mape_imputed_list,\n",
        "           error_y=dict(type=\"data\", array=mape_imputed_err_list))\n",
        "])\n",
        "# Change the bar mode\n",
        "use_sample_weight_text = \"Use sample weights\" if use_sample_weight else \"Equal weights\"\n",
        "stratify_text = \"State stratification\" if impute else \"No stratification\"\n",
        "fig.update_layout(barmode=\"group\",\n",
        "                  xaxis_title=\"Periods\",\n",
        "                  yaxis_title=\"Mean Average Percentage Error (MAPE)\",\n",
        "                  legend_title=\"Not Imputed/Imputed\",\n",
        "                  title=f\"MAPE difference for each period: dataset before and after imputation<br>Results of the cross validation after a hyperparameter tuning (MinMaxScaler & ElasticNet) (95% confidence interval)<br>{use_sample_weight_text}, {stratify_text}, {type_data} crude death rate per 100k\")\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XdfyzDh5u41Y"
      },
      "source": [
        "##### Effect of adding sample weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 653
        },
        "id": "USmQqoCNu9mU",
        "outputId": "1f4a75a6-a60a-4737-d1e4-ec23af6862e2"
      },
      "outputs": [],
      "source": [
        "# Parameters\n",
        "type_data = \"COVID-19\"\n",
        "impute = True\n",
        "stratify = True\n",
        "\n",
        "# Compute the metrics for all the periods\n",
        "mape_list = []\n",
        "mape_sample_weight_list = []\n",
        "mape_err_list = []\n",
        "mape_sample_weight_err_list = []\n",
        "for period in tqdm(range(1, 7)):\n",
        "  mape, mape_sample_weight, \\\n",
        "    mape_err, mape_sample_weight_err = effect_sample_weight(period=period,\n",
        "                                                            type_data=type_data,\n",
        "                                                            impute=impute,\n",
        "                                                            stratify=stratify,\n",
        "                                                            model_hyperparameters=model_hyperparameters)\n",
        "  mape_list.append(mape)\n",
        "  mape_sample_weight_list.append(mape_sample_weight)\n",
        "  mape_err_list.append(mape_err)\n",
        "  mape_sample_weight_err_list.append(mape_sample_weight_err)\n",
        "\n",
        "period_list = [f\"Period {i}\" for i in range(1, 7)]\n",
        "fig = go.Figure(data=[\n",
        "    go.Bar(name=\"No sample weights\", x=period_list, y=mape_list,\n",
        "           error_y=dict(type=\"data\", array=mape_err_list)),\n",
        "    go.Bar(name=\"Sample weights\", x=period_list, y=mape_sample_weight_list,\n",
        "           error_y=dict(type=\"data\", array=mape_sample_weight_err_list))\n",
        "])\n",
        "# Change the bar mode\n",
        "impute_text = \"Impute at state level\" if impute else \"No imputation\"\n",
        "stratify_text = \"State stratification\" if impute else \"No stratification\"\n",
        "fig.update_layout(barmode=\"group\",\n",
        "                  xaxis_title=\"Periods\",\n",
        "                  yaxis_title=\"Mean Average Percentage Error (MAPE)\",\n",
        "                  legend_title=\"No sample weights/Sample weights\",\n",
        "                  title=f\"MAPE difference for each period: model with and without sample weights<br>Results of the cross validation after a hyperparameter tuning (MinMaxScaler & ElasticNet) (95% confidence interval)<br>{impute_text}, {stratify_text}, {type_data} crude death rate per 100k\")\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Vzy2aUVd7N6"
      },
      "source": [
        "##### Effect of removing correlated features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 653
        },
        "id": "rqXo8-pxd91f",
        "outputId": "72f91a00-d9b4-4da9-82e7-b3f03e95469e"
      },
      "outputs": [],
      "source": [
        "# Parameters\n",
        "type_data = \"COVID-19\"\n",
        "impute = True\n",
        "stratify = True\n",
        "use_sample_weight = True\n",
        "\n",
        "# Compute the metrics for all the periods\n",
        "mape_list = []\n",
        "mape_nocorr_list = []\n",
        "mape_err_list = []\n",
        "mape_nocorr_err_list = []\n",
        "for period in tqdm(range(1, 7)):\n",
        "  mape, mape_nocorr, \\\n",
        "    mape_err, mape_nocorr_err = effect_correlated_features(period=period,\n",
        "                                                           type_data=type_data,\n",
        "                                                           impute=impute,\n",
        "                                                           stratify=stratify,\n",
        "                                                           use_sample_weight=use_sample_weight,\n",
        "                                                           model_hyperparameters=model_hyperparameters)\n",
        "  mape_list.append(mape)\n",
        "  mape_nocorr_list.append(mape_nocorr)\n",
        "  mape_err_list.append(mape_err)\n",
        "  mape_nocorr_err_list.append(mape_nocorr_err)\n",
        "\n",
        "period_list = [f\"Period {i}\" for i in range(1, 7)]\n",
        "fig = go.Figure(data=[\n",
        "    go.Bar(name=\"All numerical features\", x=period_list, y=mape_list,\n",
        "           error_y=dict(type=\"data\", array=mape_err_list)),\n",
        "    go.Bar(name=\"Remove correlated features\", x=period_list, y=mape_nocorr_list,\n",
        "           error_y=dict(type=\"data\", array=mape_nocorr_err_list))\n",
        "])\n",
        "# Change the bar mode\n",
        "impute_text = \"Impute at state level\" if impute else \"No imputation\"\n",
        "stratify_text = \"State stratification\" if impute else \"No stratification\"\n",
        "use_sample_weight_text = \"Use sample weights\" if use_sample_weight else \"Equal weights\"\n",
        "fig.update_layout(barmode=\"group\",\n",
        "                  xaxis_title=\"Periods\",\n",
        "                  yaxis_title=\"Mean Average Percentage Error (MAPE)\",\n",
        "                  legend_title=\"All features/No correlated features\",\n",
        "                  title=f\"MAPE difference for each period: model fitted with all features/without correlated features<br>Results of the cross validation after a hyperparameter tuning (MinMaxScaler & ElasticNet) (95% confidence interval)<br>{use_sample_weight_text}, {impute_text}, {stratify_text}, {type_data} crude death rate per 100k\")\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LXwFSNuyoXfE"
      },
      "source": [
        "#### Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tw3RkJnFoZ4_"
      },
      "source": [
        "##### Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WHFeYL4-obJG"
      },
      "outputs": [],
      "source": [
        "import plotly.graph_objects as go\n",
        "\n",
        "def effect_state_stratify_clf(type_data=\"COVID-19\", impute=True,\n",
        "                              use_sample_weight=True):\n",
        "\n",
        "  if impute:\n",
        "    county_db = county_databases[type_data][\"county_database2_imputed\"]\n",
        "  else:\n",
        "    county_db = county_databases[type_data][\"county_database2\"]\n",
        "\n",
        "  # Stratify shuffle split\n",
        "  dt, X, y, sample_weight, groups = virus_introduction_all(county_db,\n",
        "                                                           selected_columns_list,\n",
        "                                                           X_selected_columns_list,\n",
        "                                                           plot=False)\n",
        "\n",
        "  all_equal_weights = np.array([1 for _ in range(len(sample_weight))])\n",
        "\n",
        "  if use_sample_weight:\n",
        "    weights = sample_weight\n",
        "  else:\n",
        "    weights = all_equal_weights\n",
        "\n",
        "  try:\n",
        "    _, _, _, cv_model_stratify = lr_pipeline_period(X, y, weights, \"All\",\n",
        "                                                    groups=groups,\n",
        "                                                    plot=False)\n",
        "  except ValueError as e:\n",
        "    print(e)\n",
        "    print(f\"Can't stratify because some states represented in the dataset don't contains enough counties (period={period})\")\n",
        "    _, _, _, cv_model_stratify = lr_pipeline_period(X, y, weights, \"All\",\n",
        "                                                    groups=None,\n",
        "                                                    plot=False)\n",
        "  # No stratification\n",
        "  _, _, _, cv_model = lr_pipeline_period(X, y, weights, \"All\",\n",
        "                                         groups=None,\n",
        "                                         plot=False)\n",
        "  # Evaluate and compare the performances of the two models\n",
        "  metrics = {\"normal\": {}, \"stratify\": {},\n",
        "             \"normal_err\": {}, \"stratify_err\": {}}\n",
        "  for m in [\"auroc\", \"balanced_accuracy\", \"accuracy\", \"precision\",\n",
        "            \"recall\", \"f1\", \"auprc\", \"avg_precision\"]:\n",
        "    metrics[\"normal\"][m] = np.nanmean(cv_model[f\"test_{m}\"])\n",
        "    metrics[\"stratify\"][m] = np.nanmean(cv_model_stratify[f\"test_{m}\"])\n",
        "    metrics[\"normal_err\"][m] = 1.96 * np.nanstd(cv_model[f\"test_{m}\"]) / np.sqrt(np.sum(~np.isnan(cv_model[f\"test_{m}\"])))\n",
        "    metrics[\"stratify_err\"][m] = 1.96 * np.nanstd(cv_model_stratify[f\"test_{m}\"]) / np.sqrt(np.sum(~np.isnan(cv_model_stratify[f\"test_{m}\"])))\n",
        "  return metrics\n",
        "\n",
        "def effect_imputing_data_clf(type_data=\"COVID-19\", stratify=True,\n",
        "                             use_sample_weight=True):\n",
        "\n",
        "  county_db = county_databases[type_data][\"county_database2_imputed\"]\n",
        "\n",
        "  # Impute data\n",
        "  dt, X_imp, y_imp, sample_weight_imp, groups_imp = virus_introduction_all(county_db,\n",
        "                                                           selected_columns_list,\n",
        "                                                           X_selected_columns_list,\n",
        "                                                           plot=False)\n",
        "\n",
        "  all_equal_weights = np.array([1 for _ in range(len(sample_weight_imp))])\n",
        "\n",
        "  if use_sample_weight:\n",
        "    weights_imputed = sample_weight_imp\n",
        "  else:\n",
        "    weights_imputed = all_equal_weights\n",
        "  \n",
        "  if stratify:\n",
        "    groups = groups_imp\n",
        "  else:\n",
        "    groups = None\n",
        "\n",
        "  try:\n",
        "    _, _, _, cv_model_imputed = lr_pipeline_period(X_imp, y_imp,\n",
        "                                                   weights_imputed, \"All\",\n",
        "                                                   groups=groups,\n",
        "                                                   plot=False)\n",
        "  except ValueError as e:\n",
        "    print(e)\n",
        "    print(f\"Can't stratify because some states represented in the dataset don't contains enough counties (period={period})\")\n",
        "    _, _, _, cv_model_imputed = lr_pipeline_period(X_imp, y_imp,\n",
        "                                                   weights_imputed, \"All\",\n",
        "                                                   groups=None,\n",
        "                                                   plot=False)\n",
        "  # Regular data\n",
        "\n",
        "  county_db = county_databases[type_data][\"county_database2\"]\n",
        "\n",
        "  dt, X, y, sample_weight, groups = virus_introduction_all(county_db,\n",
        "                                                           selected_columns_list,\n",
        "                                                           X_selected_columns_list,\n",
        "                                                           plot=False)\n",
        "\n",
        "  all_equal_weights = np.array([1 for _ in range(len(sample_weight))])\n",
        "\n",
        "  if use_sample_weight:\n",
        "    weights = sample_weight\n",
        "  else:\n",
        "    weights = all_equal_weights\n",
        "  \n",
        "  if stratify:\n",
        "    groups = groups\n",
        "  else:\n",
        "    groups = None\n",
        "\n",
        "  try:\n",
        "    _, _, _, cv_model = lr_pipeline_period(X, y,\n",
        "                                           weights, \"All\",\n",
        "                                           groups=groups,\n",
        "                                           plot=False)\n",
        "  except ValueError as e:\n",
        "    print(e)\n",
        "    print(f\"Can't stratify because some states represented in the dataset don't contains enough counties (period={period})\")\n",
        "    _, _, _, cv_model = lr_pipeline_period(X, y,\n",
        "                                           weights, \"All\",\n",
        "                                           groups=None,\n",
        "                                           plot=False)\n",
        "\n",
        "  # Evaluate and compare the performances of the two models\n",
        "  metrics = {\"normal\": {}, \"imputed\": {},\n",
        "             \"normal_err\": {}, \"imputed_err\": {}}\n",
        "  for m in [\"auroc\", \"balanced_accuracy\", \"accuracy\", \"precision\",\n",
        "            \"recall\", \"f1\", \"auprc\", \"avg_precision\"]:\n",
        "    metrics[\"normal\"][m] = np.nanmean(cv_model[f\"test_{m}\"])\n",
        "    metrics[\"imputed\"][m] = np.nanmean(cv_model_imputed[f\"test_{m}\"])\n",
        "    metrics[\"normal_err\"][m] = 1.96 * np.nanstd(cv_model[f\"test_{m}\"]) / np.sqrt(np.sum(~np.isnan(cv_model[f\"test_{m}\"])))\n",
        "    metrics[\"imputed_err\"][m] = 1.96 * np.nanstd(cv_model_imputed[f\"test_{m}\"]) / np.sqrt(np.sum(~np.isnan(cv_model_imputed[f\"test_{m}\"])))\n",
        "  return metrics\n",
        "\n",
        "\n",
        "def effect_sample_weight_clf(type_data=\"COVID-19\", impute=True,\n",
        "                             stratify=True):\n",
        "\n",
        "  if impute:\n",
        "    county_db = county_databases[type_data][\"county_database2_imputed\"]\n",
        "  else:\n",
        "    county_db = county_databases[type_data][\"county_database2\"]\n",
        "\n",
        "  # Sample weight\n",
        "  dt, X, y, sample_weight, groups = virus_introduction_all(county_db,\n",
        "                                                           selected_columns_list,\n",
        "                                                           X_selected_columns_list,\n",
        "                                                           plot=False)\n",
        "  \n",
        "  if stratify:\n",
        "    groups = groups\n",
        "  else:\n",
        "    groups = None\n",
        "\n",
        "  try:\n",
        "    _, _, _, cv_model_sample_weight = lr_pipeline_period(X, y,\n",
        "                                                         sample_weight, \"All\",\n",
        "                                                         groups=groups,\n",
        "                                                         plot=False)\n",
        "  except ValueError as e:\n",
        "    print(e)\n",
        "    print(f\"Can't stratify because some states represented in the dataset don't contains enough counties (period={period})\")\n",
        "    _, _, _, cv_model_sample_weight = lr_pipeline_period(X, y,\n",
        "                                                         sample_weight, \"All\",\n",
        "                                                         groups=None,\n",
        "                                                         plot=False)\n",
        "\n",
        "  # No sample weight (only 1)\n",
        "  all_equal_weights = np.array([1 for _ in range(len(sample_weight))])\n",
        "  \n",
        "  try:\n",
        "    _, _, _, cv_model = lr_pipeline_period(X, y,\n",
        "                                           all_equal_weights, \"All\",\n",
        "                                           groups=groups,\n",
        "                                           plot=False)\n",
        "  except ValueError as e:\n",
        "    print(e)\n",
        "    print(f\"Can't stratify because some states represented in the dataset don't contains enough counties (period={period})\")\n",
        "    _, _, _, cv_model = lr_pipeline_period(X, y,\n",
        "                                           all_equal_weights, \"All\",\n",
        "                                           groups=None,\n",
        "                                           plot=False)\n",
        "\n",
        "  # Evaluate and compare the performances of the two models\n",
        "  metrics = {\"normal\": {}, \"sample_weight\": {},\n",
        "             \"normal_err\": {}, \"sample_weight_err\": {}}\n",
        "  for m in [\"auroc\", \"balanced_accuracy\", \"accuracy\", \"precision\",\n",
        "            \"recall\", \"f1\", \"auprc\", \"avg_precision\"]:\n",
        "    metrics[\"normal\"][m] = np.nanmean(cv_model[f\"test_{m}\"])\n",
        "    metrics[\"sample_weight\"][m] = np.nanmean(cv_model_sample_weight[f\"test_{m}\"])\n",
        "    metrics[\"normal_err\"][m] = 1.96 * np.nanstd(cv_model[f\"test_{m}\"]) / np.sqrt(np.sum(~np.isnan(cv_model[f\"test_{m}\"])))\n",
        "    metrics[\"sample_weight_err\"][m] = 1.96 * np.nanstd(cv_model_sample_weight[f\"test_{m}\"]) / np.sqrt(np.sum(~np.isnan(cv_model_sample_weight[f\"test_{m}\"])))\n",
        "  return metrics\n",
        "\n",
        "def effect_correlated_features_clf(type_data=\"COVID-19\", impute=True,\n",
        "                                   stratify=True, use_sample_weight=True):\n",
        "\n",
        "  if impute:\n",
        "    county_db = county_databases[type_data][\"county_database2_imputed\"]\n",
        "  else:\n",
        "    county_db = county_databases[type_data][\"county_database2\"]\n",
        "  \n",
        "  # Remove correlated features\n",
        "  nocorr_dt, nocorr_X, nocorr_y, nocorr_sample_weight, nocorr_groups = virus_introduction_all(county_db,\n",
        "                                                                                              selected_columns_list,\n",
        "                                                                                              X_selected_columns_list,\n",
        "                                                                                              plot=False)\n",
        "\n",
        "  if stratify:\n",
        "    nocorr_groups = nocorr_groups\n",
        "  else:\n",
        "    nocorr_groups = None\n",
        "\n",
        "  all_equal_weights = np.array([1 for _ in range(len(nocorr_sample_weight))])\n",
        "\n",
        "  if use_sample_weight:\n",
        "    nocorr_weights = nocorr_sample_weight\n",
        "  else:\n",
        "    nocorr_weights = all_equal_weights\n",
        "\n",
        "  try:\n",
        "    _, _, _, cv_model_nocorr = lr_pipeline_period(nocorr_X, nocorr_y,\n",
        "                                                  nocorr_weights, \"All\",\n",
        "                                                  groups=nocorr_groups,\n",
        "                                                  plot=False)\n",
        "\n",
        "  except ValueError as e:\n",
        "    print(e)\n",
        "    print(f\"Can't stratify because some states represented in the dataset don't contains enough counties (period={period})\")\n",
        "    _, _, _, cv_model_nocorr = lr_pipeline_period(nocorr_X, nocorr_y,\n",
        "                                                  nocorr_weights, \"All\",\n",
        "                                                  groups=None,\n",
        "                                                  plot=False)\n",
        "  \n",
        "  # Keep all numerical features (without response variable etc)\n",
        "  all_columns = list(county_database2_imputed.select_dtypes([\"number\"]).columns)\n",
        "  all_columns.remove(\"FIPS\")\n",
        "  X_all_columns = all_columns.copy()\n",
        "  for c in [\"total_pop\", \"county_area\"]:\n",
        "    X_all_columns.remove(c)\n",
        "  for per in range(1, 7):\n",
        "    X_all_columns.remove(f\"deathCount_period{per}\")\n",
        "    X_all_columns.remove(f\"deathRate_period{per}\")\n",
        "  \n",
        "  stringency_columns = [f\"StringencyIndex{i}_{s}\" for i in range(period+1, 7) for s in [\"mean\", \"std\", \"median\", \"min\", \"max\"]]\n",
        "  for c in stringency_columns:\n",
        "    X_all_columns.remove(c)\n",
        "  if period != 1:  # drop distance to airport\n",
        "    X_all_columns.remove(\"min_distance_top_airport\")\n",
        "\n",
        "  # convert to a list of all_columns\n",
        "  all_columns_list = [all_columns for _ in range(7)]\n",
        "  X_all_columns_list = [X_all_columns for _ in range(7)]\n",
        "\n",
        "  dt, X, y, sample_weight, groups = virus_introduction_all(county_db,\n",
        "                                                           all_columns_list,\n",
        "                                                           X_all_columns_list,\n",
        "                                                           plot=False)\n",
        "\n",
        "  if stratify:\n",
        "    groups = groups\n",
        "  else:\n",
        "    groups = None\n",
        "\n",
        "  all_equal_weights = np.array([1 for _ in range(len(sample_weight))])\n",
        "\n",
        "  if use_sample_weight:\n",
        "    weights = sample_weight\n",
        "  else:\n",
        "    weights = all_equal_weights\n",
        "\n",
        "  try:\n",
        "    _, _, _, cv_model = lr_pipeline_period(X, y,\n",
        "                                           weights, \"All\",\n",
        "                                           groups=groups,\n",
        "                                           plot=False)\n",
        "  except ValueError as e:\n",
        "    print(e)\n",
        "    print(f\"Can't stratify because some states represented in the dataset don't contains enough counties (period={period})\")\n",
        "    _, _, _, cv_model = lr_pipeline_period(X, y,\n",
        "                                           weights, \"All\",\n",
        "                                           groups=None,\n",
        "                                           plot=False)\n",
        "\n",
        "  # Evaluate and compare the performances (MAPE) of the two models\n",
        "  metrics = {\"normal\": {}, \"nocorr\": {},\n",
        "             \"normal_err\": {}, \"nocorr_err\": {}}\n",
        "  for m in [\"auroc\", \"balanced_accuracy\", \"accuracy\", \"precision\",\n",
        "            \"recall\", \"f1\", \"auprc\", \"avg_precision\"]:\n",
        "    metrics[\"normal\"][m] = np.nanmean(cv_model[f\"test_{m}\"])\n",
        "    metrics[\"nocorr\"][m] = np.nanmean(cv_model_nocorr[f\"test_{m}\"])\n",
        "    metrics[\"normal_err\"][m] = 1.96 * np.nanstd(cv_model[f\"test_{m}\"]) / np.sqrt(np.sum(~np.isnan(cv_model[f\"test_{m}\"])))\n",
        "    metrics[\"nocorr_err\"][m] = 1.96 * np.nanstd(cv_model_nocorr[f\"test_{m}\"]) / np.sqrt(np.sum(~np.isnan(cv_model_nocorr[f\"test_{m}\"])))\n",
        "  return metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EcmfsPkrw5Yf"
      },
      "source": [
        "##### Effect of stratify (represent all states in each fold) during cross validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "Fuoo-tqow_10",
        "outputId": "fb9ccb10-e0a3-460c-feac-4b468fc18be6"
      },
      "outputs": [],
      "source": [
        "# Parameters\n",
        "type_data = \"COVID-19\"\n",
        "use_sample_weight = True\n",
        "impute = True\n",
        "\n",
        "# Compute the metrics\n",
        "metrics = effect_state_stratify_clf(type_data=type_data,\n",
        "                                    use_sample_weight=use_sample_weight,\n",
        "                                    impute=impute)\n",
        "\n",
        "fig = go.Figure(data=[\n",
        "    go.Bar(name=\"Normal Shuffle Split\", x=list(metrics[\"normal\"].keys()),\n",
        "           y=list(metrics[\"normal\"].values()),\n",
        "           error_y=dict(type=\"data\", array=list(metrics[\"normal_err\"].values()))),\n",
        "    go.Bar(name=\"Stratified Shuffle Split on the state variable\",\n",
        "           x=list(metrics[\"stratify\"].keys()),\n",
        "           y=list(metrics[\"stratify\"].values()),\n",
        "           error_y=dict(type=\"data\", array=list(metrics[\"stratify_err\"].values())))\n",
        "])\n",
        "# Change the bar mode\n",
        "use_sample_weight_text = \"Use sample weights\" if use_sample_weight else \"Equal weights\"\n",
        "impute_text = \"Impute at state level\" if impute else \"No imputation\"\n",
        "fig.update_layout(barmode=\"group\",\n",
        "                  xaxis_title=\"Metric name\",\n",
        "                  yaxis_title=\"Value\",\n",
        "                  legend_title=\"No stratification/State stratification\",\n",
        "                  title=f\"Classification metrics: dataset with and without stratified shuffle split on the state<br>Results of the cross validation after a hyperparameter tuning (MinMaxScaler & LogisticRegression) (95% confidence interval)<br>{use_sample_weight_text}, {impute_text}, {type_data} introduction\")\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "99XrboUt0Iph"
      },
      "source": [
        "##### Effect of imputing data at the state level"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "50xJkvrW0JBH",
        "outputId": "1f255682-5037-4148-f8d0-53e5a46eb63c"
      },
      "outputs": [],
      "source": [
        "# Parameters\n",
        "type_data = \"COVID-19\"\n",
        "use_sample_weight = True\n",
        "stratify = True\n",
        "\n",
        "# Compute the metrics\n",
        "metrics = effect_imputing_data_clf(type_data=type_data,\n",
        "                                   stratify=stratify,\n",
        "                                   use_sample_weight=use_sample_weight)\n",
        "\n",
        "fig = go.Figure(data=[\n",
        "    go.Bar(name=\"Normal data\", x=list(metrics[\"normal\"].keys()),\n",
        "           y=list(metrics[\"normal\"].values()),\n",
        "           error_y=dict(type=\"data\", array=list(metrics[\"normal_err\"].values()))),\n",
        "    go.Bar(name=\"Imputed data\", x=list(metrics[\"imputed\"].keys()),\n",
        "           y=list(metrics[\"imputed\"].values()),\n",
        "           error_y=dict(type=\"data\", array=list(metrics[\"stratify_err\"].values())))\n",
        "])\n",
        "# Change the bar mode\n",
        "use_sample_weight_text = \"Use sample weights\" if use_sample_weight else \"Equal weights\"\n",
        "stratify_text = \"State stratification\" if impute else \"No stratification\"\n",
        "fig.update_layout(barmode=\"group\",\n",
        "                  xaxis_title=\"Metric name\",\n",
        "                  yaxis_title=\"Value\",\n",
        "                  legend_title=\"Not Imputed/Imputed\",\n",
        "                  title=f\"Classification metrics: dataset before and after imputation<br>Results of the cross validation after a hyperparameter tuning (MinMaxScaler & LogisticRegression) (95% confidence interval)<br>{use_sample_weight_text}, {stratify_text}, {type_data} introduction\")\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qSXZ6Wcs07FW"
      },
      "source": [
        "##### Effect of adding sample weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "U6h_Zfj90_Gr",
        "outputId": "1a93eb36-1041-431b-e922-fbb7f887d254"
      },
      "outputs": [],
      "source": [
        "# Parameters\n",
        "type_data = \"COVID-19\"\n",
        "impute = True\n",
        "stratify = True\n",
        "\n",
        "# Compute the metrics\n",
        "metrics = effect_sample_weight_clf(type_data=type_data,\n",
        "                                   impute=impute,\n",
        "                                   stratify=stratify)\n",
        "\n",
        "period_list = [f\"Period {i}\" for i in range(1, 7)]\n",
        "fig = go.Figure(data=[\n",
        "    go.Bar(name=\"No sample weights\", x=list(metrics[\"normal\"].keys()),\n",
        "           y=list(metrics[\"normal\"].values()),\n",
        "           error_y=dict(type=\"data\", array=list(metrics[\"normal_err\"].values()))),\n",
        "    go.Bar(name=\"Sample weights\", x=list(metrics[\"sample_weight\"].keys()),\n",
        "           y=list(metrics[\"sample_weight\"].values()),\n",
        "           error_y=dict(type=\"data\", array=list(metrics[\"sample_weight_err\"].values())))\n",
        "])\n",
        "# Change the bar mode\n",
        "impute_text = \"Impute at state level\" if impute else \"No imputation\"\n",
        "stratify_text = \"State stratification\" if impute else \"No stratification\"\n",
        "fig.update_layout(barmode=\"group\",\n",
        "                  xaxis_title=\"Metric name\",\n",
        "                  yaxis_title=\"Value\",\n",
        "                  legend_title=\"No sample weights/Sample weights\",\n",
        "                  title=f\"Classification metrics: model with and without sample weights<br>Results of the cross validation after a hyperparameter tuning (MinMaxScaler & LogisticRegression) (95% confidence interval)<br>{impute_text}, {stratify_text}, {type_data} introduction\")\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g5feceaC1YMd"
      },
      "source": [
        "##### Effect of removing correlated features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "pwJOdafi1Z5m",
        "outputId": "512ff259-3641-4a42-e12a-09a4b99a9ed7"
      },
      "outputs": [],
      "source": [
        "# Parameters\n",
        "type_data = \"COVID-19\"\n",
        "impute = True\n",
        "stratify = True\n",
        "use_sample_weight = True\n",
        "\n",
        "# Compute the metrics\n",
        "metrics = effect_correlated_features_clf(type_data=type_data,\n",
        "                                         impute=impute,\n",
        "                                         stratify=stratify,\n",
        "                                         use_sample_weight=use_sample_weight)\n",
        "\n",
        "period_list = [f\"Period {i}\" for i in range(1, 7)]\n",
        "fig = go.Figure(data=[\n",
        "    go.Bar(name=\"All numerical features\", x=list(metrics[\"normal\"].keys()),\n",
        "           y=list(metrics[\"normal\"].values()),\n",
        "           error_y=dict(type=\"data\", array=list(metrics[\"normal_err\"].values()))),\n",
        "    go.Bar(name=\"Remove correlated features\", x=list(metrics[\"nocorr\"].keys()),\n",
        "           y=list(metrics[\"nocorr\"].values()),\n",
        "           error_y=dict(type=\"data\", array=list(metrics[\"nocorr_err\"].values())))\n",
        "])\n",
        "# Change the bar mode\n",
        "impute_text = \"Impute at state level\" if impute else \"No imputation\"\n",
        "stratify_text = \"State stratification\" if impute else \"No stratification\"\n",
        "use_sample_weight_text = \"Use sample weights\" if use_sample_weight else \"Equal weights\"\n",
        "fig.update_layout(barmode=\"group\",\n",
        "                  xaxis_title=\"Metric name\",\n",
        "                  yaxis_title=\"Value\",\n",
        "                  legend_title=\"All features/No correlated features\",\n",
        "                  title=f\"Classification metrics: model fitted with all features/without correlated features<br>Results of the cross validation after a hyperparameter tuning (MinMaxScaler & LogisticRegression) (95% confidence interval)<br>{use_sample_weight_text}, {impute_text}, {stratify_text}, {type_data} introduction\")\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OjcG3tw3dwFq"
      },
      "source": [
        "## Multiple models comparison (classification & regression)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ji_Q-6uqd14G"
      },
      "source": [
        "Let's try more models (for both virus introduction and virus spread). We incorporate the extended dataset and we add the American communities as a feature."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tqgqy-QpeFzw"
      },
      "source": [
        "### Intialization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x2_H45tgimBN"
      },
      "outputs": [],
      "source": [
        "from sklearn.utils import all_estimators\n",
        "import xgboost\n",
        "import lightgbm\n",
        "from tqdm import tqdm\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import cross_validate\n",
        "from sklearn.model_selection import RepeatedKFold\n",
        "from time import perf_counter\n",
        "from sklearn.metrics import make_scorer, accuracy_score, \\\n",
        "  balanced_accuracy_score, precision_score, recall_score, f1_score, \\\n",
        "  mean_absolute_error, r2_score, mean_squared_error, roc_auc_score, \\\n",
        "  precision_recall_curve, auc, mean_absolute_percentage_error, \\\n",
        "  average_precision_score, pairwise_distances\n",
        "from sklearn.preprocessing import label_binarize\n",
        "from sklearn.utils.extmath import softmax\n",
        "\n",
        "\n",
        "def pr_auc_score(y, y_pred, **kwargs):\n",
        "  if len(y_pred.shape) == 1:  # binary classification\n",
        "    precision, recall, _ = precision_recall_curve(y, y_pred, **kwargs)\n",
        "  else:\n",
        "    classes = list(range(y_pred.shape[1]))\n",
        "    if len(classes) == 2:  # binary classification too\n",
        "        precision, recall, _ = precision_recall_curve(y, y_pred[:,1],\n",
        "                                                      **kwargs)\n",
        "    else:  # multiclass\n",
        "      Y = label_binarize(y, classes=classes)\n",
        "      precision, recall, _ = precision_recall_curve(Y.ravel(), y_pred.ravel(),\n",
        "                                                    **kwargs)\n",
        "  return auc(recall, precision)\n",
        "\n",
        "def avg_precision(y, y_pred, **kwargs):\n",
        "  if len(y_pred.shape) == 1:  # binary classification\n",
        "    return average_precision_score(y, y_pred, average=\"micro\", **kwargs)\n",
        "  classes = list(range(y_pred.shape[1]))\n",
        "  if len(classes) == 2:  # binary classification too\n",
        "    return average_precision_score(y, y_pred[:,1], average=\"micro\", **kwargs)\n",
        "  # multiclass\n",
        "  Y = label_binarize(y, classes=classes)\n",
        "  return average_precision_score(Y, y_pred, average=\"micro\", **kwargs)\n",
        "\n",
        "custom_scorer_clf = {'accuracy': make_scorer(accuracy_score,\n",
        "                                             greater_is_better=True),\n",
        "                     'balanced_accuracy': make_scorer(balanced_accuracy_score,\n",
        "                                                      greater_is_better=True),\n",
        "                     'precision': make_scorer(precision_score, average='macro'),\n",
        "                     'recall': make_scorer(recall_score, average='macro'),\n",
        "                     'f1': make_scorer(f1_score, average='macro',\n",
        "                                       greater_is_better=True),\n",
        "                     'auroc': make_scorer(roc_auc_score, multi_class=\"ovo\",\n",
        "                                          average=\"macro\",\n",
        "                                          needs_proba=True,\n",
        "                                          greater_is_better=True),\n",
        "                     'auprc': make_scorer(pr_auc_score, needs_proba=True,\n",
        "                                          greater_is_better=True),\n",
        "                     'avg_precision': make_scorer(avg_precision,\n",
        "                                                  needs_proba=True,\n",
        "                                                  greater_is_better=True)}\n",
        "def r2_score_adj(y, y_pred, n, p, fit_intercept=True):\n",
        "    if fit_intercept:\n",
        "        rsquared = 1 - np.nansum((y - y_pred) ** 2) / np.nansum((y - np.nanmean(y)) ** 2)\n",
        "        rsquared_adj = 1 - ((n - 1) / (n - p - 1)) * (1 - rsquared)\n",
        "    else:\n",
        "        rsquared = 1 - np.nansum((y - y_pred) ** 2) / np.nansum(y ** 2)\n",
        "        rsquared_adj = 1 - (n / (n - p)) * (1 - rsquared)\n",
        "    return rsquared_adj\n",
        "\n",
        "# adj r2 scorer is added in the pipeline (to retrieve the n and p parameters)\n",
        "# approximation that doesn't take into account the change of n when splitting\n",
        "# the dataset! But it's ok I guess\n",
        "custom_scorer_reg = {'r2': make_scorer(r2_score, greater_is_better=True),\n",
        "                     'rmse': make_scorer(mean_squared_error, squared=True,\n",
        "                                         greater_is_better=False),\n",
        "                     'mae': make_scorer(mean_absolute_error,\n",
        "                                        greater_is_better=False),\n",
        "                     'mape': make_scorer(mean_absolute_percentage_error,\n",
        "                                         greater_is_better=False)}\n",
        "\n",
        "CLASSIFIERS = [est for est in all_estimators(type_filter=[\"classifier\"])]\n",
        "REGRESSORS = [est for est in all_estimators(type_filter=[\"regressor\"])]\n",
        "\n",
        "CLASSIFIERS.append((\"XGBClassifier\", xgboost.XGBClassifier))\n",
        "CLASSIFIERS.append((\"LGBMClassifier\", lightgbm.LGBMClassifier))\n",
        "REGRESSORS.append((\"XGBRegressor\", xgboost.XGBRegressor))\n",
        "REGRESSORS.append((\"LGBMRegressor\", lightgbm.LGBMRegressor))\n",
        "\n",
        "classifiers_to_remove = [\"ClassifierChain\", \"GaussianProcessClassifier\",\n",
        "                         \"MultinomialNB\", \"MultiOutputClassifier\",\n",
        "                         \"NuSVC\",\n",
        "                         \"OneVsOneClassifier\",\n",
        "                         \"OneVsRestClassifier\", \"OutputCodeClassifier\",\n",
        "                         \"PassiveAggressiveClassifier\", \"StackingClassifier\",\n",
        "                         \"VotingClassifier\", \"RadiusNeighborsClassifier\",\n",
        "                         \"CategoricalNB\"]\n",
        "CLASSIFIERS = [clf for clf in CLASSIFIERS if clf[0] not in classifiers_to_remove]\n",
        "NO_SAMPLE_WEIGHT_CLASSIFIERS = [\"KNeighborsClassifier\", \"LabelPropagation\",\n",
        "                                \"LabelSpreading\", \"CategoricalNB\",\n",
        "                                \"LinearDiscriminantAnalysis\", \"MLPClassifier\",\n",
        "                                \"NearestCentroid\",\n",
        "                                \"QuadraticDiscriminantAnalysis\",\n",
        "                                \"RadiusNeighborsClassifier\"]\n",
        "NO_PREDICT_PROBA_CLASSIFIERS = [\"LinearSVC\", \"NearestCentroid\",\n",
        "                                \"Perceptron\", \"RidgeClassifier\",\n",
        "                                \"RidgeClassifierCV\"]\n",
        "regressors_to_remove = [\"CCA\", \"IsotonicRegression\", \"GammaRegressor\",\n",
        "                        \"MultiOutputRegressor\",\n",
        "                        \"MultiTaskElasticNet\", \"MultiTaskElasticNetCV\",\n",
        "                        \"MultiTaskLasso\", \"MultiTaskLassoCV\",\n",
        "                        \"PLSCanonical\", \"PLSRegression\", \"QuantileRegressor\",\n",
        "                        \"RadiusNeighborsRegressor\",\n",
        "                        \"RegressorChain\", \"StackingRegressor\",\n",
        "                        \"VotingRegressor\"]\n",
        "REGRESSORS = [reg for reg in REGRESSORS if reg[0] not in regressors_to_remove]\n",
        "NO_SAMPLE_WEIGHT_REGRESSORS = [\"ARDRegression\",\n",
        "                               \"GaussianProcessRegressor\",\n",
        "                               \"KNeighborsRegressor\", \"Lars\", \"LarsCV\",\n",
        "                               \"LassoLars\", \"LassoLarsCV\", \"LassoLarsIC\",\n",
        "                               \"MLPRegressor\", \"OrthogonalMatchingPursuit\",\n",
        "                               \"OrthogonalMatchingPursuitCV\",\n",
        "                               \"PassiveAggressiveRegressor\",\n",
        "                               \"TheilSenRegressor\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A1ywkQSyj83c"
      },
      "outputs": [],
      "source": [
        "def all_models_clf(models_to_try, no_sample_weight, no_predict_proba,\n",
        "                   custom_scorer, X, y, sample_weight=None, groups=None):\n",
        "  Accuracy = []\n",
        "  B_Accuracy = []\n",
        "  PRECISION = []\n",
        "  RECALL = []\n",
        "  F1 = []\n",
        "  AUROC = []\n",
        "  AUPRC = []\n",
        "  MICRO_AVG_PREC = []\n",
        "  names = []\n",
        "  TIME = []\n",
        "\n",
        "  for name, model in tqdm(models_to_try):\n",
        "    print(name)\n",
        "    top = perf_counter()\n",
        "    if name in no_predict_proba:  # extend the class with a custom predict proba function based on the decision function\n",
        "      if name == \"NearestCentroid\":  # no decision function, calculate pairwise distance to clusters instead\n",
        "        class model2(model):\n",
        "          def __init__(self):\n",
        "            super().__init__()\n",
        "          def predict_proba(self, X):\n",
        "            distances = pairwise_distances(X, Y=self.centroids_,\n",
        "                                           metric=\"euclidean\",\n",
        "                                           n_jobs=-1)\n",
        "            return softmax(-distances)\n",
        "      else:\n",
        "        class model2(model):\n",
        "          def __init__(self):\n",
        "            super().__init__()\n",
        "          def predict_proba(self, X):\n",
        "            d = self.decision_function(X)\n",
        "            return softmax(d)\n",
        "      model = model2\n",
        "      model_name = \"model2\"\n",
        "    else:\n",
        "      model_name = f\"{name.lower()}\"\n",
        "\n",
        "    pipe = make_pipeline(MinMaxScaler(), model())\n",
        "\n",
        "    model_params = {}\n",
        "    if \"random_state\" in model().get_params().keys():\n",
        "      model_params[f\"{model_name}__random_state\"] = 42\n",
        "    if \"n_jobs\" in model().get_params().keys():\n",
        "      model_params[f\"{model_name}__n_jobs\"] = -1\n",
        "    if \"probability\" in model().get_params().keys():\n",
        "      model_params[f\"{model_name}__probability\"] = True\n",
        "    if name == \"SGDClassifier\":  # change the loss to allow multiclass and predict proba (log or modified huber?)\n",
        "      model_params[f\"{model_name}__loss\"] = \"log\"\n",
        "    if model_params != {}:\n",
        "      pipe.set_params(**model_params)\n",
        "    \n",
        "    if name in no_sample_weight:\n",
        "      fit_params = {}\n",
        "    else:\n",
        "      fit_params = {f\"{model_name}__sample_weight\": sample_weight}\n",
        "\n",
        "    if groups is None:\n",
        "      cv = RepeatedKFold(n_splits=5, n_repeats=5, random_state=42)\n",
        "    else:\n",
        "      sss = StratifiedShuffleSplit(n_splits=5, test_size=0.33, random_state=42)\n",
        "      cv = sss.split(X, groups)\n",
        "\n",
        "    if hasattr(model, \"predict_proba\"):\n",
        "      new_custom_scorer = custom_scorer\n",
        "    else:\n",
        "      new_custom_scorer = custom_scorer.copy()\n",
        "      new_custom_scorer.pop(\"avg_precision\")\n",
        "      new_custom_scorer.pop(\"auroc\")\n",
        "      new_custom_scorer.pop(\"auprc\")\n",
        "\n",
        "    try:\n",
        "      cv_model = cross_validate(\n",
        "      pipe, X, y, cv=cv,\n",
        "      return_estimator=False, n_jobs=-1, fit_params=fit_params,\n",
        "      scoring=new_custom_scorer)\n",
        "    except ValueError as e:\n",
        "      print(e)\n",
        "      print(f\"Can't stratify because some states represented in the dataset don't contains enough counties\")\n",
        "      cv = RepeatedKFold(n_splits=5, n_repeats=5, random_state=42)\n",
        "      cv_model = cross_validate(\n",
        "      pipe, X, y, cv=cv,\n",
        "      return_estimator=False, n_jobs=-1, fit_params=fit_params,\n",
        "      scoring=new_custom_scorer)\n",
        "    except Exception as e:\n",
        "      print(e)\n",
        "    \n",
        "    # add metrics to the lists\n",
        "    Accuracy.append(np.nanmean(cv_model[\"test_accuracy\"]))\n",
        "    B_Accuracy.append(np.nanmean(cv_model[\"test_balanced_accuracy\"]))\n",
        "    PRECISION.append(np.nanmean(cv_model[\"test_precision\"]))\n",
        "    RECALL.append(np.nanmean(cv_model[\"test_recall\"]))\n",
        "    F1.append(np.nanmean(cv_model[\"test_f1\"]))\n",
        "    # nan mean, because when folding, a label can be missing in a fold and cause nan values\n",
        "    if hasattr(model, \"predict_proba\"):  # we can predict proba\n",
        "      auroc_val = np.nanmean(cv_model[\"test_auroc\"])\n",
        "      auprc_val = np.nanmean(cv_model[\"test_auprc\"])\n",
        "      micro_avg_prec_val = np.nanmean(cv_model[\"test_avg_precision\"])\n",
        "    else:\n",
        "      auroc_val = np.nan\n",
        "      auprc_val = np.nan\n",
        "      micro_avg_prec_val = np.nan\n",
        "    AUROC.append(auroc_val)\n",
        "    AUPRC.append(auprc_val)\n",
        "    MICRO_AVG_PREC.append(micro_avg_prec_val)\n",
        "    names.append(name)\n",
        "    TIME.append(perf_counter() - top)\n",
        "  df_models = pd.DataFrame({\"Model\": names,\n",
        "                            \"Accuracy\": Accuracy,\n",
        "                            \"Balanced Accuracy\": B_Accuracy,\n",
        "                            \"Recall\": RECALL,\n",
        "                            \"Precision\": PRECISION,\n",
        "                            \"F1 Score\": F1,\n",
        "                            \"AUROC\": AUROC,\n",
        "                            \"AUPRC\": AUPRC,\n",
        "                            \"Micro avg Precision\": MICRO_AVG_PREC,\n",
        "                            \"Time Taken\": TIME})\n",
        "  df_models = df_models.sort_values(by=\"Balanced Accuracy\", ascending=False).set_index(\"Model\")\n",
        "  return df_models\n",
        "\n",
        "def all_models_reg(models_to_try, no_sample_weight, custom_scorer, X, y,\n",
        "                   sample_weight=None, groups=None):\n",
        "  R2 = []\n",
        "  Adj_R2 = []\n",
        "  RMSE = []\n",
        "  MAE = []\n",
        "  MAPE = []\n",
        "  names = []\n",
        "  TIME = []\n",
        "\n",
        "  # add adjusted r2 to the custom_scorer\n",
        "  # we will fix fit_intercept to True\n",
        "  new_custom_scorer = custom_scorer.copy()\n",
        "  new_custom_scorer['adj_r2'] = make_scorer(r2_score_adj, n=X.shape[0],\n",
        "                                            p=X.shape[1], fit_intercept=True,\n",
        "                                            greater_is_better=True)\n",
        "\n",
        "  for name, model in tqdm(models_to_try):\n",
        "    print(name)\n",
        "    top = perf_counter()\n",
        "    pipe = make_pipeline(MinMaxScaler(), model())\n",
        "\n",
        "    model_params = {}\n",
        "    if \"random_state\" in model().get_params().keys():\n",
        "      model_params[f\"{name.lower()}__random_state\"] = 42\n",
        "    if \"n_jobs\" in model().get_params().keys():\n",
        "      model_params[f\"{name.lower()}__n_jobs\"] = -1\n",
        "    if model_params != {}:\n",
        "      pipe.set_params(**model_params)\n",
        "    \n",
        "    if name in no_sample_weight:\n",
        "      fit_params = {}\n",
        "    else:\n",
        "      fit_params = {f\"{name.lower()}__sample_weight\": sample_weight}\n",
        "\n",
        "    if groups is None:\n",
        "      cv = RepeatedKFold(n_splits=5, n_repeats=5, random_state=42)\n",
        "    else:\n",
        "      sss = StratifiedShuffleSplit(n_splits=5, test_size=0.33, random_state=42)\n",
        "      cv = sss.split(X, groups)\n",
        "\n",
        "    try:\n",
        "      cv_model = cross_validate(\n",
        "      pipe, X, y, cv=cv,\n",
        "      return_estimator=False, n_jobs=-1, fit_params=fit_params,\n",
        "      scoring=new_custom_scorer)\n",
        "    except ValueError as e:\n",
        "      print(e)\n",
        "      print(f\"Can't stratify because some states represented in the dataset don't contains enough counties\")\n",
        "      cv = RepeatedKFold(n_splits=5, n_repeats=5, random_state=42)\n",
        "      cv_model = cross_validate(\n",
        "      pipe, X, y, cv=cv,\n",
        "      return_estimator=False, n_jobs=-1, fit_params=fit_params,\n",
        "      scoring=new_custom_scorer)\n",
        "    except Exception as e:\n",
        "      print(e)\n",
        "   \n",
        "    # add metrics to the lists\n",
        "    R2.append(np.nanmean(cv_model[\"test_r2\"]))\n",
        "    Adj_R2.append(np.nanmean(cv_model[\"test_adj_r2\"]))\n",
        "    RMSE.append(np.nanmean(cv_model[\"test_rmse\"]))\n",
        "    MAE.append(np.nanmean(cv_model[\"test_mae\"]))\n",
        "    MAPE.append(np.nanmean(cv_model[\"test_mape\"]))\n",
        "    names.append(name)\n",
        "    TIME.append(perf_counter() - top)\n",
        "  # sign flip these metrics (because lower is better)\n",
        "  RMSE = [-x for x in RMSE]\n",
        "  MAE = [-x for x in MAE]\n",
        "  MAPE = [-x for x in MAPE]\n",
        "  df_models = pd.DataFrame({\"Model\": names,\n",
        "                            \"R2\": R2,\n",
        "                            \"Adjusted R2\": Adj_R2,\n",
        "                            \"RMSE\": RMSE,\n",
        "                            \"MAE\": MAE,\n",
        "                            \"MAPE\": MAPE,\n",
        "                            \"Time Taken\": TIME})\n",
        "  df_models = df_models.sort_values(by=\"MAPE\", ascending=True).set_index(\"Model\")\n",
        "  return df_models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3uI95zn3pGia"
      },
      "source": [
        "### Virus introduction (all periods)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "id": "baow3b-3dyjM",
        "outputId": "d6ccc659-ad64-4a90-fb6c-dc6cd6d4c82f"
      },
      "outputs": [],
      "source": [
        "dt, X, y, sample_weight, groups = virus_introduction_all(county_database2_imputed,\n",
        "                                                         selected_columns_list,\n",
        "                                                         X_selected_columns_list,\n",
        "                                                         plot=False)\n",
        "plt.hist(y)\n",
        "plt.xlabel(\"Period\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.title(\"Distribution of COVID-19 introductions for each periods (-1) (3=no introduction)\")\n",
        "plt.tight_layout()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ApgbTd-qHOtY",
        "outputId": "484c3f7f-c1d4-424e-d669-30c72954b0bc"
      },
      "outputs": [],
      "source": [
        "df_clf = all_models_clf(CLASSIFIERS, NO_SAMPLE_WEIGHT_CLASSIFIERS,\n",
        "                        NO_PREDICT_PROBA_CLASSIFIERS,\n",
        "                        custom_scorer_clf, X, y, sample_weight,\n",
        "                        groups)\n",
        "df_clf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "anPIO-lI7mLu"
      },
      "source": [
        "### Virus introduction (each period)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "id": "Z5hKw4qr7n81",
        "outputId": "0825e0a5-365b-44b7-9f8f-6a9961b9b148"
      },
      "outputs": [],
      "source": [
        "dt, X, y_list, \\\n",
        "  sample_weight, groups = virus_introduction_periods(county_database2_imputed,\n",
        "                                                     selected_columns_list,\n",
        "                                                     X_selected_columns_list,\n",
        "                                                     plot=False)\n",
        "y1, y2, y3, yother = y_list\n",
        "\n",
        "fig, axs = plt.subplots(2, 2)\n",
        "axs[0, 0].hist(y1)\n",
        "axs[0, 0].set_title(\"Introduction Period 1\")\n",
        "axs[0, 1].hist(y2)\n",
        "axs[0, 1].set_title(\"Introduction Period 2\")\n",
        "axs[1, 0].hist(y3)\n",
        "axs[1, 0].set_title(\"Introduction Period 3\")\n",
        "axs[1, 1].hist(yother)\n",
        "axs[1, 1].set_title(\"Introduction Later\")\n",
        "\n",
        "for ax in axs.flat:\n",
        "    ax.set(xlabel=\"Period\", ylabel=\"Count\")\n",
        "\n",
        "# Hide x labels and tick labels for top plots and y ticks for right plots.\n",
        "for ax in axs.flat:\n",
        "    ax.label_outer()\n",
        "plt.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ZXLMGsf7_FB"
      },
      "source": [
        "Period 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7v5ozOnCRvu2",
        "outputId": "c1cb91c2-46fa-4b0c-8781-dbcb3a9c193f"
      },
      "outputs": [],
      "source": [
        "# Benchmark\n",
        "print(f\"Benchmark: {100 * (max(np.count_nonzero(y1==0), np.count_nonzero(y1==1))) / len(y1)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "RYY1ihPc7pyd",
        "outputId": "9af0d0ac-d8f5-41e1-e7f1-53f24293ba84"
      },
      "outputs": [],
      "source": [
        "df_clf = all_models_clf(CLASSIFIERS, NO_SAMPLE_WEIGHT_CLASSIFIERS,\n",
        "                        NO_PREDICT_PROBA_CLASSIFIERS,\n",
        "                        custom_scorer_clf, X, y1, sample_weight,\n",
        "                        groups)\n",
        "df_clf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fQlSl3omRDv6"
      },
      "source": [
        "Period 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SpgFHZJ6R-MH",
        "outputId": "aa333402-5ba5-4711-9b50-eb0d0886d6c5"
      },
      "outputs": [],
      "source": [
        "# Benchmark\n",
        "print(f\"Benchmark: {100 * (max(np.count_nonzero(y2==0), np.count_nonzero(y2==1))) / len(y2)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "-PfEC-ZOREf8",
        "outputId": "ffe02ad7-e8dd-48ba-aaa6-2e5a44e0add0"
      },
      "outputs": [],
      "source": [
        "df_clf = all_models_clf(CLASSIFIERS, NO_SAMPLE_WEIGHT_CLASSIFIERS,\n",
        "                        NO_PREDICT_PROBA_CLASSIFIERS,\n",
        "                        custom_scorer_clf, X, y2, sample_weight,\n",
        "                        groups)\n",
        "df_clf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-nMTUHJqREmo"
      },
      "source": [
        "Period 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rZsgmmXNSCSq",
        "outputId": "db2feebb-6641-4b41-8e81-594215022557"
      },
      "outputs": [],
      "source": [
        "# Benchmark\n",
        "print(f\"Benchmark: {100 * (max(np.count_nonzero(y3==0), np.count_nonzero(y3==1))) / len(y3)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "NXHiY5tqRFRs",
        "outputId": "08d51f0d-e434-4537-8e27-432ec8056978"
      },
      "outputs": [],
      "source": [
        "df_clf = all_models_clf(CLASSIFIERS, NO_SAMPLE_WEIGHT_CLASSIFIERS,\n",
        "                        NO_PREDICT_PROBA_CLASSIFIERS,\n",
        "                        custom_scorer_clf, X, y3, sample_weight,\n",
        "                        groups)\n",
        "df_clf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UA0x7RUyRFkV"
      },
      "source": [
        "Period later than 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aRPbaqvoSFgx",
        "outputId": "21cc3f1e-79de-475c-d82f-f94b0a898192"
      },
      "outputs": [],
      "source": [
        "# Benchmark\n",
        "print(f\"Benchmark: {100 * (max(np.count_nonzero(yother==0), np.count_nonzero(yother==1))) / len(yother)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "GNveKsG7RHfa",
        "outputId": "d4ababd3-ce16-4aba-fcfc-0e58675c29a6"
      },
      "outputs": [],
      "source": [
        "df_clf = all_models_clf(CLASSIFIERS, NO_SAMPLE_WEIGHT_CLASSIFIERS,\n",
        "                        NO_PREDICT_PROBA_CLASSIFIERS,\n",
        "                        custom_scorer_clf, X, yother, sample_weight,\n",
        "                        groups)\n",
        "df_clf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1tRH_jnlpJIX"
      },
      "source": [
        "### Virus spread"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YVVuSubZpKqM"
      },
      "outputs": [],
      "source": [
        "dt_list, X_list, y_list, \\\n",
        "sample_weight_list, groups_list = virus_spread_dt(county_database2_imputed,\n",
        "                                                  selected_columns_list,\n",
        "                                                  X_selected_columns_list,\n",
        "                                                  type_data=\"COVID-19\",\n",
        "                                                  plot=False)\n",
        "dt1, dt2, dt3, dt4, dt5, dt6 = dt_list\n",
        "X1, X2, X3, X4, X5, X6 = X_list\n",
        "y1, y2, y3, y4, y5, y6 = y_list\n",
        "sample_weight1, sample_weight2, sample_weight3, \\\n",
        "  sample_weight4, sample_weight5, sample_weight6 = sample_weight_list\n",
        "groups1, groups2, groups3, groups4, groups5, \\\n",
        "  groups6 = groups_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "_NCQOhVJHUoU",
        "outputId": "2e5f63db-2e52-4978-9764-5d7c8befda7e"
      },
      "outputs": [],
      "source": [
        "df_reg1 = all_models_reg(REGRESSORS, NO_SAMPLE_WEIGHT_REGRESSORS,\n",
        "                         custom_scorer_reg, X1, y1, sample_weight1,\n",
        "                         groups1)\n",
        "df_reg1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "69AivIJiqMWX",
        "outputId": "1166023c-5c0f-4525-8d76-fd89b8ba8309"
      },
      "outputs": [],
      "source": [
        "df_reg2 = all_models_reg(REGRESSORS, NO_SAMPLE_WEIGHT_REGRESSORS,\n",
        "                         custom_scorer_reg, X2, y2, sample_weight2,\n",
        "                         groups2)\n",
        "df_reg2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "AAXpmpQMqM9q",
        "outputId": "4beac515-e22a-4944-a20d-019b25ad1d5a"
      },
      "outputs": [],
      "source": [
        "df_reg3 = all_models_reg(REGRESSORS, NO_SAMPLE_WEIGHT_REGRESSORS,\n",
        "                         custom_scorer_reg, X3, y3, sample_weight3,\n",
        "                         groups3)\n",
        "df_reg3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "4r6gHoxi_cVm",
        "outputId": "584ed0bf-9813-4c5e-84a0-eb025ce7b665"
      },
      "outputs": [],
      "source": [
        "df_reg4 = all_models_reg(REGRESSORS, NO_SAMPLE_WEIGHT_REGRESSORS,\n",
        "                         custom_scorer_reg, X4, y4, sample_weight4,\n",
        "                         groups4)\n",
        "df_reg4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "BzXx3o2M_fpI",
        "outputId": "312df72a-ca48-4890-fe15-2ce7dea6aa0b"
      },
      "outputs": [],
      "source": [
        "df_reg5 = all_models_reg(REGRESSORS, NO_SAMPLE_WEIGHT_REGRESSORS,\n",
        "                         custom_scorer_reg, X5, y5, sample_weight5,\n",
        "                         groups5)\n",
        "df_reg5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "GkGU2tqa_ima",
        "outputId": "ea93e89e-47a2-4a49-e74e-421f7829a1dc"
      },
      "outputs": [],
      "source": [
        "df_reg6 = all_models_reg(REGRESSORS, NO_SAMPLE_WEIGHT_REGRESSORS,\n",
        "                         custom_scorer_reg, X6, y6, sample_weight6,\n",
        "                         groups6)\n",
        "df_reg6"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2md2hZC1x5s-"
      },
      "source": [
        "### Ensemble model - Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gKV7dVtex8xI"
      },
      "outputs": [],
      "source": [
        "chosen_models = [\"HistGradientBoostingRegressor\", \"LGBMR\",]\n",
        "for mod in chosen_models:\n",
        "  if mod in no_predict_proba:  # extend the class with a custom predict proba function based on the decision function\n",
        "    if mod == \"NearestCentroid\":  # no decision function, calculate pairwise distance to clusters instead\n",
        "      class model2(model):\n",
        "        def __init__(self):\n",
        "          super().__init__()\n",
        "        def predict_proba(self, X):\n",
        "          distances = pairwise_distances(X, Y=self.centroids_,\n",
        "                                          metric=\"euclidean\",\n",
        "                                          n_jobs=-1)\n",
        "          return softmax(-distances)\n",
        "    else:\n",
        "      class model2(model):\n",
        "        def __init__(self):\n",
        "          super().__init__()\n",
        "        def predict_proba(self, X):\n",
        "          d = self.decision_function(X)\n",
        "          return softmax(d)\n",
        "    model = model2\n",
        "    model_name = \"model2\"\n",
        "  else:\n",
        "    model_name = f\"{mod.lower()}\"\n",
        "\n",
        "HistGBMR, LGBMR, SVR, et LinearSVR, XGBRegressor\n",
        "eclf = VotingClassifier(estimators=[('lr', clf1), ('rf', clf2), ('gnb', clf3)], voting='hard')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jxdS95pxu7kG"
      },
      "source": [
        "## Calibration plots"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EP9fMYxIJEVm"
      },
      "source": [
        "### Initialization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C4x8PCY9JIWu"
      },
      "outputs": [],
      "source": [
        "def cal_data_group(prob, true, group, bins, plot=False, wrap_col=3, sns_height=4, save_plot=False):\n",
        "  cal_dat = pd.DataFrame({\"prob\": prob,\n",
        "                          \"true\": true,\n",
        "                          \"group\": group})\n",
        "  cal_dat[\"Count\"] = 1\n",
        "  cal_dat[\"Bin\"] = (cal_dat.groupby(\"group\",as_index=False)[\"prob\"]\n",
        "                      ).transform(lambda x: pd.qcut(x, bins, labels=range(bins))\n",
        "                      ).astype(int) + 1\n",
        "  agg_bins = cal_dat.groupby([\"group\", \"Bin\"], as_index=False)[\"Count\", \"prob\",\n",
        "                                                               \"true\"].sum()\n",
        "  agg_bins[\"Predicted\"] = agg_bins[\"prob\"] / agg_bins[\"Count\"]\n",
        "  agg_bins[\"Actual\"] = agg_bins[\"true\"] / agg_bins[\"Count\"]\n",
        "  agg_long = pd.melt(agg_bins, id_vars=[\"Bin\", \"group\"],\n",
        "                     value_vars=[\"Predicted\", \"Actual\"], \n",
        "                     var_name=\"Type\", value_name=\"Probability\")\n",
        "  if plot:\n",
        "    d = {\"marker\": [\"o\", \"X\"]}\n",
        "    ax = sns.FacetGrid(data=agg_long, col=\"group\", hue=\"Type\", hue_kws=d,\n",
        "                       col_wrap=wrap_col, despine=False, height=sns_height)\n",
        "    ax.map(plt.plot, \"Bin\", \"Probability\", markeredgecolor=\"w\")\n",
        "    ax.set_titles(\"{col_name}\")\n",
        "    ax.set_xlabels(\"\")\n",
        "    ax.set_xticklabels(\"\")\n",
        "    ax.axes[0].legend(loc=\"upper left\")\n",
        "    # Setting xlim in FacetGrid not behaving how I want\n",
        "    for a in ax.axes:\n",
        "        a.set_xlim([0,bins+1])\n",
        "        a.tick_params(length=0)\n",
        "    if save_plot:\n",
        "        plt.savefig(save_plot, dpi=500, bbox_inches=\"tight\")\n",
        "    plt.show()\n",
        "  return agg_bins\n",
        "\n",
        "# function to use when determining groups using quantiles\n",
        "def determine_group(p: float, quantiles: list,\n",
        "                    names: list) -> str:\n",
        "  for i in range(len(quantiles)-1, -1, -1):\n",
        "    if p >= quantiles[i]:\n",
        "      return names[i+1]\n",
        "  return names[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kdQUehNyvAyS"
      },
      "source": [
        "### Virus Introduction (LogisticRegression)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dNcfmSq6u-jC",
        "outputId": "7c68fece-8b5a-4cc3-f046-8c1c98dcd2b8"
      },
      "outputs": [],
      "source": [
        "county_database2_imputed.index = county_database2_imputed.FIPS\n",
        "dt = county_database2_imputed[selected_columns]  # total_pop and deathcounts will be removed later\n",
        "\n",
        "dt[\"total_pop\"] = np.log(dt[\"total_pop\"])  # sample weights\n",
        "print(dt.shape)\n",
        "\n",
        "# drop nan values\n",
        "dt.dropna(inplace=True)\n",
        "print(\"Dataset shape after nan values removed: {}\".format(dt.shape))\n",
        "\n",
        "# Dataset: Period 1, 2, 3 or after\n",
        "X = dt[X_selected_columns]\n",
        "y = np.array([0 for _ in range(len(dt))])\n",
        "for i in range(len(dt)):\n",
        "  p1_count = (dt[\"deathCount_period1\"].iloc[i] >= 5)\n",
        "  p2_count = (dt[\"deathCount_period2\"].iloc[i] >= 5)\n",
        "  p3_count = (dt[\"deathCount_period3\"].iloc[i] >= 5)\n",
        "  if p1_count:\n",
        "    y[i] = 0\n",
        "\n",
        "  else:\n",
        "    y[i] = 1\n",
        "  \"\"\"\n",
        "  elif p2_count and not(p1_count):\n",
        "    y[i] = 1\n",
        "  elif p3_count and not(p1_count or p2_count):\n",
        "    y[i] = 2\n",
        "  else:\n",
        "    y[i] = 3\n",
        "  \"\"\"\n",
        "# weight the regression by the log of a county’s population\n",
        "sample_weight = dt[\"total_pop\"].to_numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XH35TCQozHIT"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "name, model = \"LogisticRegression\", LogisticRegression\n",
        "pipe = make_pipeline(MinMaxScaler(), model())\n",
        "\n",
        "model_params = {}\n",
        "if \"random_state\" in model().get_params().keys():\n",
        "  model_params[f\"{name.lower()}__random_state\"] = 42\n",
        "if \"n_jobs\" in model().get_params().keys():\n",
        "  model_params[f\"{name.lower()}__n_jobs\"] = -1\n",
        "if model_params != {}:\n",
        "  pipe.set_params(**model_params)\n",
        "\n",
        "if name in NO_SAMPLE_WEIGHT_CLASSIFIERS:\n",
        "  fit_params = {}\n",
        "else:\n",
        "  fit_params = {f\"{name.lower()}__sample_weight\": sample_weight}  # sample_weight_train\n",
        "\n",
        "pipe.fit(X, y, **fit_params)  # X_train, y_train, **fit_params\n",
        "y_pred = pipe.predict_proba(X)  # X_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G50oRPnPzyxQ"
      },
      "outputs": [],
      "source": [
        "from sklearn.calibration import calibration_curve\n",
        "true_prob, pred_prob = calibration_curve(y, y_pred[:, 1], n_bins=10)  # y_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "id": "l80uhCBY0Wth",
        "outputId": "5b1bffc4-bd7c-4ab6-b061-acf0a368587d"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots()\n",
        "# only these two lines are calibration curves\n",
        "plt.plot(pred_prob, true_prob, marker='o', linewidth=1, label=\"LogisticRegression\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KwmXl46vGytZ"
      },
      "source": [
        "American Communities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "feM3oShT6h8q",
        "outputId": "aa557bee-36a4-404b-fc89-4435cf1f1b20"
      },
      "outputs": [],
      "source": [
        "group = list(map(lambda x: county_info_dic[x], X[\"acp\"].to_numpy()))\n",
        "cal_data_group(prob=y_pred[:,1], true=y, group=group, bins=20, plot=True,\n",
        "               save_plot=\"Plots/Calibration Plots by American Communities.png\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2J5vjvI0GvZk"
      },
      "source": [
        "HHS Regions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "oYZ06EO0EJ6g",
        "outputId": "ec2abd18-a84e-4938-be6a-3762e18796a1"
      },
      "outputs": [],
      "source": [
        "group = list(map(lambda x: hhs_region_dic[x], X[\"HHS Region\"].to_numpy()))\n",
        "cal_data_group(prob=y_pred[:,1], true=y, group=group, bins=20, plot=True,\n",
        "               save_plot=\"Plots/Calibration Plots by HHS Regions.png\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yWzm772SG5hj"
      },
      "source": [
        "Political leaning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "YasBGyYvG7_D",
        "outputId": "ca7cdd8a-c42f-4f90-b139-975036354f2d"
      },
      "outputs": [],
      "source": [
        "political_quantiles = list(map(lambda q: np.quantile(X[\"political_leaning\"], q),\n",
        "                          [0.2, 0.4, 0.6, 0.8]))\n",
        "political_names = [\"Dem.\", \"Weakly dem\", \"Weakly rep.\", \"Rep.\", \"Strongly Rep\"]\n",
        "\n",
        "group = list(map(lambda x: determine_group(x, political_quantiles,\n",
        "                                           political_names),\n",
        "                 X[\"political_leaning\"].to_numpy()))\n",
        "cal_data_group(prob=y_pred[:,1], true=y, group=group, bins=20, plot=True,\n",
        "               save_plot=\"Plots/Calibration Plots by Political Leaning.png\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dBLM1zRkIabF"
      },
      "source": [
        "Ethnicity (hispanic)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "W7k9VQW1In8n",
        "outputId": "24680de4-1e2a-415a-c54e-aa1e7878aef2"
      },
      "outputs": [],
      "source": [
        "ethnicity_quantiles = list(map(lambda q: np.quantile(X[\"pct_hispanic\"], q),\n",
        "                          [1/3, 2/3]))\n",
        "ethnicity_names = [\"Low hispanic ethnicity\", \"Medium hispanic ethnicity\",\n",
        "                   \"High hispanic ethnicity\"]\n",
        "group = list(map(lambda x: determine_group(x, ethnicity_quantiles,\n",
        "                                           ethnicity_names),\n",
        "                 X[\"pct_hispanic\"].to_numpy()))\n",
        "cal_data_group(prob=y_pred[:,1], true=y, group=group, bins=20, plot=True,\n",
        "               save_plot=\"Plots/Calibration Plots by Hispanic Ethnicity.png\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ot2wME88WTtc"
      },
      "source": [
        "# Analysis through the lens of county community classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b61MwMuBnv7T"
      },
      "source": [
        "## First analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "0zHBNN7gZaoE",
        "outputId": "3547c515-e349-49e5-9004-e3612a933755"
      },
      "outputs": [],
      "source": [
        "import plotly.express as px\n",
        "import math\n",
        "# rename communities\n",
        "fig = px.pie(county_database, names=\"acp_name_with_number\", title=\"Number of counties per communities\")\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JnHATqMLM_Yx"
      },
      "outputs": [],
      "source": [
        "import plotly.express as px\n",
        "# rename communities\n",
        "bypop = county_database.groupby(by=\"acp_name_with_number\")[\"total_pop\"].sum().to_frame()\n",
        "bypop.reset_index(drop=False, inplace=True)\n",
        "bypop\n",
        "fig = px.pie(bypop, names=\"acp_name_with_number\", values=\"total_pop\",\n",
        "             title=\"Population per communities\")\n",
        "fig.update_traces(textposition=\"inside\", textinfo=\"percent+label\")\n",
        "fig.update_layout(legend_title=\"American Communities (id & name)\")\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "MTlM6Ex0qbBo",
        "outputId": "4bd70ca8-b3cf-4a3b-cd87-6861fb43d73f"
      },
      "outputs": [],
      "source": [
        "from plotly.subplots import make_subplots\n",
        "import plotly.graph_objects as go\n",
        "from plotly.graph_objs.layout import Grid\n",
        "\n",
        "# population threshold: percentage of the american population >= pop_threshold\n",
        "total_american_pop = county_database[\"total_pop\"].sum()\n",
        "pop_threshold = 0.05  # 5%\n",
        "communities_to_select = []\n",
        "for acp in range(1, 16):\n",
        "  acp_df = county_database[county_database[\"acp\"] == acp]\n",
        "  if (acp_df[\"total_pop\"].sum() / total_american_pop) >= pop_threshold:\n",
        "    communities_to_select.append(acp)\n",
        "\n",
        "fig = make_subplots(rows=2, cols=3)\n",
        "\n",
        "# colors of the communities\n",
        "color_list = [\"cyan\", \"red\", \"green\", \"pink\",\n",
        "              \"gray\", \"darkblue\", \"coral\", \"maroon\"]\n",
        "\n",
        "for per in range(6):\n",
        "  acp = communities_to_select[0]\n",
        "  acp_df = county_database[county_database[\"acp\"] == acp]\n",
        "  acp_name_with_number = acp_df['acp_name_with_number'].iloc[0]\n",
        "  if per != 0:\n",
        "    name = \"\"\n",
        "  else:\n",
        "    name = f\"{acp_name_with_number}\"\n",
        "  fig.add_trace(go.Histogram(x=acp_df[f\"deathRate_period{per+1}\"].to_numpy(),\n",
        "                             y=(100 * acp_df[\"total_pop\"] / total_american_pop).to_numpy(),\n",
        "                             histfunc=\"sum\",\n",
        "                             name=name,\n",
        "                             marker={\"color\": color_list[0]}),\n",
        "                row=(per//3 + 1), col=(per%3 + 1))\n",
        "  for j, acp in enumerate(communities_to_select[1:]):\n",
        "    acp_df = county_database[county_database[\"acp\"] == acp]\n",
        "    acp_name_with_number = acp_df['acp_name_with_number'].iloc[0]\n",
        "    if per != 0:\n",
        "      name = \"\"\n",
        "    else:\n",
        "      name = f\"{acp_name_with_number}\"\n",
        "    fig.append_trace(go.Histogram(x=acp_df[f\"deathRate_period{per+1}\"].to_numpy(),\n",
        "                                  y=(100 * acp_df[\"total_pop\"] / total_american_pop).to_numpy(),\n",
        "                                  histfunc=\"sum\",\n",
        "                                  name=name,\n",
        "                                  marker={\"color\": color_list[j+1]}),\n",
        "                     row=(per//3 + 1), col=(per%3 + 1))\n",
        "\n",
        "fig.update_layout(title=\"County's Death Rates (COVID-19 related, per 100k residents) per community<br>Only plot communities accounting for more than {thresh}% of the american population\".format(thresh=round(100*pop_threshold, 0)),\n",
        "                  xaxis_title_text=\"COVID-19 death rate (per 100k residents)\",\n",
        "                  yaxis_title_text=\"% of the American population\",\n",
        "                  plot_bgcolor =\"rgb(255,255,255)\")\n",
        "fig.update_xaxes(showline=True, linewidth=2, linecolor=\"black\", gridcolor=\"grey\")\n",
        "fig.update_yaxes(showline=True, linewidth=2, linecolor=\"black\", gridcolor=\"grey\")\n",
        "\n",
        "# Overlay both histograms\n",
        "fig.update_layout(barmode='overlay')\n",
        "# Reduce opacity to see all histograms\n",
        "fig.update_traces(opacity=0.75)\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "UtKJd1sRXYkY",
        "outputId": "051f6ca2-6171-48e6-fe11-8bba02891b17"
      },
      "outputs": [],
      "source": [
        "import plotly.graph_objects as go\n",
        "from plotly.graph_objs.layout import Grid\n",
        "\n",
        "# population threshold: percentage of the american population >= pop_threshold\n",
        "total_american_pop = county_database[\"total_pop\"].sum()\n",
        "pop_threshold = 0.05  # 5%\n",
        "communities_to_select = []\n",
        "for acp in range(1, 16):\n",
        "  acp_df = county_database[county_database[\"acp\"] == acp]\n",
        "  if (acp_df[\"total_pop\"].sum() / total_american_pop) >= pop_threshold:\n",
        "    communities_to_select.append(acp)\n",
        "\n",
        "fig = go.Figure()\n",
        "for acp in communities_to_select:\n",
        "  acp_df = county_database[county_database[\"acp\"] == acp]\n",
        "  fig.add_trace(go.Histogram(x=acp_df[\"deathRate_period2\"].to_numpy(),\n",
        "                             y=(100 * acp_df[\"total_pop\"] / total_american_pop).to_numpy(),\n",
        "                             histfunc=\"sum\",\n",
        "                             name=acp_df[\"acp_name_with_number\"].iloc[0]))\n",
        "\n",
        "fig.update_layout(title=\"Period 2 County's Death Rates (COVID-19 related, per 100k residents) per community<br>Only plot communities accounting for more than {thresh}% of the american population\".format(thresh=round(100*pop_threshold, 0)),\n",
        "                  xaxis_title_text=\"COVID-19 death rate (per 100k residents)\",\n",
        "                  yaxis_title_text=\"% of the American population\",\n",
        "                  plot_bgcolor =\"rgb(255,255,255)\")\n",
        "fig.update_xaxes(showline=True, linewidth=2, linecolor=\"black\", gridcolor=\"grey\")\n",
        "fig.update_yaxes(showline=True, linewidth=2, linecolor=\"black\", gridcolor=\"grey\")\n",
        "\n",
        "# Overlay both histograms\n",
        "fig.update_layout(barmode='overlay')\n",
        "# Reduce opacity to see all histograms\n",
        "fig.update_traces(opacity=0.75)\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "XROYDMskaJAj",
        "outputId": "04eb4ed9-5c2b-4598-97df-a421cc658c69"
      },
      "outputs": [],
      "source": [
        "import plotly.graph_objects as go\n",
        "from plotly.graph_objs.layout import Grid\n",
        "\n",
        "# population threshold: percentage of the american population >= pop_threshold\n",
        "total_american_pop = county_database[\"total_pop\"].sum()\n",
        "pop_threshold = 0.05  # 5%\n",
        "communities_to_select = []\n",
        "for acp in range(1, 16):\n",
        "  acp_df = county_database[county_database[\"acp\"] == acp]\n",
        "  if (acp_df[\"total_pop\"].sum() / total_american_pop) >= pop_threshold:\n",
        "    communities_to_select.append(acp)\n",
        "\n",
        "fig = go.Figure()\n",
        "for acp in communities_to_select:\n",
        "  acp_df = county_database[county_database[\"acp\"] == acp]\n",
        "  fig.add_trace(go.Histogram(x=acp_df[\"deathRate_period3\"].to_numpy(),\n",
        "                             y=(100 * acp_df[\"total_pop\"] / total_american_pop).to_numpy(),\n",
        "                             histfunc=\"sum\",\n",
        "                             name=acp_df[\"acp_name_with_number\"].iloc[0]))\n",
        "\n",
        "fig.update_layout(title=\"Period 3 County's Death Rates (COVID-19 related, per 100k residents) per community<br>Only plot communities accounting for more than {thresh}% of the american population\".format(thresh=round(100*pop_threshold, 0)),\n",
        "                  xaxis_title_text=\"COVID-19 death rate (per 100k residents)\",\n",
        "                  yaxis_title_text=\"% of the American population\",\n",
        "                  plot_bgcolor =\"rgb(255,255,255)\")\n",
        "fig.update_xaxes(showline=True, linewidth=2, linecolor=\"black\", gridcolor=\"grey\")\n",
        "fig.update_yaxes(showline=True, linewidth=2, linecolor=\"black\", gridcolor=\"grey\")\n",
        "\n",
        "# Overlay both histograms\n",
        "fig.update_layout(barmode='overlay')\n",
        "# Reduce opacity to see all histograms\n",
        "fig.update_traces(opacity=0.75)\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "xvOGf1JX7YZL",
        "outputId": "8596f6e1-4132-4294-e18e-6b97da5562b4"
      },
      "outputs": [],
      "source": [
        "import plotly.graph_objects as go\n",
        "acp = np.array(list(county_info_dic.items()))[:,1]  # communities ordered by id (from 1 to 15) (without Unknown)\n",
        "\n",
        "period1_data = []\n",
        "period2_data = []\n",
        "period3_data = []\n",
        "error_p1 = []\n",
        "error_p2 = []\n",
        "error_p3 = []\n",
        "for c_name in acp:\n",
        "  subdf = county_database[county_database[\"acp_name\"] == c_name]\n",
        "  p1 = subdf[\"deathRate_period1\"].mean()\n",
        "  e1 = 1.96 * subdf[\"deathRate_period1\"].std() / np.sqrt(len(subdf))\n",
        "  p2 = subdf[\"deathRate_period2\"].mean()\n",
        "  e2 = 1.96 * subdf[\"deathRate_period2\"].std() / np.sqrt(len(subdf))\n",
        "  p3 = subdf[\"deathRate_period3\"].mean()\n",
        "  e3 = 1.96 * subdf[\"deathRate_period3\"].std() / np.sqrt(len(subdf))\n",
        "  period1_data.append(p1)\n",
        "  period2_data.append(p2)\n",
        "  period3_data.append(p3)\n",
        "  error_p1.append(e1)\n",
        "  error_p2.append(e2)\n",
        "  error_p3.append(e3)\n",
        "fig = go.Figure(data=[\n",
        "    go.Bar(name=\"Period 1 (February 2020 - May 2020)\", x=acp, y=period1_data,\n",
        "           error_y=dict(type=\"data\", array=error_p1)),\n",
        "    go.Bar(name=\"Period 2 (June 2020 - October 2020)\", x=acp, y=period2_data,\n",
        "           error_y=dict(type=\"data\", array=error_p2)),\n",
        "    go.Bar(name=\"Period 3 (November 2020 - February 2021)\", x=acp,\n",
        "           y=period3_data, error_y=dict(type=\"data\", array=error_p3))\n",
        "])\n",
        "# Change the bar mode\n",
        "fig.update_layout(barmode=\"group\",\n",
        "                  xaxis_title=\"American Communities\",\n",
        "                  yaxis_title=\"Counties' average death rate\",\n",
        "                  legend_title=\"Periods\",\n",
        "                  title=\"Counties' average death rate (COVID-19 related, per 100k residents)<br> per period per community - 95% confidence interval\")\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z6vnA7iyWst9",
        "outputId": "617459b2-979c-4b7d-be21-51a18c0b49b9"
      },
      "outputs": [],
      "source": [
        "period1_data_sorted, acp1 = [], []\n",
        "for x, y in sorted(zip(period1_data, acp), key=lambda x: x[0], reverse=True):\n",
        "  period1_data_sorted.append(x)\n",
        "  acp1.append(y)\n",
        "ranking_period1 = pd.DataFrame(period1_data_sorted, index=acp1)\n",
        "print(\"Ranking period 1\")\n",
        "print(ranking_period1)\n",
        "period2_data_sorted, acp2 = [], []\n",
        "for x, y in sorted(zip(period2_data, acp), key=lambda x: x[0], reverse=True):\n",
        "  period2_data_sorted.append(x)\n",
        "  acp2.append(y)\n",
        "ranking_period2 = pd.DataFrame(period2_data_sorted, index=acp2)\n",
        "print(\"Ranking period 2\")\n",
        "print(ranking_period2)\n",
        "period3_data_sorted, acp3 = [], []\n",
        "for x, y in sorted(zip(period3_data, acp), key=lambda x: x[0], reverse=True):\n",
        "  period3_data_sorted.append(x)\n",
        "  acp3.append(y)\n",
        "ranking_period3 = pd.DataFrame(period3_data_sorted, index=acp3)\n",
        "print(\"Ranking period 3\")\n",
        "print(ranking_period3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CPl5tdHw9Q9g"
      },
      "source": [
        "Remove average per period across all counties to remove a trend"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "pwHWQGf38NTO",
        "outputId": "7f462e47-5859-4226-d5ee-6c3776c37894"
      },
      "outputs": [],
      "source": [
        "import plotly.graph_objects as go\n",
        "acp = np.array(list(county_info_dic.items()))[:,1]  # communities ordered by id (from 1 to 15) (without Unknown)\n",
        "\n",
        "period1_data = []\n",
        "period2_data = []\n",
        "period3_data = []\n",
        "avg_p1 = county_database[\"deathRate_period1\"].dropna().mean()\n",
        "avg_p2 = county_database[\"deathRate_period2\"].dropna().mean()\n",
        "avg_p3 = county_database[\"deathRate_period3\"].dropna().mean()\n",
        "# error_p1 = []\n",
        "# error_p2 = []\n",
        "# error_p3 = []\n",
        "for c_name in acp:\n",
        "  subdf = county_database[county_database[\"acp_name\"] == c_name]\n",
        "  p1 = subdf[\"deathRate_period1\"].mean()\n",
        "  # e1 = 1.96 * subdf[\"deathRate_period1\"].std() / np.sqrt(len(subdf))\n",
        "  p2 = subdf[\"deathRate_period2\"].mean()\n",
        "  # e2 = 1.96 * subdf[\"deathRate_period2\"].std() / np.sqrt(len(subdf))\n",
        "  p3 = subdf[\"deathRate_period3\"].mean()\n",
        "  # e3 = 1.96 * subdf[\"deathRate_period3\"].std() / np.sqrt(len(subdf))\n",
        "  period1_data.append(p1 - avg_p1)\n",
        "  period2_data.append(p2 - avg_p2)\n",
        "  period3_data.append(p3 - avg_p3)\n",
        "fig = go.Figure(data=[\n",
        "    # go.Bar(name=\"Period 1 (February 2020 - May 2020)\", x=acp, y=period1_data,\n",
        "    #        error_y=dict(type=\"data\", array=error_p1)),\n",
        "    # go.Bar(name=\"Period 2 (June 2020 - October 2020)\", x=acp, y=period2_data,\n",
        "    #        error_y=dict(type=\"data\", array=error_p2)),\n",
        "    # go.Bar(name=\"Period 3 (November 2020 - February 2021)\", x=acp,\n",
        "    #        y=period3_data, error_y=dict(type=\"data\", array=error_p3))\n",
        "    go.Bar(name=\"Period 1 (February 2020 - May 2020)\", x=acp, y=period1_data),\n",
        "    go.Bar(name=\"Period 2 (June 2020 - October 2020)\", x=acp, y=period2_data),\n",
        "    go.Bar(name=\"Period 3 (November 2020 - February 2021)\", x=acp,\n",
        "           y=period3_data)\n",
        "])\n",
        "# Change the bar mode\n",
        "fig.update_layout(barmode=\"group\",\n",
        "                  xaxis_title=\"American Communities\",\n",
        "                  yaxis_title=\"Difference between counties' average death rate and average death rate per period\",\n",
        "                  legend_title=\"Periods\",\n",
        "                  title=\"Counties' average death rate (COVID-19 related, per 100k residents)<br> per period per community minus average death per period\")\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1S8AbiJMn0BH"
      },
      "source": [
        "## Scatter plot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xXlJ7NUunPyQ"
      },
      "source": [
        "Plot for period 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "PF6hd6jin32D",
        "outputId": "d61a8214-989b-4fa2-ef11-baf8baf356e0"
      },
      "outputs": [],
      "source": [
        "import plotly.graph_objects as go\n",
        "\n",
        "col_to_select = [\"acp\", \"acp_name\", \"total_pop\", \"FIPS\"]\n",
        "for i in range(1, 7):\n",
        "  col_to_select.append(f\"deathRate_period{i}\")\n",
        "county_database_covid = county_databases[\"COVID-19\"][\"county_database\"][col_to_select].dropna()\n",
        "county_database_excess = county_databases[\"Excess Mortality\"][\"county_database\"][col_to_select].dropna()\n",
        "county_database_excess = county_database_excess[county_database_excess[\"FIPS\"].isin(county_database_covid[\"FIPS\"])]\n",
        "\n",
        "fig = go.Figure()\n",
        "    \n",
        "for acp in np.unique(county_database_excess[\"acp\"]):\n",
        "  subdf = county_database_excess[county_database_excess[\"acp\"] == acp]\n",
        "  size = np.log(subdf[\"total_pop\"]).to_numpy()\n",
        "  y = subdf[\"deathRate_period1\"].to_numpy()\n",
        "  x = []\n",
        "  for _, fips in enumerate(subdf[\"FIPS\"]):\n",
        "    x.append(county_database_covid.loc[fips][\"deathRate_period1\"])\n",
        "  x = np.array(x)\n",
        "  fig.add_trace(go.Scatter(\n",
        "    x=x,\n",
        "    y=y,\n",
        "    mode=\"markers\",\n",
        "    marker=dict(size=size),\n",
        "    name=subdf[\"acp_name\"].iloc[0]))\n",
        "\n",
        "x_tot = []\n",
        "for _, fips in enumerate(county_database_excess[\"FIPS\"]):\n",
        "  x_tot.append(county_database_covid.loc[fips][\"deathRate_period1\"])\n",
        "x_tot = np.array(x_tot)\n",
        "\n",
        "fig.add_trace(go.Scatter(\n",
        "    x=np.linspace(x_tot.min(), x_tot.max(), 1000),\n",
        "    y=np.linspace(x_tot.min(), x_tot.max(), 1000),\n",
        "    line=dict(shape=\"linear\", color=\"rgb(0, 0, 0)\", dash=\"dash\"),\n",
        "    showlegend=False\n",
        "))\n",
        "\n",
        "fig.add_trace(go.Scatter(\n",
        "    x=np.linspace(x_tot.min(), x_tot.max(), 1000),\n",
        "    y=np.zeros(1000),\n",
        "    line=dict(shape=\"linear\", color=\"rgb(0, 0, 0)\"),\n",
        "    showlegend=False\n",
        "))\n",
        "\n",
        "\n",
        "fig.update_layout(legend_title_text=\"American Communities\")\n",
        "fig.update_xaxes(title_text=\"COVID-19 Crude Deaths Rate (per 100k residents)\")\n",
        "fig.update_yaxes(title_text=\"Excess All Causes Crude Deaths Rate (per 100k residents)\")\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kfT9qo5QnMzo"
      },
      "source": [
        "Bar for interactive plot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "651WQCNIpX9I",
        "outputId": "4722d7b3-07de-430f-d5c8-31845cd6d378"
      },
      "outputs": [],
      "source": [
        "import plotly.graph_objects as go\n",
        "\n",
        "\n",
        "col_to_select = [\"acp\", \"acp_name\", \"total_pop\", \"FIPS\"]\n",
        "for i in range(1, 7):\n",
        "  col_to_select.append(f\"deathRate_period{i}\")\n",
        "county_database_covid = county_databases[\"COVID-19\"][\"county_database\"][col_to_select].dropna()\n",
        "county_database_excess = county_databases[\"Excess Mortality\"][\"county_database\"][col_to_select].dropna()\n",
        "county_database_excess = county_database_excess[county_database_excess[\"FIPS\"].isin(county_database_covid[\"FIPS\"])]\n",
        "\n",
        "fig = go.Figure()\n",
        "\n",
        "# Add traces for each slider step (period)\n",
        "for period in range(1, 7):\n",
        "  for acp in np.unique(county_database_excess[\"acp\"]):\n",
        "    subdf = county_database_excess[county_database_excess[\"acp\"] == acp]\n",
        "    size = np.log(subdf[\"total_pop\"]).to_numpy()\n",
        "    y = subdf[f\"deathRate_period{period}\"].to_numpy()\n",
        "    x = []\n",
        "    for _, fips in enumerate(subdf[\"FIPS\"]):\n",
        "      x.append(county_database_covid.loc[fips][f\"deathRate_period{period}\"])\n",
        "    x = np.array(x)\n",
        "    fig.add_trace(go.Scatter(\n",
        "      visible=False,\n",
        "      x=x,\n",
        "      y=y,\n",
        "      mode=\"markers\",\n",
        "      marker=dict(size=size),\n",
        "      name=subdf[\"acp_name\"].iloc[0]))\n",
        "\n",
        "  x_tot = []\n",
        "  for _, fips in enumerate(county_database_excess[\"FIPS\"]):\n",
        "    x_tot.append(county_database_covid.loc[fips][f\"deathRate_period{period}\"])\n",
        "  x_tot = np.array(x_tot)\n",
        "\n",
        "  fig.add_trace(go.Scatter(\n",
        "      visible=False,\n",
        "      x=np.linspace(x_tot.min(), x_tot.max(), 1000),\n",
        "      y=np.linspace(x_tot.min(), x_tot.max(), 1000),\n",
        "      line=dict(shape=\"linear\", color=\"rgb(0, 0, 0)\", dash=\"dash\"),\n",
        "      showlegend=False\n",
        "  ))\n",
        "\n",
        "  fig.add_trace(go.Scatter(\n",
        "      visible=False,\n",
        "      x=np.linspace(x_tot.min(), x_tot.max(), 1000),\n",
        "      y=np.zeros(1000),\n",
        "      line=dict(shape=\"linear\", color=\"rgb(0, 0, 0)\"),\n",
        "      showlegend=False\n",
        "  ))\n",
        "\n",
        "# Make 1th period visible\n",
        "for i in range(17):\n",
        "  fig.data[i].visible = True\n",
        "\n",
        "# Create and add slider\n",
        "steps = []\n",
        "for i in range(1, 7):  # len(fig.data)\n",
        "    step = dict(\n",
        "        method=\"restyle\",\n",
        "        args=[{\"visible\": [False] * len(fig.data)},\n",
        "              {\"title\": \"Excess Mortality vs COVID-19 mortality per county per period. Period: \" + str(i)}],  # layout attribute\n",
        "        label=f\"{i}\"\n",
        "    )\n",
        "    for j in range(17):\n",
        "      step[\"args\"][0][\"visible\"][(i-1)*17 + j] = True  # Toggle i'th period to \"visible\"\n",
        "    steps.append(step)\n",
        "\n",
        "sliders = [dict(\n",
        "    active=0,\n",
        "    currentvalue={\"prefix\": \"Period: \"},\n",
        "    pad={\"t\": 10},\n",
        "    steps=steps\n",
        ")]\n",
        "\n",
        "fig.update_layout(\n",
        "    sliders=sliders,\n",
        "    title=\"Excess Mortality vs COVID-19 mortality per county per period.\",\n",
        "    legend_title_text=\"American Communities\",\n",
        "    xaxis_title=\"COVID-19 Crude Deaths Rate (per 100k residents)\",\n",
        "    yaxis_title=\"Excess All Causes Crude Deaths Rate (per 100k residents)\"\n",
        ")\n",
        "\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CFNmHXRlnJKD"
      },
      "source": [
        "Animation with play button"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "NkiH5_SkPrYp",
        "outputId": "a0df3c17-6b7e-4e5b-b688-f751e9347e2d"
      },
      "outputs": [],
      "source": [
        "import plotly.graph_objects as go\n",
        "\n",
        "# Import Data\n",
        "col_to_select = [\"acp\", \"acp_name\", \"total_pop\", \"FIPS\"]\n",
        "for i in range(1, 7):\n",
        "  col_to_select.append(f\"deathRate_period{i}\")\n",
        "county_database_covid = county_databases[\"COVID-19\"][\"county_database\"][col_to_select].dropna()\n",
        "county_database_excess = county_databases[\"Excess Mortality\"][\"county_database\"][col_to_select].dropna()\n",
        "county_database_excess = county_database_excess[county_database_excess[\"FIPS\"].isin(county_database_covid[\"FIPS\"])]\n",
        "\n",
        "for period in range(1, 7):  # add covid mortality into excess mortality dataframe\n",
        "  x_tot = []\n",
        "  for _, fips in enumerate(county_database_excess[\"FIPS\"]):\n",
        "    x_tot.append(county_database_covid.loc[fips][f\"deathRate_period{period}\"])\n",
        "  county_database_excess[f\"deathRate_covid_period{period}\"] = x_tot\n",
        "\n",
        "periods = list(str(i) for i in range(1, 7))\n",
        "\n",
        "# make list of american communities (acp=american communities project)\n",
        "acps = list(np.unique(county_database_excess[\"acp\"]))\n",
        "\n",
        "# Figure\n",
        "fig_dict  = {\n",
        "  'data': [],\n",
        "  'layout': {},\n",
        "  'frames': [],\n",
        "  'config': {'scrollzoom': True}\n",
        "}\n",
        "\n",
        "# fill in most of layout\n",
        "max_covid = county_database_excess[[f\"deathRate_covid_period{period}\" for period in periods]].max().max()\n",
        "min_excess = county_database_excess[[f\"deathRate_period{period}\" for period in periods]].min().min()\n",
        "max_excess = county_database_excess[[f\"deathRate_period{period}\" for period in periods]].max().max()\n",
        "# adjust a little bit the min and max\n",
        "max_covid = 1.05 * max_covid\n",
        "max_excess = 1.05 * max_excess\n",
        "min_excess = 1.05 * min_excess if min_excess < 0 else 0.95 * min_excess\n",
        "fig_dict['layout']['xaxis'] = {'title': 'COVID-19 Crude Deaths Rate (per 100k residents)',\n",
        "                               'gridcolor': '#FFFFFF',\n",
        "                               'range': [0, max_covid]}\n",
        "fig_dict['layout']['yaxis'] = {'title': 'Excess All Causes Crude Deaths Rate (per 100k residents)',\n",
        "                               'gridcolor': '#FFFFFF',\n",
        "                               'range': [min_excess, max_excess]}\n",
        "fig_dict['layout']['legend'] = {'title_text': 'American Communities'}\n",
        "fig_dict['layout']['title'] = 'Excess Mortality vs COVID-19 mortality per county per period.'\n",
        "fig_dict['layout']['hovermode'] = 'closest'\n",
        "fig_dict['layout']['plot_bgcolor'] = 'rgb(223, 232, 243)'\n",
        "fig_dict['layout']['width'] = 1280\n",
        "fig_dict['layout']['height'] = 720\n",
        "\n",
        "# Add Slider\n",
        "sliders_dict = {\n",
        "  'active': 0,\n",
        "  'yanchor': 'top',\n",
        "  'xanchor': 'left',\n",
        "  'currentvalue': {\n",
        "    'font': {'size': 20},\n",
        "    'prefix': 'Period:',\n",
        "    'visible': True,\n",
        "    'xanchor': 'right'\n",
        "  },\n",
        "  'transition': {'duration': 300, 'easing': 'cubic-in-out'},\n",
        "  'pad': {'b': 10, 't': 50},\n",
        "  'len': 0.9,\n",
        "  'x': 0.1,\n",
        "  'y': 0,\n",
        "  'steps': []\n",
        "}\n",
        "\n",
        "# Add Play and Pause Buttons\n",
        "fig_dict['layout']['updatemenus'] = [\n",
        "  {\n",
        "    'buttons': [\n",
        "      {\n",
        "        'args': [None, {'frame': {'duration': 500, 'redraw': False},\n",
        "                        'fromcurrent': True,\n",
        "                        'transition': {'duration': 300,\n",
        "                                       'easing': 'quadratic-in-out'}}],\n",
        "        'label': 'Play',\n",
        "        'method': 'animate'\n",
        "      },\n",
        "      {\n",
        "        'args': [[None], {'frame': {'duration': 0, 'redraw': False},\n",
        "                          'mode': 'immediate',\n",
        "                          'transition': {'duration': 0}}],\n",
        "        'label': 'Pause',\n",
        "        'method': 'animate'\n",
        "      }\n",
        "    ],\n",
        "    'direction': 'left',\n",
        "    'pad': {'r': 10, 't': 87},\n",
        "    'showactive': False,\n",
        "    'type': 'buttons',\n",
        "    'x': 0.1,\n",
        "    'xanchor': 'right',\n",
        "    'y': 0,\n",
        "    'yanchor': 'top'\n",
        "  }\n",
        "]\n",
        "\n",
        "# Intitialization: period 1\n",
        "period = 1\n",
        "for acp in acps:\n",
        "  dataset_by_acp = county_database_excess[county_database_excess[\"acp\"] == acp]\n",
        "\n",
        "  data_dict = {\n",
        "    \"x\": list(dataset_by_acp[f\"deathRate_covid_period{period}\"]),\n",
        "    \"y\": list(dataset_by_acp[f\"deathRate_period{period}\"]),\n",
        "    \"mode\": \"markers\",\n",
        "    \"text\": list(dataset_by_acp[\"FIPS\"]),\n",
        "    \"marker\": {\n",
        "      \"sizemode\": \"area\",\n",
        "      # \"sizeref\": 1e6,\n",
        "      \"size\": list(np.log(dataset_by_acp[\"total_pop\"]))\n",
        "    },\n",
        "    \"name\": dataset_by_acp[\"acp_name\"].iloc[0]\n",
        "  }\n",
        "  fig_dict[\"data\"].append(data_dict)\n",
        "# Add diagonal line\n",
        "data_dict = {\n",
        "  \"x\": list(np.linspace(0, max_covid, 1000)),\n",
        "  \"y\": list(np.linspace(0, max_covid, 1000)),\n",
        "  \"mode\": \"lines\",\n",
        "  \"line\": {\n",
        "    \"shape\": \"linear\",\n",
        "    \"color\": \"rgb(0, 0, 0)\",\n",
        "    \"dash\": \"dash\"\n",
        "  },\n",
        "  \"showlegend\": False\n",
        "}\n",
        "fig_dict[\"data\"].append(data_dict)\n",
        "# Add horizontal line\n",
        "data_dict = {\n",
        "  \"x\": list(np.linspace(min_excess, max_excess, 1000)),\n",
        "  \"y\": list(np.zeros(1000)),\n",
        "  \"mode\": \"lines\",\n",
        "  \"line\": {\n",
        "    \"shape\": \"linear\",\n",
        "    \"color\": \"rgb(0, 0, 0)\"\n",
        "  },\n",
        "  \"showlegend\": False\n",
        "}\n",
        "fig_dict[\"data\"].append(data_dict)\n",
        "\n",
        "# Create Frames and Animation\n",
        "for period in periods:\n",
        "  frame = {\"data\": [], \"name\": str(period)}\n",
        "  for acp in acps:\n",
        "    dataset_by_acp = county_database_excess[county_database_excess[\"acp\"] == acp]\n",
        "\n",
        "    data_dict = {\n",
        "      \"x\": list(dataset_by_acp[f\"deathRate_covid_period{period}\"]),\n",
        "      \"y\": list(dataset_by_acp[f\"deathRate_period{period}\"]),\n",
        "      \"mode\": \"markers\",\n",
        "      \"text\": list(dataset_by_acp[\"FIPS\"]),\n",
        "      \"marker\": {\n",
        "        \"sizemode\": \"area\",\n",
        "        # \"sizeref\": 1e6,\n",
        "        \"size\": list(np.log(dataset_by_acp[\"total_pop\"]))\n",
        "      },\n",
        "      \"name\": dataset_by_acp[\"acp_name\"].iloc[0]\n",
        "    }\n",
        "    frame[\"data\"].append(data_dict)\n",
        "  \n",
        "  # Add diagonal line\n",
        "  data_dict = {\n",
        "    \"x\": list(np.linspace(0, max_covid, 1000)),\n",
        "    \"y\": list(np.linspace(0, max_covid, 1000)),\n",
        "    \"mode\": \"lines\",\n",
        "    \"line\": {\n",
        "      \"shape\": \"linear\",\n",
        "      \"color\": \"rgb(0, 0, 0)\",\n",
        "      \"dash\": \"dash\"\n",
        "    },\n",
        "    \"showlegend\": False\n",
        "  }\n",
        "  frame[\"data\"].append(data_dict)\n",
        "  # Add horizontal line\n",
        "  data_dict = {\n",
        "    \"x\": list(np.linspace(min_excess, max_excess, 1000)),\n",
        "    \"y\": list(np.zeros(1000)),\n",
        "    \"mode\": \"lines\",\n",
        "    \"line\": {\n",
        "      \"shape\": \"linear\",\n",
        "      \"color\": \"rgb(0, 0, 0)\"\n",
        "    },\n",
        "    \"showlegend\": False\n",
        "  }\n",
        "  frame[\"data\"].append(data_dict)\n",
        "\n",
        "  fig_dict[\"frames\"].append(frame)\n",
        "  slider_step = {\"args\": [\n",
        "      [period],\n",
        "      {\"frame\": {\"duration\": 300, \"redraw\": False},\n",
        "        \"mode\": \"immediate\",\n",
        "        \"transition\": {\"duration\": 300}}\n",
        "  ],\n",
        "      \"label\": period,\n",
        "      \"method\": \"animate\"}\n",
        "  sliders_dict[\"steps\"].append(slider_step)\n",
        "\n",
        "fig_dict[\"layout\"][\"sliders\"] = [sliders_dict]\n",
        "\n",
        "fig = go.Figure(fig_dict)\n",
        "\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zjPM8BJJnVMa"
      },
      "source": [
        "Save the animation in gif format"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install -U gif[plotly]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HdO3HyELjD5n"
      },
      "outputs": [],
      "source": [
        "# Save animation\n",
        "\n",
        "import gif  # !pip install -U gif[plotly]\n",
        "@gif.frame\n",
        "def for_gif(frame):\n",
        "  fig_dict  = {\n",
        "    'data': [],\n",
        "    'layout': {},\n",
        "    'config': {'scrollzoom': True}\n",
        "  }\n",
        "\n",
        "  period = frame[\"name\"]\n",
        "\n",
        "  fig_dict['layout']['xaxis'] = {'title': 'COVID-19 Crude Deaths Rate (per 100k residents)',\n",
        "                                'gridcolor': '#FFFFFF',\n",
        "                                'range': [0, max_covid]}\n",
        "  fig_dict['layout']['yaxis'] = {'title': 'Excess All Causes Crude Deaths Rate (per 100k residents)',\n",
        "                                'gridcolor': '#FFFFFF',\n",
        "                                'range': [min_excess, max_excess]}\n",
        "  fig_dict['layout']['legend'] = {'title_text': 'American Communities'}\n",
        "  fig_dict['layout']['title'] = f'Excess Mortality vs COVID-19 mortality per county. Period: {period}'\n",
        "  fig_dict['layout']['hovermode'] = 'closest'\n",
        "  fig_dict['layout']['plot_bgcolor'] = 'rgb(223, 232, 243)'\n",
        "  fig_dict['layout']['width'] = 1280\n",
        "  fig_dict['layout']['height'] = 720\n",
        "\n",
        "  for data_dict in frame[\"data\"]:\n",
        "    fig_dict[\"data\"].append(data_dict)\n",
        "\n",
        "  fig = go.Figure(fig_dict)\n",
        "  return fig\n",
        "\n",
        "frames = []\n",
        "for frame in fig.to_dict()[\"frames\"]:\n",
        "  f = for_gif(frame)\n",
        "  frames.append(f)\n",
        "for _ in range(4):  # duplicate the last frame for a smoother animation!\n",
        "  frames.append(f)\n",
        "gif.save(frames, \"Plot/animation.gif\", duration=450)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QDNjArNztEcY"
      },
      "source": [
        "Animation: mean per american communities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 757
        },
        "id": "1GyWNJ0TtCQk",
        "outputId": "2cc29cd4-ef10-4a1f-dec3-ee411d9aaebd"
      },
      "outputs": [],
      "source": [
        "import plotly.graph_objects as go\n",
        "\n",
        "# Import Data\n",
        "col_to_select = [\"acp\", \"acp_name\", \"total_pop\", \"FIPS\"]\n",
        "for i in range(1, 7):\n",
        "  col_to_select.append(f\"deathRate_period{i}\")\n",
        "county_database_covid = county_databases[\"COVID-19\"][\"county_database\"][col_to_select].dropna()\n",
        "county_database_excess = county_databases[\"Excess Mortality\"][\"county_database\"][col_to_select].dropna()\n",
        "county_database_excess = county_database_excess[county_database_excess[\"FIPS\"].isin(county_database_covid[\"FIPS\"])]\n",
        "\n",
        "for period in range(1, 7):  # add covid mortality into excess mortality dataframe\n",
        "  x_tot = []\n",
        "  for _, fips in enumerate(county_database_excess[\"FIPS\"]):\n",
        "    x_tot.append(county_database_covid.loc[fips][f\"deathRate_period{period}\"])\n",
        "  county_database_excess[f\"deathRate_covid_period{period}\"] = x_tot\n",
        "\n",
        "periods = list(str(i) for i in range(1, 7))\n",
        "\n",
        "# make list of american communities (acp=american communities project)\n",
        "acps = list(np.unique(county_database_excess[\"acp\"]))\n",
        "\n",
        "# Figure\n",
        "fig_dict  = {\n",
        "  'data': [],\n",
        "  'layout': {},\n",
        "  'frames': [],\n",
        "  'config': {'scrollzoom': True}\n",
        "}\n",
        "\n",
        "# fill in most of layout\n",
        "max_covid = county_database_excess[[\"acp\"]+[f\"deathRate_covid_period{period}\" for period in periods]].groupby(by=\"acp\").mean().max().max()\n",
        "min_excess = county_database_excess[[\"acp\"]+[f\"deathRate_period{period}\" for period in periods]].groupby(by=\"acp\").mean().min().min()\n",
        "max_excess = county_database_excess[[\"acp\"]+[f\"deathRate_period{period}\" for period in periods]].groupby(by=\"acp\").mean().max().max()\n",
        "# adjust a little bit the min and max\n",
        "max_covid = 1.05 * max_covid\n",
        "max_excess = 1.05 * max_excess\n",
        "min_excess = 1.05 * min_excess if min_excess < 0 else 0.95 * min_excess\n",
        "fig_dict['layout']['xaxis'] = {'title': 'Average COVID-19 Crude Deaths Rate (per 100k residents)',\n",
        "                               'gridcolor': '#FFFFFF',\n",
        "                               'range': [0, max_covid]}\n",
        "fig_dict['layout']['yaxis'] = {'title': 'Average Excess All Causes Crude Deaths Rate (per 100k residents)',\n",
        "                               'gridcolor': '#FFFFFF',\n",
        "                               'range': [min_excess, max_excess]}\n",
        "fig_dict['layout']['legend'] = {'title_text': 'American Communities'}\n",
        "fig_dict['layout']['title'] = 'Excess Mortality vs COVID-19 mortality per american communities (average) per period.'\n",
        "fig_dict['layout']['hovermode'] = 'closest'\n",
        "fig_dict['layout']['plot_bgcolor'] = 'rgb(223, 232, 243)'\n",
        "fig_dict['layout']['width'] = 1280\n",
        "fig_dict['layout']['height'] = 720\n",
        "\n",
        "# Add Slider\n",
        "sliders_dict = {\n",
        "  'active': 0,\n",
        "  'yanchor': 'top',\n",
        "  'xanchor': 'left',\n",
        "  'currentvalue': {\n",
        "    'font': {'size': 20},\n",
        "    'prefix': 'Period:',\n",
        "    'visible': True,\n",
        "    'xanchor': 'right'\n",
        "  },\n",
        "  'transition': {'duration': 300, 'easing': 'cubic-in-out'},\n",
        "  'pad': {'b': 10, 't': 50},\n",
        "  'len': 0.9,\n",
        "  'x': 0.1,\n",
        "  'y': 0,\n",
        "  'steps': []\n",
        "}\n",
        "\n",
        "# Add Play and Pause Buttons\n",
        "fig_dict['layout']['updatemenus'] = [\n",
        "  {\n",
        "    'buttons': [\n",
        "      {\n",
        "        'args': [None, {'frame': {'duration': 500, 'redraw': False},\n",
        "                        'fromcurrent': True,\n",
        "                        'transition': {'duration': 300,\n",
        "                                       'easing': 'quadratic-in-out'}}],\n",
        "        'label': 'Play',\n",
        "        'method': 'animate'\n",
        "      },\n",
        "      {\n",
        "        'args': [[None], {'frame': {'duration': 0, 'redraw': False},\n",
        "                          'mode': 'immediate',\n",
        "                          'transition': {'duration': 0}}],\n",
        "        'label': 'Pause',\n",
        "        'method': 'animate'\n",
        "      }\n",
        "    ],\n",
        "    'direction': 'left',\n",
        "    'pad': {'r': 10, 't': 87},\n",
        "    'showactive': False,\n",
        "    'type': 'buttons',\n",
        "    'x': 0.1,\n",
        "    'xanchor': 'right',\n",
        "    'y': 0,\n",
        "    'yanchor': 'top'\n",
        "  }\n",
        "]\n",
        "\n",
        "# Intitialization: period 1\n",
        "period = 1\n",
        "for acp in acps:\n",
        "  dataset_by_acp = county_database_excess[county_database_excess[\"acp\"] == acp]\n",
        "\n",
        "  data_dict = {\n",
        "    \"x\": [dataset_by_acp[f\"deathRate_covid_period{period}\"].mean()],\n",
        "    \"y\": [dataset_by_acp[f\"deathRate_period{period}\"].mean()],\n",
        "    \"mode\": \"markers\",\n",
        "    \"text\": [dataset_by_acp[\"acp_name\"].iloc[0]],\n",
        "    \"marker\": {\n",
        "      \"sizemode\": \"area\",\n",
        "      \"sizeref\": 300,\n",
        "      \"size\": [dataset_by_acp[\"total_pop\"].mean()]\n",
        "    },\n",
        "    \"name\": dataset_by_acp[\"acp_name\"].iloc[0]\n",
        "  }\n",
        "  fig_dict[\"data\"].append(data_dict)\n",
        "# Add diagonal line\n",
        "data_dict = {\n",
        "  \"x\": list(np.linspace(0, max_covid, 1000)),\n",
        "  \"y\": list(np.linspace(0, max_covid, 1000)),\n",
        "  \"mode\": \"lines\",\n",
        "  \"line\": {\n",
        "    \"shape\": \"linear\",\n",
        "    \"color\": \"rgb(0, 0, 0)\",\n",
        "    \"dash\": \"dash\"\n",
        "  },\n",
        "  \"showlegend\": False\n",
        "}\n",
        "fig_dict[\"data\"].append(data_dict)\n",
        "# Add horizontal line\n",
        "data_dict = {\n",
        "  \"x\": list(np.linspace(min_excess, max_excess, 1000)),\n",
        "  \"y\": list(np.zeros(1000)),\n",
        "  \"mode\": \"lines\",\n",
        "  \"line\": {\n",
        "    \"shape\": \"linear\",\n",
        "    \"color\": \"rgb(0, 0, 0)\"\n",
        "  },\n",
        "  \"showlegend\": False\n",
        "}\n",
        "fig_dict[\"data\"].append(data_dict)\n",
        "\n",
        "# Create Frames and Animation\n",
        "for period in periods:\n",
        "  frame = {\"data\": [], \"name\": str(period)}\n",
        "  for acp in acps:\n",
        "    dataset_by_acp = county_database_excess[county_database_excess[\"acp\"] == acp]\n",
        "\n",
        "    data_dict = {\n",
        "      \"x\": [dataset_by_acp[f\"deathRate_covid_period{period}\"].mean()],\n",
        "      \"y\": [dataset_by_acp[f\"deathRate_period{period}\"].mean()],\n",
        "      \"mode\": \"markers\",\n",
        "      \"text\": [dataset_by_acp[\"acp_name\"].iloc[0]],\n",
        "      \"marker\": {\n",
        "        \"sizemode\": \"area\",\n",
        "        \"sizeref\": 300,\n",
        "        \"size\": [dataset_by_acp[\"total_pop\"].mean()]\n",
        "      },\n",
        "      \"name\": dataset_by_acp[\"acp_name\"].iloc[0]\n",
        "    }\n",
        "    frame[\"data\"].append(data_dict)\n",
        "  \n",
        "  # Add diagonal line\n",
        "  data_dict = {\n",
        "    \"x\": list(np.linspace(0, max_covid, 1000)),\n",
        "    \"y\": list(np.linspace(0, max_covid, 1000)),\n",
        "    \"mode\": \"lines\",\n",
        "    \"line\": {\n",
        "      \"shape\": \"linear\",\n",
        "      \"color\": \"rgb(0, 0, 0)\",\n",
        "      \"dash\": \"dash\"\n",
        "    },\n",
        "    \"showlegend\": False\n",
        "  }\n",
        "  frame[\"data\"].append(data_dict)\n",
        "  # Add horizontal line\n",
        "  data_dict = {\n",
        "    \"x\": list(np.linspace(min_excess, max_excess, 1000)),\n",
        "    \"y\": list(np.zeros(1000)),\n",
        "    \"mode\": \"lines\",\n",
        "    \"line\": {\n",
        "      \"shape\": \"linear\",\n",
        "      \"color\": \"rgb(0, 0, 0)\"\n",
        "    },\n",
        "    \"showlegend\": False\n",
        "  }\n",
        "  frame[\"data\"].append(data_dict)\n",
        "\n",
        "  fig_dict[\"frames\"].append(frame)\n",
        "  slider_step = {\"args\": [\n",
        "      [period],\n",
        "      {\"frame\": {\"duration\": 300, \"redraw\": False},\n",
        "        \"mode\": \"immediate\",\n",
        "        \"transition\": {\"duration\": 300}}\n",
        "  ],\n",
        "      \"label\": period,\n",
        "      \"method\": \"animate\"}\n",
        "  sliders_dict[\"steps\"].append(slider_step)\n",
        "\n",
        "fig_dict[\"layout\"][\"sliders\"] = [sliders_dict]\n",
        "\n",
        "fig = go.Figure(fig_dict)\n",
        "\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6S2qcH19zrWa"
      },
      "source": [
        "Save to gif"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0nUus8JUzovl"
      },
      "outputs": [],
      "source": [
        "# Save animation\n",
        "\n",
        "import gif  # !pip install -U gif[plotly]\n",
        "@gif.frame\n",
        "def for_gif(frame):\n",
        "  fig_dict  = {\n",
        "    'data': [],\n",
        "    'layout': {},\n",
        "    'config': {'scrollzoom': True}\n",
        "  }\n",
        "\n",
        "  period = frame[\"name\"]\n",
        "\n",
        "  fig_dict['layout']['xaxis'] = {'title': 'Average COVID-19 Crude Deaths Rate (per 100k residents)',\n",
        "                                'gridcolor': '#FFFFFF',\n",
        "                                'range': [0, max_covid]}\n",
        "  fig_dict['layout']['yaxis'] = {'title': 'Average Excess All Causes Crude Deaths Rate (per 100k residents)',\n",
        "                                'gridcolor': '#FFFFFF',\n",
        "                                'range': [min_excess, max_excess]}\n",
        "  fig_dict['layout']['legend'] = {'title_text': 'American Communities'}\n",
        "  fig_dict['layout']['title'] = f'Excess Mortality vs COVID-19 mortality per american communities (average). Period: {period}'\n",
        "  fig_dict['layout']['hovermode'] = 'closest'\n",
        "  fig_dict['layout']['plot_bgcolor'] = 'rgb(223, 232, 243)'\n",
        "  fig_dict['layout']['width'] = 1280\n",
        "  fig_dict['layout']['height'] = 720\n",
        "\n",
        "  for data_dict in frame[\"data\"]:\n",
        "    fig_dict[\"data\"].append(data_dict)\n",
        "\n",
        "  fig = go.Figure(fig_dict)\n",
        "  return fig\n",
        "\n",
        "frames = []\n",
        "for frame in fig.to_dict()[\"frames\"]:\n",
        "  f = for_gif(frame)\n",
        "  frames.append(f)\n",
        "for _ in range(4):  # duplicate the last frame for a smoother animation!\n",
        "  frames.append(f)\n",
        "gif.save(frames, \"Plot/animation_mean_acp.gif\", duration=450)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "7IZY6Ig-oRBv",
        "rSQSKdResV_U",
        "MivPzgM-m_r_",
        "NmYjap4GzIlB",
        "-G5EOYXfpLMQ",
        "YpOl0SaknWz7",
        "JU2_kb-ppMqM",
        "mBM_T60zpQor",
        "tvs6yHT2otj8",
        "1JFwEuUjoyiJ",
        "Q7v6Ocj3pC4a",
        "JMDBcvO6o9ie",
        "UFwR9dSTpGPV",
        "nGPeK73u8fpa",
        "snVbKnLXYF6A",
        "XsrZU1moKw9u",
        "wvdGxO9EKzuf",
        "VsqisC7Q55CM",
        "DgnQP45t6BNf",
        "_W4zrgeYVlsi",
        "4_HDbp-le_hQ",
        "kFvGixXPeEAc",
        "UAVt7HQmYWnq",
        "VGjieN1tmYgF",
        "0diFzUACbJuI",
        "HCT3K136fytF",
        "ZOzhJUcuzMHy",
        "YFFTXMp1zRSI",
        "NQLCSsxPzT7X",
        "A1Opi2J_zaES",
        "Gt1AzsOv09bj",
        "uHB9KyDO1BKH",
        "hA9CwK6szdwI",
        "3_BClck81t_1",
        "2HrKnUeLzf64",
        "QDA5qTl8zjiF",
        "uGFqaBFp3EnL",
        "UUQEr4QY3Jlx",
        "JPdZ4ffB3NP0",
        "dH4DTW5g3SxK",
        "7QH7QOOG3vwC",
        "qBL_psDjqcm2",
        "bGDWLtNeF3eb",
        "KftvP7W7pjJr",
        "62w89C7lbGCq",
        "BkO1ZJP8whpy",
        "AiRA4bQGxQY_",
        "t1W-EuH8xZyf",
        "0dKQCcsjgve4",
        "VbpEyIHAAmml",
        "y9UtCFBqxiiW",
        "x9INom4zbSyF",
        "ANRyqmA1FxkP",
        "mDKMfLml8Vyx",
        "4eKfDDGq8-Ky",
        "sI3TBDHl9IoG",
        "2S_FptB_9SX9",
        "EhYA_TI2Aw-3",
        "f3SEJw5oBG60",
        "hx2POqrlBKHt",
        "tJ-Yhm7J9cS7",
        "KONciFtDu6GF",
        "mbSGAt3wyO_m",
        "aS1h2le8yQbg",
        "TwC4bWRJCnTR",
        "EdT7EKIzFNgC",
        "spap_k2oCxs1",
        "8yCFRugJC11O",
        "DcS4oD7BC56t",
        "iuSGXoseFuaf",
        "1Qq3i74xFylR",
        "Y4fFVj1yF3US",
        "SWjv6lvsC_l8",
        "anPIO-lI7mLu",
        "jxdS95pxu7kG"
      ],
      "name": "MIT_Internship.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "interpreter": {
      "hash": "9a8029a51b57fe2d11fbd3b82c1cfa41a63a260330aa68039b758425bc8dfc97"
    },
    "kernelspec": {
      "display_name": "Python 3.7.10 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.10"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "09065aca8f7c48a6a801f3b760716e72": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "18724235591b450e9fcc0a436051516d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "34500ae12cd049c389956a9a5c16f2d8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_59604904ed874df0a3f6885525cbe9b9",
            "placeholder": "​",
            "style": "IPY_MODEL_6d4d4425bc7348ffa666258102753048",
            "value": " 1/1 [00:02&lt;00:00,  2.32s/it]"
          }
        },
        "3673ab82c5044152aa5dc0f3a0255682": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3bfd433a96c949dda7adc66394199a84": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "430fc357f18a4903a71bde653692520a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4a253363cbd14ccda67e8b64ebe6d597",
              "IPY_MODEL_8b1d5c7847df4a9cb309a4782b6530c1",
              "IPY_MODEL_e34011f650cb4c03bf8fcba7f5d38162"
            ],
            "layout": "IPY_MODEL_d0cc4e6615e24326b7d735184873e2d4"
          }
        },
        "4922e8013103495bbb55d4a5602eb56c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4a253363cbd14ccda67e8b64ebe6d597": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_543c2ac1c0514dde8e32c0c9206ce789",
            "placeholder": "​",
            "style": "IPY_MODEL_18724235591b450e9fcc0a436051516d",
            "value": "Export report to file: 100%"
          }
        },
        "4de42db77a83433598e325e29bfe77f0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4922e8013103495bbb55d4a5602eb56c",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d816f05c405f4a0ebc48e12d42454414",
            "value": 1
          }
        },
        "543c2ac1c0514dde8e32c0c9206ce789": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5551e6ab9399444b9caa3639855e712c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "59604904ed874df0a3f6885525cbe9b9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5cfc7a39f4d94040a211bcd815b1034d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_dde0ca8614fb4228aeb3c4838ce78fba",
              "IPY_MODEL_4de42db77a83433598e325e29bfe77f0",
              "IPY_MODEL_34500ae12cd049c389956a9a5c16f2d8"
            ],
            "layout": "IPY_MODEL_3673ab82c5044152aa5dc0f3a0255682"
          }
        },
        "61bdeafd837341079c12ff7260ba83e0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6d4d4425bc7348ffa666258102753048": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8387e7edffc7440090e41a40a9783662": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8b1d5c7847df4a9cb309a4782b6530c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_aa8e805a1084497692b181a4ea2fa32c",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3bfd433a96c949dda7adc66394199a84",
            "value": 1
          }
        },
        "aa8e805a1084497692b181a4ea2fa32c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d0cc4e6615e24326b7d735184873e2d4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d816f05c405f4a0ebc48e12d42454414": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "dde0ca8614fb4228aeb3c4838ce78fba": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5551e6ab9399444b9caa3639855e712c",
            "placeholder": "​",
            "style": "IPY_MODEL_09065aca8f7c48a6a801f3b760716e72",
            "value": "Render HTML: 100%"
          }
        },
        "e34011f650cb4c03bf8fcba7f5d38162": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8387e7edffc7440090e41a40a9783662",
            "placeholder": "​",
            "style": "IPY_MODEL_61bdeafd837341079c12ff7260ba83e0",
            "value": " 1/1 [00:00&lt;00:00, 14.23it/s]"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
